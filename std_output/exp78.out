2020-02-04 11:49:46.473007: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX
2020-02-04 11:49:46.599095: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:897] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-02-04 11:49:46.600351: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1405] Found device 0 with properties: 
name: TITAN Xp major: 6 minor: 1 memoryClockRate(GHz): 1.582
pciBusID: 0000:01:00.0
totalMemory: 11.91GiB freeMemory: 11.73GiB
2020-02-04 11:49:46.600369: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1484] Adding visible gpu devices: 0
2020-02-04 11:49:46.834429: I tensorflow/core/common_runtime/gpu/gpu_device.cc:965] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-02-04 11:49:46.834474: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971]      0 
2020-02-04 11:49:46.834483: I tensorflow/core/common_runtime/gpu/gpu_device.cc:984] 0:   N 
2020-02-04 11:49:46.834750: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1097] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 11348 MB memory) -> physical GPU (device: 0, name: TITAN Xp, pci bus id: 0000:01:00.0, compute capability: 6.1)
2020-02-04 11:49:47.112362: I tensorflow/core/kernels/cuda_solvers.cc:159] Creating CudaSolver handles for stream 0x563647617620
Epoch 1/8000

Epoch 00001: val_loss improved from inf to 20.77604, saving model to model_weights/model_2020-02-04_11-49-45.h5
 - 45s - loss: 30.8913 - val_loss: 20.7760
Epoch 2/8000

Epoch 00002: val_loss improved from 20.77604 to 19.90297, saving model to model_weights/model_2020-02-04_11-49-45.h5
 - 43s - loss: 19.8837 - val_loss: 19.9030
Epoch 3/8000

Epoch 00003: val_loss improved from 19.90297 to 16.81195, saving model to model_weights/model_2020-02-04_11-49-45.h5
 - 43s - loss: 18.2297 - val_loss: 16.8120
Epoch 4/8000

Epoch 00004: val_loss improved from 16.81195 to 16.45048, saving model to model_weights/model_2020-02-04_11-49-45.h5
 - 43s - loss: 16.5179 - val_loss: 16.4505
Epoch 5/8000

Epoch 00005: val_loss improved from 16.45048 to 14.99291, saving model to model_weights/model_2020-02-04_11-49-45.h5
 - 43s - loss: 15.3826 - val_loss: 14.9929
Epoch 6/8000

Epoch 00006: val_loss improved from 14.99291 to 14.37263, saving model to model_weights/model_2020-02-04_11-49-45.h5
 - 43s - loss: 14.8298 - val_loss: 14.3726
Epoch 7/8000

Epoch 00007: val_loss improved from 14.37263 to 14.03946, saving model to model_weights/model_2020-02-04_11-49-45.h5
 - 43s - loss: 14.2349 - val_loss: 14.0395
Epoch 8/8000

Epoch 00008: val_loss improved from 14.03946 to 13.59560, saving model to model_weights/model_2020-02-04_11-49-45.h5
 - 43s - loss: 13.8565 - val_loss: 13.5956
Epoch 9/8000

Epoch 00009: val_loss improved from 13.59560 to 13.05830, saving model to model_weights/model_2020-02-04_11-49-45.h5
 - 43s - loss: 13.5091 - val_loss: 13.0583
Epoch 10/8000

Epoch 00010: val_loss did not improve from 13.05830
 - 43s - loss: 13.2615 - val_loss: 13.4285
Epoch 11/8000

Epoch 00011: val_loss did not improve from 13.05830
 - 43s - loss: 12.8836 - val_loss: 13.1564
Epoch 12/8000

Epoch 00012: val_loss improved from 13.05830 to 12.27574, saving model to model_weights/model_2020-02-04_11-49-45.h5
 - 43s - loss: 12.4201 - val_loss: 12.2757
Epoch 13/8000

Epoch 00013: val_loss improved from 12.27574 to 12.24280, saving model to model_weights/model_2020-02-04_11-49-45.h5
 - 43s - loss: 12.1769 - val_loss: 12.2428
Epoch 14/8000

Epoch 00014: val_loss improved from 12.24280 to 11.83917, saving model to model_weights/model_2020-02-04_11-49-45.h5
 - 43s - loss: 12.1630 - val_loss: 11.8392
Epoch 15/8000

Epoch 00015: val_loss did not improve from 11.83917
 - 43s - loss: 12.0603 - val_loss: 12.6579
Epoch 16/8000

Epoch 00016: val_loss did not improve from 11.83917
 - 43s - loss: 11.9709 - val_loss: 13.3407
Epoch 17/8000

Epoch 00017: val_loss did not improve from 11.83917
 - 43s - loss: 11.9549 - val_loss: 14.4819
Epoch 18/8000

Epoch 00018: val_loss improved from 11.83917 to 11.15485, saving model to model_weights/model_2020-02-04_11-49-45.h5
 - 43s - loss: 11.7732 - val_loss: 11.1549
Epoch 19/8000

Epoch 00019: val_loss did not improve from 11.15485
 - 43s - loss: 11.2136 - val_loss: 11.7099
Epoch 20/8000

Epoch 00020: val_loss improved from 11.15485 to 10.79960, saving model to model_weights/model_2020-02-04_11-49-45.h5
 - 43s - loss: 11.3107 - val_loss: 10.7996
Epoch 21/8000

Epoch 00021: val_loss did not improve from 10.79960
 - 43s - loss: 11.4977 - val_loss: 15.2609
Epoch 22/8000

Epoch 00022: val_loss did not improve from 10.79960
 - 43s - loss: 12.5577 - val_loss: 10.9569
Epoch 23/8000

Epoch 00023: val_loss improved from 10.79960 to 10.24890, saving model to model_weights/model_2020-02-04_11-49-45.h5
 - 43s - loss: 11.0774 - val_loss: 10.2489
Epoch 24/8000

Epoch 00024: val_loss did not improve from 10.24890
 - 43s - loss: 11.3121 - val_loss: 11.8546
Epoch 25/8000

Epoch 00025: val_loss did not improve from 10.24890
 - 43s - loss: 10.7303 - val_loss: 10.8336
Epoch 26/8000

Epoch 00026: val_loss did not improve from 10.24890
 - 43s - loss: 11.0404 - val_loss: 10.5019
Epoch 27/8000

Epoch 00027: val_loss did not improve from 10.24890
 - 43s - loss: 11.0028 - val_loss: 11.8724
Epoch 28/8000

Epoch 00028: val_loss did not improve from 10.24890
 - 43s - loss: 10.8791 - val_loss: 11.8077
Epoch 29/8000

Epoch 00029: val_loss did not improve from 10.24890
 - 43s - loss: 10.9159 - val_loss: 11.5986
Epoch 30/8000

Epoch 00030: val_loss did not improve from 10.24890
 - 43s - loss: 10.6828 - val_loss: 11.1912
Epoch 31/8000

Epoch 00031: val_loss did not improve from 10.24890
 - 43s - loss: 10.5863 - val_loss: 10.3205
Epoch 32/8000

Epoch 00032: val_loss improved from 10.24890 to 10.11181, saving model to model_weights/model_2020-02-04_11-49-45.h5
 - 43s - loss: 10.4361 - val_loss: 10.1118
Epoch 33/8000

Epoch 00033: val_loss did not improve from 10.11181
 - 43s - loss: 11.1989 - val_loss: 10.4575
Epoch 34/8000

Epoch 00034: val_loss did not improve from 10.11181
 - 43s - loss: 11.0123 - val_loss: 10.3158
Epoch 35/8000

Epoch 00035: val_loss did not improve from 10.11181
 - 43s - loss: 10.6861 - val_loss: 10.1643
Epoch 36/8000

Epoch 00036: val_loss did not improve from 10.11181
 - 43s - loss: 10.3416 - val_loss: 10.1537
Epoch 37/8000

Epoch 00037: val_loss improved from 10.11181 to 10.01561, saving model to model_weights/model_2020-02-04_11-49-45.h5
 - 43s - loss: 10.1784 - val_loss: 10.0156
Epoch 38/8000

Epoch 00038: val_loss did not improve from 10.01561
 - 43s - loss: 10.3902 - val_loss: 10.8821
Epoch 39/8000

Epoch 00039: val_loss improved from 10.01561 to 9.55837, saving model to model_weights/model_2020-02-04_11-49-45.h5
 - 43s - loss: 10.0571 - val_loss: 9.5584
Epoch 40/8000

Epoch 00040: val_loss improved from 9.55837 to 9.54822, saving model to model_weights/model_2020-02-04_11-49-45.h5
 - 43s - loss: 10.2298 - val_loss: 9.5482
Epoch 41/8000

Epoch 00041: val_loss did not improve from 9.54822
 - 43s - loss: 10.1071 - val_loss: 9.8149
Epoch 42/8000

Epoch 00042: val_loss did not improve from 9.54822
 - 43s - loss: 9.8363 - val_loss: 9.8843
Epoch 43/8000

Epoch 00043: val_loss improved from 9.54822 to 9.40846, saving model to model_weights/model_2020-02-04_11-49-45.h5
 - 43s - loss: 9.7468 - val_loss: 9.4085
Epoch 44/8000

Epoch 00044: val_loss did not improve from 9.40846
 - 43s - loss: 9.8411 - val_loss: 9.4885
Epoch 45/8000

Epoch 00045: val_loss did not improve from 9.40846
 - 43s - loss: 9.7188 - val_loss: 10.3912
Epoch 46/8000

Epoch 00046: val_loss did not improve from 9.40846
 - 43s - loss: 9.5717 - val_loss: 10.1407
Epoch 47/8000

Epoch 00047: val_loss improved from 9.40846 to 9.19100, saving model to model_weights/model_2020-02-04_11-49-45.h5
 - 43s - loss: 9.6755 - val_loss: 9.1910
Epoch 48/8000

Epoch 00048: val_loss improved from 9.19100 to 9.03444, saving model to model_weights/model_2020-02-04_11-49-45.h5
 - 43s - loss: 9.3328 - val_loss: 9.0344
Epoch 49/8000

Epoch 00049: val_loss did not improve from 9.03444
 - 43s - loss: 9.5393 - val_loss: 9.4462
Epoch 50/8000

Epoch 00050: val_loss did not improve from 9.03444
 - 43s - loss: 9.5997 - val_loss: 11.8373
Epoch 51/8000

Epoch 00051: val_loss did not improve from 9.03444
 - 43s - loss: 9.3183 - val_loss: 9.6946
Epoch 52/8000

Epoch 00052: val_loss did not improve from 9.03444
 - 43s - loss: 9.5564 - val_loss: 9.2326
Epoch 53/8000

Epoch 00053: val_loss did not improve from 9.03444
 - 43s - loss: 9.4773 - val_loss: 10.0862
Epoch 54/8000

Epoch 00054: val_loss improved from 9.03444 to 8.95257, saving model to model_weights/model_2020-02-04_11-49-45.h5
 - 43s - loss: 9.2528 - val_loss: 8.9526
Epoch 55/8000

Epoch 00055: val_loss did not improve from 8.95257
 - 43s - loss: 9.1719 - val_loss: 9.1227
Epoch 56/8000

Epoch 00056: val_loss did not improve from 8.95257
 - 43s - loss: 9.1506 - val_loss: 9.6548
Epoch 57/8000

Epoch 00057: val_loss did not improve from 8.95257
 - 43s - loss: 9.1552 - val_loss: 11.3806
Epoch 58/8000

Epoch 00058: val_loss improved from 8.95257 to 8.65554, saving model to model_weights/model_2020-02-04_11-49-45.h5
 - 43s - loss: 9.1286 - val_loss: 8.6555
Epoch 59/8000

Epoch 00059: val_loss did not improve from 8.65554
 - 43s - loss: 9.0026 - val_loss: 9.0719
Epoch 60/8000

Epoch 00060: val_loss improved from 8.65554 to 8.24040, saving model to model_weights/model_2020-02-04_11-49-45.h5
 - 43s - loss: 8.9798 - val_loss: 8.2404
Epoch 61/8000

Epoch 00061: val_loss did not improve from 8.24040
 - 43s - loss: 8.9498 - val_loss: 8.4984
Epoch 62/8000

Epoch 00062: val_loss did not improve from 8.24040
 - 43s - loss: 9.0558 - val_loss: 8.9211
Epoch 63/8000

Epoch 00063: val_loss did not improve from 8.24040
 - 43s - loss: 9.1496 - val_loss: 8.8643
Epoch 64/8000

Epoch 00064: val_loss did not improve from 8.24040
 - 43s - loss: 9.3304 - val_loss: 10.1462
Epoch 65/8000

Epoch 00065: val_loss did not improve from 8.24040
 - 43s - loss: 9.2379 - val_loss: 9.5761
Epoch 66/8000

Epoch 00066: val_loss did not improve from 8.24040
 - 43s - loss: 9.3174 - val_loss: 8.4469
Epoch 67/8000

Epoch 00067: val_loss did not improve from 8.24040
 - 43s - loss: 9.1939 - val_loss: 8.8932
Epoch 68/8000

Epoch 00068: val_loss did not improve from 8.24040
 - 43s - loss: 9.0797 - val_loss: 9.8577
Epoch 69/8000

Epoch 00069: val_loss did not improve from 8.24040
 - 43s - loss: 9.1089 - val_loss: 10.2158
Epoch 70/8000

Epoch 00070: val_loss did not improve from 8.24040
 - 43s - loss: 9.1752 - val_loss: 9.5063
Epoch 71/8000

Epoch 00071: val_loss did not improve from 8.24040
 - 43s - loss: 9.2837 - val_loss: 8.7614
Epoch 72/8000

Epoch 00072: val_loss did not improve from 8.24040
 - 43s - loss: 9.1604 - val_loss: 10.1611
Epoch 73/8000

Epoch 00073: val_loss did not improve from 8.24040
 - 43s - loss: 9.1107 - val_loss: 10.2390
Epoch 74/8000

Epoch 00074: val_loss did not improve from 8.24040
 - 43s - loss: 9.4841 - val_loss: 10.5430
Epoch 75/8000

Epoch 00075: val_loss did not improve from 8.24040
 - 43s - loss: 9.5832 - val_loss: 8.3703
Epoch 76/8000

Epoch 00076: val_loss did not improve from 8.24040
 - 43s - loss: 9.2594 - val_loss: 8.6328
Epoch 77/8000

Epoch 00077: val_loss did not improve from 8.24040
 - 43s - loss: 9.1197 - val_loss: 9.9339
Epoch 78/8000

Epoch 00078: val_loss did not improve from 8.24040
 - 43s - loss: 9.3152 - val_loss: 10.2874
Epoch 79/8000

Epoch 00079: val_loss did not improve from 8.24040
 - 43s - loss: 9.1201 - val_loss: 8.3144
Epoch 80/8000

Epoch 00080: val_loss did not improve from 8.24040
 - 43s - loss: 9.3399 - val_loss: 8.5355
Epoch 81/8000

Epoch 00081: val_loss did not improve from 8.24040
 - 43s - loss: 9.4085 - val_loss: 8.5842
Epoch 82/8000

Epoch 00082: val_loss did not improve from 8.24040
 - 43s - loss: 9.5820 - val_loss: 9.7239
Epoch 83/8000

Epoch 00083: val_loss did not improve from 8.24040
 - 43s - loss: 9.2919 - val_loss: 9.6590
Epoch 84/8000

Epoch 00084: val_loss did not improve from 8.24040
 - 43s - loss: 9.3095 - val_loss: 9.1276
Epoch 85/8000

Epoch 00085: val_loss did not improve from 8.24040
 - 43s - loss: 9.2877 - val_loss: 9.2471
Epoch 86/8000

Epoch 00086: val_loss did not improve from 8.24040
 - 43s - loss: 9.4726 - val_loss: 9.4337
Epoch 87/8000

Epoch 00087: val_loss did not improve from 8.24040
 - 43s - loss: 9.5409 - val_loss: 9.9948
Epoch 88/8000

Epoch 00088: val_loss did not improve from 8.24040
 - 43s - loss: 10.9749 - val_loss: 12.7009
Epoch 89/8000

Epoch 00089: val_loss did not improve from 8.24040
 - 43s - loss: 9.3726 - val_loss: 8.8073
Epoch 90/8000

Epoch 00090: val_loss did not improve from 8.24040
 - 43s - loss: 9.0863 - val_loss: 9.6930
Epoch 91/8000

Epoch 00091: val_loss did not improve from 8.24040
 - 43s - loss: 9.1666 - val_loss: 8.8914
Epoch 92/8000

Epoch 00092: val_loss did not improve from 8.24040
 - 43s - loss: 9.1358 - val_loss: 9.0001
Epoch 93/8000

Epoch 00093: val_loss did not improve from 8.24040
 - 43s - loss: 9.5315 - val_loss: 9.2996
Epoch 94/8000

Epoch 00094: val_loss did not improve from 8.24040
 - 43s - loss: 9.2648 - val_loss: 9.9950
Epoch 95/8000

Epoch 00095: val_loss did not improve from 8.24040
 - 43s - loss: 9.2234 - val_loss: 11.3697
Epoch 96/8000

Epoch 00096: val_loss did not improve from 8.24040
 - 43s - loss: 9.4388 - val_loss: 9.2100
Epoch 97/8000

Epoch 00097: val_loss did not improve from 8.24040
 - 43s - loss: 9.4544 - val_loss: 9.2352
Epoch 98/8000

Epoch 00098: val_loss did not improve from 8.24040
 - 43s - loss: 9.3955 - val_loss: 10.1032
Epoch 99/8000

Epoch 00099: val_loss did not improve from 8.24040
 - 43s - loss: 9.5482 - val_loss: 11.0320
Epoch 100/8000

Epoch 00100: val_loss did not improve from 8.24040
 - 43s - loss: 9.6332 - val_loss: 8.9330
Epoch 101/8000

Epoch 00101: val_loss did not improve from 8.24040
 - 43s - loss: 9.1817 - val_loss: 9.5157
Epoch 102/8000

Epoch 00102: val_loss did not improve from 8.24040
 - 43s - loss: 9.5598 - val_loss: 10.4975
Epoch 103/8000

Epoch 00103: val_loss did not improve from 8.24040
 - 43s - loss: 10.1058 - val_loss: 9.0688
Epoch 104/8000

Epoch 00104: val_loss did not improve from 8.24040
 - 43s - loss: 9.8277 - val_loss: 10.6948
Epoch 105/8000

Epoch 00105: val_loss did not improve from 8.24040
 - 43s - loss: 9.4493 - val_loss: 9.7806
Epoch 106/8000

Epoch 00106: val_loss did not improve from 8.24040
 - 43s - loss: 9.4233 - val_loss: 8.7620
Epoch 107/8000

Epoch 00107: val_loss did not improve from 8.24040
 - 43s - loss: 9.3233 - val_loss: 9.0793
Epoch 108/8000

Epoch 00108: val_loss did not improve from 8.24040
 - 43s - loss: 9.1056 - val_loss: 8.8606
Epoch 109/8000

Epoch 00109: val_loss did not improve from 8.24040
 - 43s - loss: 9.2208 - val_loss: 9.7808
Epoch 110/8000

Epoch 00110: val_loss did not improve from 8.24040
 - 43s - loss: 9.6257 - val_loss: 8.2455
Epoch 111/8000

Epoch 00111: val_loss improved from 8.24040 to 7.87479, saving model to model_weights/model_2020-02-04_11-49-45.h5
 - 43s - loss: 9.0420 - val_loss: 7.8748
Epoch 112/8000

Epoch 00112: val_loss did not improve from 7.87479
 - 43s - loss: 9.2593 - val_loss: 8.0851
Epoch 113/8000

Epoch 00113: val_loss did not improve from 7.87479
 - 43s - loss: 9.2374 - val_loss: 9.6696
Epoch 114/8000

Epoch 00114: val_loss did not improve from 7.87479
 - 43s - loss: 9.5703 - val_loss: 9.2806
Epoch 115/8000

Epoch 00115: val_loss did not improve from 7.87479
 - 43s - loss: 9.8022 - val_loss: 8.6115
Epoch 116/8000

Epoch 00116: val_loss did not improve from 7.87479
 - 43s - loss: 9.3747 - val_loss: 11.1138
Epoch 117/8000

Epoch 00117: val_loss did not improve from 7.87479
 - 43s - loss: 9.4966 - val_loss: 10.3028
Epoch 118/8000

Epoch 00118: val_loss did not improve from 7.87479
 - 43s - loss: 9.3899 - val_loss: 9.4668
Epoch 119/8000

Epoch 00119: val_loss did not improve from 7.87479
 - 43s - loss: 9.4845 - val_loss: 10.4346
Epoch 120/8000

Epoch 00120: val_loss did not improve from 7.87479
 - 43s - loss: 10.0599 - val_loss: 9.2435
Epoch 121/8000

Epoch 00121: val_loss did not improve from 7.87479
 - 43s - loss: 9.9434 - val_loss: 9.6613
Epoch 122/8000

Epoch 00122: val_loss did not improve from 7.87479
 - 43s - loss: 9.6042 - val_loss: 10.2214
Epoch 123/8000

Epoch 00123: val_loss did not improve from 7.87479
 - 43s - loss: 9.4665 - val_loss: 10.6722
Epoch 124/8000

Epoch 00124: val_loss did not improve from 7.87479
 - 43s - loss: 10.3452 - val_loss: 9.7697
Epoch 125/8000

Epoch 00125: val_loss did not improve from 7.87479
 - 43s - loss: 10.2075 - val_loss: 9.6004
Epoch 126/8000

Epoch 00126: val_loss did not improve from 7.87479
 - 43s - loss: 9.9913 - val_loss: 9.2967
Epoch 127/8000

Epoch 00127: val_loss did not improve from 7.87479
 - 43s - loss: 10.1083 - val_loss: 10.6469
Epoch 128/8000

Epoch 00128: val_loss did not improve from 7.87479
 - 43s - loss: 9.7376 - val_loss: 9.3855
Epoch 129/8000

Epoch 00129: val_loss did not improve from 7.87479
 - 43s - loss: 9.8777 - val_loss: 10.6139
Epoch 130/8000

Epoch 00130: val_loss did not improve from 7.87479
 - 43s - loss: 9.5087 - val_loss: 8.9664
Epoch 131/8000

Epoch 00131: val_loss did not improve from 7.87479
 - 43s - loss: 9.8203 - val_loss: 9.5987
Epoch 132/8000

Epoch 00132: val_loss did not improve from 7.87479
 - 43s - loss: 10.1150 - val_loss: 10.1913
Epoch 133/8000

Epoch 00133: val_loss did not improve from 7.87479
 - 43s - loss: 9.7328 - val_loss: 13.0160
Epoch 134/8000

Epoch 00134: val_loss did not improve from 7.87479
 - 43s - loss: 22.1270 - val_loss: 19.2978
Epoch 135/8000

Epoch 00135: val_loss did not improve from 7.87479
 - 43s - loss: 17.0872 - val_loss: 15.8880
Epoch 136/8000

Epoch 00136: val_loss did not improve from 7.87479
 - 43s - loss: 15.3096 - val_loss: 15.0386
Epoch 137/8000

Epoch 00137: val_loss did not improve from 7.87479
 - 43s - loss: 14.5753 - val_loss: 14.2293
Epoch 138/8000

Epoch 00138: val_loss did not improve from 7.87479
 - 43s - loss: 14.3772 - val_loss: 14.1505
Epoch 139/8000

Epoch 00139: val_loss did not improve from 7.87479
 - 43s - loss: 13.8574 - val_loss: 13.6288
Epoch 140/8000

Epoch 00140: val_loss did not improve from 7.87479
 - 43s - loss: 13.4677 - val_loss: 13.5373
Epoch 141/8000

Epoch 00141: val_loss did not improve from 7.87479
 - 43s - loss: 13.3012 - val_loss: 13.0081
Epoch 142/8000

Epoch 00142: val_loss did not improve from 7.87479
 - 43s - loss: 13.0715 - val_loss: 14.1035
Epoch 143/8000

Epoch 00143: val_loss did not improve from 7.87479
 - 43s - loss: 13.0315 - val_loss: 12.6279
Epoch 144/8000

Epoch 00144: val_loss did not improve from 7.87479
 - 43s - loss: 12.8690 - val_loss: 12.9570
Epoch 145/8000

Epoch 00145: val_loss did not improve from 7.87479
 - 43s - loss: 12.5830 - val_loss: 12.5611
Epoch 146/8000

Epoch 00146: val_loss did not improve from 7.87479
 - 43s - loss: 12.2110 - val_loss: 12.1094
Epoch 147/8000

Epoch 00147: val_loss did not improve from 7.87479
 - 43s - loss: 12.1369 - val_loss: 12.0007
Epoch 148/8000

Epoch 00148: val_loss did not improve from 7.87479
 - 43s - loss: 11.9298 - val_loss: 12.4497
Epoch 149/8000

Epoch 00149: val_loss did not improve from 7.87479
 - 43s - loss: 11.8564 - val_loss: 11.6446
Epoch 150/8000

Epoch 00150: val_loss did not improve from 7.87479
 - 43s - loss: 11.5049 - val_loss: 11.9339
Epoch 151/8000

Epoch 00151: val_loss did not improve from 7.87479
 - 43s - loss: 11.4638 - val_loss: 10.9617
Epoch 152/8000

Epoch 00152: val_loss did not improve from 7.87479
 - 43s - loss: 11.5382 - val_loss: 11.5757
Epoch 153/8000

Epoch 00153: val_loss did not improve from 7.87479
 - 43s - loss: 11.2124 - val_loss: 10.8962
Epoch 154/8000

Epoch 00154: val_loss did not improve from 7.87479
 - 43s - loss: 11.0790 - val_loss: 10.9261
Epoch 155/8000

Epoch 00155: val_loss did not improve from 7.87479
 - 43s - loss: 10.9184 - val_loss: 10.5616
Epoch 156/8000

Epoch 00156: val_loss did not improve from 7.87479
 - 43s - loss: 11.0506 - val_loss: 10.7782
Epoch 157/8000

Epoch 00157: val_loss did not improve from 7.87479
 - 43s - loss: 10.9202 - val_loss: 10.4625
Epoch 158/8000

Epoch 00158: val_loss did not improve from 7.87479
 - 43s - loss: 11.1224 - val_loss: 10.6893
Epoch 159/8000

Epoch 00159: val_loss did not improve from 7.87479
 - 43s - loss: 10.7248 - val_loss: 11.3283
Epoch 160/8000

Epoch 00160: val_loss did not improve from 7.87479
 - 43s - loss: 10.8255 - val_loss: 11.0059
Epoch 161/8000

Epoch 00161: val_loss did not improve from 7.87479
 - 43s - loss: 10.6166 - val_loss: 10.6420
Epoch 162/8000

Epoch 00162: val_loss did not improve from 7.87479
 - 43s - loss: 10.7440 - val_loss: 10.3182
Epoch 163/8000

Epoch 00163: val_loss did not improve from 7.87479
 - 43s - loss: 10.6028 - val_loss: 10.1061
Epoch 164/8000

Epoch 00164: val_loss did not improve from 7.87479
 - 43s - loss: 10.8196 - val_loss: 11.1718
Epoch 165/8000

Epoch 00165: val_loss did not improve from 7.87479
 - 43s - loss: 10.5384 - val_loss: 10.8768
Epoch 166/8000

Epoch 00166: val_loss did not improve from 7.87479
 - 43s - loss: 10.5143 - val_loss: 11.1410
Epoch 167/8000

Epoch 00167: val_loss did not improve from 7.87479
 - 43s - loss: 10.4746 - val_loss: 10.2582
Epoch 168/8000

Epoch 00168: val_loss did not improve from 7.87479
 - 43s - loss: 10.7944 - val_loss: 11.2738
Epoch 169/8000

Epoch 00169: val_loss did not improve from 7.87479
 - 43s - loss: 10.8595 - val_loss: 10.4724
Epoch 170/8000

Epoch 00170: val_loss did not improve from 7.87479
 - 43s - loss: 11.2071 - val_loss: 11.0819
Epoch 171/8000

Epoch 00171: val_loss did not improve from 7.87479
 - 43s - loss: 10.9124 - val_loss: 11.3939
Epoch 172/8000

Epoch 00172: val_loss did not improve from 7.87479
 - 43s - loss: 10.6265 - val_loss: 10.1507
Epoch 173/8000

Epoch 00173: val_loss did not improve from 7.87479
 - 43s - loss: 10.4493 - val_loss: 9.7932
Epoch 174/8000

Epoch 00174: val_loss did not improve from 7.87479
 - 43s - loss: 10.8627 - val_loss: 11.1672
Epoch 175/8000

Epoch 00175: val_loss did not improve from 7.87479
 - 43s - loss: 10.3821 - val_loss: 10.0089
Epoch 176/8000

Epoch 00176: val_loss did not improve from 7.87479
 - 43s - loss: 10.4356 - val_loss: 10.9952
Epoch 177/8000

Epoch 00177: val_loss did not improve from 7.87479
 - 43s - loss: 10.3447 - val_loss: 13.2353
Epoch 178/8000

Epoch 00178: val_loss did not improve from 7.87479
 - 43s - loss: 10.7204 - val_loss: 10.2615
Epoch 179/8000

Epoch 00179: val_loss did not improve from 7.87479
 - 43s - loss: 10.0411 - val_loss: 10.0659
Epoch 180/8000

Epoch 00180: val_loss did not improve from 7.87479
 - 43s - loss: 10.4794 - val_loss: 10.7952
Epoch 181/8000

Epoch 00181: val_loss did not improve from 7.87479
 - 43s - loss: 10.7150 - val_loss: 12.1343
Epoch 182/8000

Epoch 00182: val_loss did not improve from 7.87479
 - 43s - loss: 11.0000 - val_loss: 10.5438
Epoch 183/8000

Epoch 00183: val_loss did not improve from 7.87479
 - 43s - loss: 11.1995 - val_loss: 10.2501
Epoch 184/8000

Epoch 00184: val_loss did not improve from 7.87479
 - 43s - loss: 10.7404 - val_loss: 10.6547
Epoch 185/8000

Epoch 00185: val_loss did not improve from 7.87479
 - 43s - loss: 10.6671 - val_loss: 10.3846
Epoch 186/8000

Epoch 00186: val_loss did not improve from 7.87479
 - 43s - loss: 10.6475 - val_loss: 9.8773
Epoch 187/8000

Epoch 00187: val_loss did not improve from 7.87479
 - 43s - loss: 10.5584 - val_loss: 10.4412
Epoch 188/8000

Epoch 00188: val_loss did not improve from 7.87479
 - 43s - loss: 10.6098 - val_loss: 10.6618
Epoch 189/8000

Epoch 00189: val_loss did not improve from 7.87479
 - 43s - loss: 10.6720 - val_loss: 9.9636
Epoch 190/8000

Epoch 00190: val_loss did not improve from 7.87479
 - 43s - loss: 11.1831 - val_loss: 10.2683
Epoch 191/8000

Epoch 00191: val_loss did not improve from 7.87479
 - 43s - loss: 10.3923 - val_loss: 10.8040
Epoch 192/8000

Epoch 00192: val_loss did not improve from 7.87479
 - 43s - loss: 10.4956 - val_loss: 9.9811
Epoch 193/8000

Epoch 00193: val_loss did not improve from 7.87479
 - 43s - loss: 10.4993 - val_loss: 11.8861
Epoch 194/8000

Epoch 00194: val_loss did not improve from 7.87479
 - 43s - loss: 10.6399 - val_loss: 10.6421
Epoch 195/8000

Epoch 00195: val_loss did not improve from 7.87479
 - 43s - loss: 10.4756 - val_loss: 11.6670
Epoch 196/8000

Epoch 00196: val_loss did not improve from 7.87479
 - 43s - loss: 10.3505 - val_loss: 9.6303
Epoch 197/8000

Epoch 00197: val_loss did not improve from 7.87479
 - 43s - loss: 10.3329 - val_loss: 10.1311
Epoch 198/8000

Epoch 00198: val_loss did not improve from 7.87479
 - 43s - loss: 10.2548 - val_loss: 10.6384
Epoch 199/8000

Epoch 00199: val_loss did not improve from 7.87479
 - 43s - loss: 10.1564 - val_loss: 9.8184
Epoch 200/8000

Epoch 00200: val_loss did not improve from 7.87479
 - 43s - loss: 10.3119 - val_loss: 10.0165
Epoch 201/8000

Epoch 00201: val_loss did not improve from 7.87479
 - 43s - loss: 10.4836 - val_loss: 10.8880
Epoch 202/8000

Epoch 00202: val_loss did not improve from 7.87479
 - 43s - loss: 10.8286 - val_loss: 10.5392
Epoch 203/8000

Epoch 00203: val_loss did not improve from 7.87479
 - 43s - loss: 11.1419 - val_loss: 11.2531
Epoch 204/8000

Epoch 00204: val_loss did not improve from 7.87479
 - 43s - loss: 11.3345 - val_loss: 10.5174
Epoch 205/8000

Epoch 00205: val_loss did not improve from 7.87479
 - 43s - loss: 10.8805 - val_loss: 10.6896
Epoch 206/8000

Epoch 00206: val_loss did not improve from 7.87479
 - 43s - loss: 10.2938 - val_loss: 12.2507
Epoch 207/8000

Epoch 00207: val_loss did not improve from 7.87479
 - 43s - loss: 10.3875 - val_loss: 9.9392
Epoch 208/8000

Epoch 00208: val_loss did not improve from 7.87479
 - 43s - loss: 10.7627 - val_loss: 10.0986
Epoch 209/8000

Epoch 00209: val_loss did not improve from 7.87479
 - 43s - loss: 10.7922 - val_loss: 11.2857
Epoch 210/8000

Epoch 00210: val_loss did not improve from 7.87479
 - 43s - loss: 10.5420 - val_loss: 10.5593
Epoch 211/8000

Epoch 00211: val_loss did not improve from 7.87479
 - 43s - loss: 10.5009 - val_loss: 10.0312
Epoch 00211: early stopping
