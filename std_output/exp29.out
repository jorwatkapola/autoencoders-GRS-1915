2019-12-20 09:06:12.419954: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX
2019-12-20 09:06:12.546026: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:897] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-12-20 09:06:12.546580: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1405] Found device 0 with properties: 
name: TITAN Xp major: 6 minor: 1 memoryClockRate(GHz): 1.582
pciBusID: 0000:01:00.0
totalMemory: 11.91GiB freeMemory: 11.73GiB
2019-12-20 09:06:12.546597: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1484] Adding visible gpu devices: 0
2019-12-20 09:06:12.780165: I tensorflow/core/common_runtime/gpu/gpu_device.cc:965] Device interconnect StreamExecutor with strength 1 edge matrix:
2019-12-20 09:06:12.780213: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971]      0 
2019-12-20 09:06:12.780222: I tensorflow/core/common_runtime/gpu/gpu_device.cc:984] 0:   N 
2019-12-20 09:06:12.780487: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1097] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 11346 MB memory) -> physical GPU (device: 0, name: TITAN Xp, pci bus id: 0000:01:00.0, compute capability: 6.1)
2019-12-20 09:06:13.048448: I tensorflow/core/kernels/cuda_solvers.cc:159] Creating CudaSolver handles for stream 0x55926f632950
Train on 32962 samples, validate on 1735 samples
Epoch 1/250
 - 57s - loss: 0.3553 - val_loss: 0.2890

Epoch 00001: val_loss improved from inf to 0.28905, saving model to lstm_autoencoder_2019-12-20_09-06-11.h5
Epoch 2/250
 - 55s - loss: 0.2876 - val_loss: 0.2626

Epoch 00002: val_loss improved from 0.28905 to 0.26259, saving model to lstm_autoencoder_2019-12-20_09-06-11.h5
Epoch 3/250
 - 56s - loss: 0.2505 - val_loss: 0.2275

Epoch 00003: val_loss improved from 0.26259 to 0.22748, saving model to lstm_autoencoder_2019-12-20_09-06-11.h5
Epoch 4/250
 - 57s - loss: 0.2356 - val_loss: 0.2419

Epoch 00004: val_loss did not improve from 0.22748
Epoch 5/250
 - 56s - loss: 0.2300 - val_loss: 0.2433

Epoch 00005: val_loss did not improve from 0.22748
Epoch 6/250
 - 57s - loss: 0.2241 - val_loss: 0.2752

Epoch 00006: val_loss did not improve from 0.22748
Epoch 7/250
 - 57s - loss: 0.2199 - val_loss: 0.2031

Epoch 00007: val_loss improved from 0.22748 to 0.20312, saving model to lstm_autoencoder_2019-12-20_09-06-11.h5
Epoch 8/250
 - 57s - loss: 0.2096 - val_loss: 0.2023

Epoch 00008: val_loss improved from 0.20312 to 0.20233, saving model to lstm_autoencoder_2019-12-20_09-06-11.h5
Epoch 9/250
 - 56s - loss: 0.2068 - val_loss: 0.2001

Epoch 00009: val_loss improved from 0.20233 to 0.20013, saving model to lstm_autoencoder_2019-12-20_09-06-11.h5
Epoch 10/250
 - 56s - loss: 0.2028 - val_loss: 0.1999

Epoch 00010: val_loss improved from 0.20013 to 0.19985, saving model to lstm_autoencoder_2019-12-20_09-06-11.h5
Epoch 11/250
 - 57s - loss: 0.2001 - val_loss: 0.1924

Epoch 00011: val_loss improved from 0.19985 to 0.19244, saving model to lstm_autoencoder_2019-12-20_09-06-11.h5
Epoch 12/250
 - 56s - loss: 0.1954 - val_loss: 0.1927

Epoch 00012: val_loss did not improve from 0.19244
Epoch 13/250
 - 56s - loss: 0.1919 - val_loss: 0.1931

Epoch 00013: val_loss did not improve from 0.19244
Epoch 14/250
 - 56s - loss: 0.1905 - val_loss: 0.1833

Epoch 00014: val_loss improved from 0.19244 to 0.18328, saving model to lstm_autoencoder_2019-12-20_09-06-11.h5
Epoch 15/250
 - 56s - loss: 0.1863 - val_loss: 0.1823

Epoch 00015: val_loss improved from 0.18328 to 0.18234, saving model to lstm_autoencoder_2019-12-20_09-06-11.h5
Epoch 16/250
 - 57s - loss: 0.1850 - val_loss: 0.2036

Epoch 00016: val_loss did not improve from 0.18234
Epoch 17/250
 - 57s - loss: 0.1829 - val_loss: 0.2276

Epoch 00017: val_loss did not improve from 0.18234
Epoch 18/250
 - 57s - loss: 0.1851 - val_loss: 0.1768

Epoch 00018: val_loss improved from 0.18234 to 0.17678, saving model to lstm_autoencoder_2019-12-20_09-06-11.h5
Epoch 19/250
 - 57s - loss: 0.1799 - val_loss: 0.1812

Epoch 00019: val_loss did not improve from 0.17678
Epoch 20/250
 - 57s - loss: 0.1806 - val_loss: 0.1873

Epoch 00020: val_loss did not improve from 0.17678
Epoch 21/250
 - 56s - loss: 0.1755 - val_loss: 0.1759

Epoch 00021: val_loss improved from 0.17678 to 0.17585, saving model to lstm_autoencoder_2019-12-20_09-06-11.h5
Epoch 22/250
 - 56s - loss: 0.1767 - val_loss: 0.1990

Epoch 00022: val_loss did not improve from 0.17585
Epoch 23/250
 - 56s - loss: 0.1730 - val_loss: 0.1665

Epoch 00023: val_loss improved from 0.17585 to 0.16647, saving model to lstm_autoencoder_2019-12-20_09-06-11.h5
Epoch 24/250
 - 56s - loss: 0.1725 - val_loss: 0.1779

Epoch 00024: val_loss did not improve from 0.16647
Epoch 25/250
 - 57s - loss: 0.1722 - val_loss: 0.1668

Epoch 00025: val_loss did not improve from 0.16647
Epoch 26/250
 - 56s - loss: 0.1689 - val_loss: 0.2074

Epoch 00026: val_loss did not improve from 0.16647
Epoch 27/250
 - 56s - loss: 0.1703 - val_loss: 0.1625

Epoch 00027: val_loss improved from 0.16647 to 0.16250, saving model to lstm_autoencoder_2019-12-20_09-06-11.h5
Epoch 28/250
 - 56s - loss: 0.1674 - val_loss: 0.1594

Epoch 00028: val_loss improved from 0.16250 to 0.15938, saving model to lstm_autoencoder_2019-12-20_09-06-11.h5
Epoch 29/250
 - 56s - loss: 0.1651 - val_loss: 0.1839

Epoch 00029: val_loss did not improve from 0.15938
Epoch 30/250
 - 56s - loss: 0.1667 - val_loss: 0.1687

Epoch 00030: val_loss did not improve from 0.15938
Epoch 31/250
 - 57s - loss: 0.1637 - val_loss: 0.1665

Epoch 00031: val_loss did not improve from 0.15938
Epoch 32/250
 - 57s - loss: 0.1634 - val_loss: 0.1780

Epoch 00032: val_loss did not improve from 0.15938
Epoch 33/250
 - 56s - loss: 0.1623 - val_loss: 0.1666

Epoch 00033: val_loss did not improve from 0.15938
Epoch 34/250
 - 57s - loss: 0.1625 - val_loss: 0.1684

Epoch 00034: val_loss did not improve from 0.15938
Epoch 35/250
 - 56s - loss: 0.1597 - val_loss: 0.1646

Epoch 00035: val_loss did not improve from 0.15938
Epoch 36/250
 - 57s - loss: 0.1616 - val_loss: 0.1664

Epoch 00036: val_loss did not improve from 0.15938
Epoch 37/250
 - 57s - loss: 0.1598 - val_loss: 0.1533

Epoch 00037: val_loss improved from 0.15938 to 0.15330, saving model to lstm_autoencoder_2019-12-20_09-06-11.h5
Epoch 38/250
 - 56s - loss: 0.1581 - val_loss: 0.1575

Epoch 00038: val_loss did not improve from 0.15330
Epoch 39/250
 - 56s - loss: 0.1587 - val_loss: 0.1565

Epoch 00039: val_loss did not improve from 0.15330
Epoch 40/250
 - 56s - loss: 0.1583 - val_loss: 0.1593

Epoch 00040: val_loss did not improve from 0.15330
Epoch 41/250
 - 56s - loss: 0.1564 - val_loss: 0.2225

Epoch 00041: val_loss did not improve from 0.15330
Epoch 42/250
 - 56s - loss: 0.1579 - val_loss: 0.1732

Epoch 00042: val_loss did not improve from 0.15330
Epoch 43/250
 - 56s - loss: 0.1557 - val_loss: 0.1564

Epoch 00043: val_loss did not improve from 0.15330
Epoch 44/250
 - 56s - loss: 0.1565 - val_loss: 0.1550

Epoch 00044: val_loss did not improve from 0.15330
Epoch 45/250
 - 56s - loss: 0.1553 - val_loss: 0.1605

Epoch 00045: val_loss did not improve from 0.15330
Epoch 46/250
 - 56s - loss: 0.1540 - val_loss: 0.1530

Epoch 00046: val_loss improved from 0.15330 to 0.15297, saving model to lstm_autoencoder_2019-12-20_09-06-11.h5
Epoch 47/250
 - 56s - loss: 0.1535 - val_loss: 0.1491

Epoch 00047: val_loss improved from 0.15297 to 0.14905, saving model to lstm_autoencoder_2019-12-20_09-06-11.h5
Epoch 48/250
 - 56s - loss: 0.1526 - val_loss: 0.1755

Epoch 00048: val_loss did not improve from 0.14905
Epoch 49/250
 - 57s - loss: 0.1538 - val_loss: 0.1580

Epoch 00049: val_loss did not improve from 0.14905
Epoch 50/250
 - 56s - loss: 0.1521 - val_loss: 0.1699

Epoch 00050: val_loss did not improve from 0.14905
Epoch 51/250
 - 56s - loss: 0.1541 - val_loss: 0.1792

Epoch 00051: val_loss did not improve from 0.14905
Epoch 52/250
 - 57s - loss: 0.1498 - val_loss: 0.1488

Epoch 00052: val_loss improved from 0.14905 to 0.14882, saving model to lstm_autoencoder_2019-12-20_09-06-11.h5
Epoch 53/250
 - 57s - loss: 0.1517 - val_loss: 0.1621

Epoch 00053: val_loss did not improve from 0.14882
Epoch 54/250
 - 56s - loss: 0.1505 - val_loss: 0.1539

Epoch 00054: val_loss did not improve from 0.14882
Epoch 55/250
 - 57s - loss: 0.1504 - val_loss: 0.1495

Epoch 00055: val_loss did not improve from 0.14882
Epoch 56/250
 - 56s - loss: 0.1494 - val_loss: 0.1687

Epoch 00056: val_loss did not improve from 0.14882
Epoch 57/250
 - 56s - loss: 0.1504 - val_loss: 0.1522

Epoch 00057: val_loss did not improve from 0.14882
Epoch 58/250
 - 56s - loss: 0.1490 - val_loss: 0.1466

Epoch 00058: val_loss improved from 0.14882 to 0.14663, saving model to lstm_autoencoder_2019-12-20_09-06-11.h5
Epoch 59/250
 - 56s - loss: 0.1485 - val_loss: 0.1718

Epoch 00059: val_loss did not improve from 0.14663
Epoch 60/250
 - 56s - loss: 0.1507 - val_loss: 0.1512

Epoch 00060: val_loss did not improve from 0.14663
Epoch 61/250
 - 57s - loss: 0.1478 - val_loss: 0.1682

Epoch 00061: val_loss did not improve from 0.14663
Epoch 62/250
 - 56s - loss: 0.1519 - val_loss: 0.1480

Epoch 00062: val_loss did not improve from 0.14663
Epoch 63/250
 - 57s - loss: 0.1529 - val_loss: 0.1574

Epoch 00063: val_loss did not improve from 0.14663
Epoch 64/250
 - 56s - loss: 0.1503 - val_loss: 0.1508

Epoch 00064: val_loss did not improve from 0.14663
Epoch 65/250
 - 56s - loss: 0.1526 - val_loss: 0.1477

Epoch 00065: val_loss did not improve from 0.14663
Epoch 66/250
 - 56s - loss: 0.1513 - val_loss: 0.1538

Epoch 00066: val_loss did not improve from 0.14663
Epoch 67/250
 - 56s - loss: 0.1496 - val_loss: 0.1468

Epoch 00067: val_loss did not improve from 0.14663
Epoch 68/250
 - 57s - loss: 0.1485 - val_loss: 0.1444

Epoch 00068: val_loss improved from 0.14663 to 0.14440, saving model to lstm_autoencoder_2019-12-20_09-06-11.h5
Epoch 69/250
 - 57s - loss: 0.1489 - val_loss: 0.1471

Epoch 00069: val_loss did not improve from 0.14440
Epoch 70/250
 - 57s - loss: 0.1461 - val_loss: 0.1440

Epoch 00070: val_loss improved from 0.14440 to 0.14395, saving model to lstm_autoencoder_2019-12-20_09-06-11.h5
Epoch 71/250
 - 56s - loss: 0.1461 - val_loss: 0.1582

Epoch 00071: val_loss did not improve from 0.14395
Epoch 72/250
 - 56s - loss: 0.1476 - val_loss: 0.1514

Epoch 00072: val_loss did not improve from 0.14395
Epoch 73/250
 - 57s - loss: 0.1510 - val_loss: 0.1768

Epoch 00073: val_loss did not improve from 0.14395
Epoch 74/250
 - 56s - loss: 0.1469 - val_loss: 0.1560

Epoch 00074: val_loss did not improve from 0.14395
Epoch 75/250
 - 57s - loss: 0.1462 - val_loss: 0.1437

Epoch 00075: val_loss improved from 0.14395 to 0.14366, saving model to lstm_autoencoder_2019-12-20_09-06-11.h5
Epoch 76/250
 - 56s - loss: 0.1455 - val_loss: 0.1423

Epoch 00076: val_loss improved from 0.14366 to 0.14230, saving model to lstm_autoencoder_2019-12-20_09-06-11.h5
Epoch 77/250
 - 56s - loss: 0.1476 - val_loss: 0.1480

Epoch 00077: val_loss did not improve from 0.14230
Epoch 78/250
 - 57s - loss: 0.1472 - val_loss: 0.1466

Epoch 00078: val_loss did not improve from 0.14230
Epoch 79/250
 - 56s - loss: 0.1462 - val_loss: 0.1499

Epoch 00079: val_loss did not improve from 0.14230
Epoch 80/250
 - 57s - loss: 0.1469 - val_loss: 0.1443

Epoch 00080: val_loss did not improve from 0.14230
Epoch 81/250
 - 56s - loss: 0.1449 - val_loss: 0.1597

Epoch 00081: val_loss did not improve from 0.14230
Epoch 82/250
 - 56s - loss: 0.1500 - val_loss: 0.1495

Epoch 00082: val_loss did not improve from 0.14230
Epoch 83/250
 - 56s - loss: 0.1449 - val_loss: 0.1448

Epoch 00083: val_loss did not improve from 0.14230
Epoch 84/250
 - 56s - loss: 0.1489 - val_loss: 0.1489

Epoch 00084: val_loss did not improve from 0.14230
Epoch 85/250
 - 56s - loss: 0.1477 - val_loss: 0.1536

Epoch 00085: val_loss did not improve from 0.14230
Epoch 86/250
 - 56s - loss: 0.1491 - val_loss: 0.1465

Epoch 00086: val_loss did not improve from 0.14230
Epoch 87/250
 - 57s - loss: 0.1455 - val_loss: 0.1443

Epoch 00087: val_loss did not improve from 0.14230
Epoch 88/250
 - 56s - loss: 0.1425 - val_loss: 0.1470

Epoch 00088: val_loss did not improve from 0.14230
Epoch 89/250
 - 56s - loss: 0.1453 - val_loss: 0.1443

Epoch 00089: val_loss did not improve from 0.14230
Epoch 90/250
 - 56s - loss: 0.1457 - val_loss: 0.1543

Epoch 00090: val_loss did not improve from 0.14230
Epoch 91/250
 - 56s - loss: 0.1463 - val_loss: 0.1503

Epoch 00091: val_loss did not improve from 0.14230
Epoch 92/250
 - 57s - loss: 0.1471 - val_loss: 0.1513

Epoch 00092: val_loss did not improve from 0.14230
Epoch 93/250
 - 57s - loss: 0.1462 - val_loss: 0.1476

Epoch 00093: val_loss did not improve from 0.14230
Epoch 94/250
 - 56s - loss: 0.1452 - val_loss: 0.1447

Epoch 00094: val_loss did not improve from 0.14230
Epoch 95/250
 - 56s - loss: 0.1451 - val_loss: 0.1444

Epoch 00095: val_loss did not improve from 0.14230
Epoch 96/250
 - 57s - loss: 0.1445 - val_loss: 0.1407

Epoch 00096: val_loss improved from 0.14230 to 0.14066, saving model to lstm_autoencoder_2019-12-20_09-06-11.h5
Epoch 97/250
 - 56s - loss: 0.1431 - val_loss: 0.1401

Epoch 00097: val_loss improved from 0.14066 to 0.14009, saving model to lstm_autoencoder_2019-12-20_09-06-11.h5
Epoch 98/250
 - 57s - loss: 0.1453 - val_loss: 0.1414

Epoch 00098: val_loss did not improve from 0.14009
Epoch 99/250
 - 56s - loss: 0.1417 - val_loss: 0.1502

Epoch 00099: val_loss did not improve from 0.14009
Epoch 100/250
 - 56s - loss: 0.1438 - val_loss: 0.1420

Epoch 00100: val_loss did not improve from 0.14009
Epoch 101/250
 - 56s - loss: 0.1462 - val_loss: 0.1453

Epoch 00101: val_loss did not improve from 0.14009
Epoch 102/250
 - 56s - loss: 0.1459 - val_loss: 0.1475

Epoch 00102: val_loss did not improve from 0.14009
Epoch 103/250
 - 56s - loss: 0.1441 - val_loss: 0.1431

Epoch 00103: val_loss did not improve from 0.14009
Epoch 104/250
 - 56s - loss: 0.1400 - val_loss: 0.1403

Epoch 00104: val_loss did not improve from 0.14009
Epoch 105/250
 - 57s - loss: 0.1406 - val_loss: 0.1470

Epoch 00105: val_loss did not improve from 0.14009
Epoch 106/250
 - 56s - loss: 0.1449 - val_loss: 0.1726

Epoch 00106: val_loss did not improve from 0.14009
Epoch 107/250
 - 56s - loss: 0.1419 - val_loss: 0.1406

Epoch 00107: val_loss did not improve from 0.14009
Epoch 108/250
 - 57s - loss: 0.1405 - val_loss: 0.1476

Epoch 00108: val_loss did not improve from 0.14009
Epoch 109/250
 - 56s - loss: 0.1446 - val_loss: 0.1378

Epoch 00109: val_loss improved from 0.14009 to 0.13780, saving model to lstm_autoencoder_2019-12-20_09-06-11.h5
Epoch 110/250
 - 57s - loss: 0.1438 - val_loss: 0.1421

Epoch 00110: val_loss did not improve from 0.13780
Epoch 111/250
 - 56s - loss: 0.1421 - val_loss: 0.1415

Epoch 00111: val_loss did not improve from 0.13780
Epoch 112/250
 - 57s - loss: 0.1409 - val_loss: 0.1434

Epoch 00112: val_loss did not improve from 0.13780
Epoch 113/250
 - 56s - loss: 0.1415 - val_loss: 0.1717

Epoch 00113: val_loss did not improve from 0.13780
Epoch 114/250
 - 56s - loss: 0.1449 - val_loss: 0.1409

Epoch 00114: val_loss did not improve from 0.13780
Epoch 115/250
 - 56s - loss: 0.1387 - val_loss: 0.1381

Epoch 00115: val_loss did not improve from 0.13780
Epoch 116/250
 - 56s - loss: 0.1432 - val_loss: 0.1460

Epoch 00116: val_loss did not improve from 0.13780
Epoch 117/250
 - 56s - loss: 0.1381 - val_loss: 0.1341

Epoch 00117: val_loss improved from 0.13780 to 0.13414, saving model to lstm_autoencoder_2019-12-20_09-06-11.h5
Epoch 118/250
 - 57s - loss: 0.1380 - val_loss: 0.1438

Epoch 00118: val_loss did not improve from 0.13414
Epoch 119/250
 - 57s - loss: 0.1466 - val_loss: 0.1461

Epoch 00119: val_loss did not improve from 0.13414
Epoch 120/250
 - 56s - loss: 0.1411 - val_loss: 0.1585

Epoch 00120: val_loss did not improve from 0.13414
Epoch 121/250
 - 56s - loss: 0.1391 - val_loss: 0.1340

Epoch 00121: val_loss improved from 0.13414 to 0.13400, saving model to lstm_autoencoder_2019-12-20_09-06-11.h5
Epoch 122/250
 - 56s - loss: 0.1389 - val_loss: 0.1737

Epoch 00122: val_loss did not improve from 0.13400
Epoch 123/250
 - 56s - loss: 0.1396 - val_loss: 0.1501

Epoch 00123: val_loss did not improve from 0.13400
Epoch 124/250
 - 56s - loss: 0.1376 - val_loss: 0.1443

Epoch 00124: val_loss did not improve from 0.13400
Epoch 125/250
 - 57s - loss: 0.1366 - val_loss: 0.1587

Epoch 00125: val_loss did not improve from 0.13400
Epoch 126/250
 - 56s - loss: 0.1382 - val_loss: 0.1657

Epoch 00126: val_loss did not improve from 0.13400
Epoch 127/250
 - 56s - loss: 0.1436 - val_loss: 0.1443

Epoch 00127: val_loss did not improve from 0.13400
Epoch 128/250
 - 56s - loss: 0.1392 - val_loss: 0.1759

Epoch 00128: val_loss did not improve from 0.13400
Epoch 129/250
 - 56s - loss: 0.1399 - val_loss: 0.1343

Epoch 00129: val_loss did not improve from 0.13400
Epoch 130/250
 - 57s - loss: 0.1371 - val_loss: 0.1317

Epoch 00130: val_loss improved from 0.13400 to 0.13175, saving model to lstm_autoencoder_2019-12-20_09-06-11.h5
Epoch 131/250
 - 56s - loss: 0.1369 - val_loss: 0.1372

Epoch 00131: val_loss did not improve from 0.13175
Epoch 132/250
 - 56s - loss: 0.1363 - val_loss: 0.1341

Epoch 00132: val_loss did not improve from 0.13175
Epoch 133/250
 - 56s - loss: 0.1368 - val_loss: 0.1380

Epoch 00133: val_loss did not improve from 0.13175
Epoch 134/250
 - 57s - loss: 0.1401 - val_loss: 0.1377

Epoch 00134: val_loss did not improve from 0.13175
Epoch 135/250
 - 57s - loss: 0.1467 - val_loss: 0.1351

Epoch 00135: val_loss did not improve from 0.13175
Epoch 136/250
 - 57s - loss: 0.1359 - val_loss: 0.1291

Epoch 00136: val_loss improved from 0.13175 to 0.12909, saving model to lstm_autoencoder_2019-12-20_09-06-11.h5
Epoch 137/250
 - 56s - loss: 0.1359 - val_loss: 0.1426

Epoch 00137: val_loss did not improve from 0.12909
Epoch 138/250
 - 56s - loss: 0.1373 - val_loss: 0.1342

Epoch 00138: val_loss did not improve from 0.12909
Epoch 139/250
 - 56s - loss: 0.1377 - val_loss: 0.1492

Epoch 00139: val_loss did not improve from 0.12909
Epoch 140/250
 - 56s - loss: 0.1361 - val_loss: 0.1384

Epoch 00140: val_loss did not improve from 0.12909
Epoch 141/250
 - 56s - loss: 0.1374 - val_loss: 0.1341

Epoch 00141: val_loss did not improve from 0.12909
Epoch 142/250
 - 56s - loss: 0.1379 - val_loss: 0.1387

Epoch 00142: val_loss did not improve from 0.12909
Epoch 143/250
 - 57s - loss: 0.1351 - val_loss: 0.1338

Epoch 00143: val_loss did not improve from 0.12909
Epoch 144/250
 - 56s - loss: 0.1360 - val_loss: 0.1302

Epoch 00144: val_loss did not improve from 0.12909
Epoch 145/250
 - 56s - loss: 0.1348 - val_loss: 0.1331

Epoch 00145: val_loss did not improve from 0.12909
Epoch 146/250
 - 56s - loss: 0.1351 - val_loss: 0.1312

Epoch 00146: val_loss did not improve from 0.12909
Epoch 147/250
 - 56s - loss: 0.1356 - val_loss: 0.1391

Epoch 00147: val_loss did not improve from 0.12909
Epoch 148/250
 - 56s - loss: 0.1352 - val_loss: 0.1355

Epoch 00148: val_loss did not improve from 0.12909
Epoch 149/250
 - 56s - loss: 0.1331 - val_loss: 0.1478

Epoch 00149: val_loss did not improve from 0.12909
Epoch 150/250
 - 56s - loss: 0.1349 - val_loss: 0.1363

Epoch 00150: val_loss did not improve from 0.12909
Epoch 151/250
 - 56s - loss: 0.1337 - val_loss: 0.1463

Epoch 00151: val_loss did not improve from 0.12909
Epoch 152/250
 - 56s - loss: 0.1382 - val_loss: 0.1382

Epoch 00152: val_loss did not improve from 0.12909
Epoch 153/250
 - 57s - loss: 0.1356 - val_loss: 0.1549

Epoch 00153: val_loss did not improve from 0.12909
Epoch 154/250
 - 56s - loss: 0.1360 - val_loss: 0.1364

Epoch 00154: val_loss did not improve from 0.12909
Epoch 155/250
 - 56s - loss: 0.1351 - val_loss: 0.1293

Epoch 00155: val_loss did not improve from 0.12909
Epoch 156/250
 - 57s - loss: 0.1340 - val_loss: 0.1317

Epoch 00156: val_loss did not improve from 0.12909
Epoch 157/250
 - 57s - loss: 0.1338 - val_loss: 0.1344

Epoch 00157: val_loss did not improve from 0.12909
Epoch 158/250
 - 57s - loss: 0.1346 - val_loss: 0.1611

Epoch 00158: val_loss did not improve from 0.12909
Epoch 159/250
 - 56s - loss: 0.1369 - val_loss: 0.1351

Epoch 00159: val_loss did not improve from 0.12909
Epoch 160/250
 - 57s - loss: 0.1333 - val_loss: 0.1401

Epoch 00160: val_loss did not improve from 0.12909
Epoch 161/250
 - 56s - loss: 0.1364 - val_loss: 0.1434

Epoch 00161: val_loss did not improve from 0.12909
Epoch 162/250
 - 56s - loss: 0.1354 - val_loss: 0.1320

Epoch 00162: val_loss did not improve from 0.12909
Epoch 163/250
 - 56s - loss: 0.1375 - val_loss: 0.1498

Epoch 00163: val_loss did not improve from 0.12909
Epoch 164/250
 - 56s - loss: 0.1340 - val_loss: 0.1319

Epoch 00164: val_loss did not improve from 0.12909
Epoch 165/250
 - 57s - loss: 0.1326 - val_loss: 0.1321

Epoch 00165: val_loss did not improve from 0.12909
Epoch 166/250
 - 56s - loss: 0.1314 - val_loss: 0.1472

Epoch 00166: val_loss did not improve from 0.12909
Epoch 00166: early stopping
