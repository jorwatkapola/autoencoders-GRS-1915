2020-01-28 07:56:38.525877: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX
2020-01-28 07:56:38.647253: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:897] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-01-28 07:56:38.647810: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1405] Found device 0 with properties: 
name: TITAN Xp major: 6 minor: 1 memoryClockRate(GHz): 1.582
pciBusID: 0000:01:00.0
totalMemory: 11.91GiB freeMemory: 11.73GiB
2020-01-28 07:56:38.647827: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1484] Adding visible gpu devices: 0
2020-01-28 07:56:38.880818: I tensorflow/core/common_runtime/gpu/gpu_device.cc:965] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-01-28 07:56:38.880863: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971]      0 
2020-01-28 07:56:38.880872: I tensorflow/core/common_runtime/gpu/gpu_device.cc:984] 0:   N 
2020-01-28 07:56:38.881133: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1097] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 11348 MB memory) -> physical GPU (device: 0, name: TITAN Xp, pci bus id: 0000:01:00.0, compute capability: 6.1)
2020-01-28 07:56:39.272771: I tensorflow/core/kernels/cuda_solvers.cc:159] Creating CudaSolver handles for stream 0x55c31e23d7c0
Epoch 1/8000

Epoch 00001: val_loss improved from inf to 29.07777, saving model to model_weights/model_2020-01-28_07-56-37.h5
 - 93s - loss: 40.2158 - val_loss: 29.0778
Epoch 2/8000

Epoch 00002: val_loss improved from 29.07777 to 24.94822, saving model to model_weights/model_2020-01-28_07-56-37.h5
 - 92s - loss: 25.6432 - val_loss: 24.9482
Epoch 3/8000

Epoch 00003: val_loss improved from 24.94822 to 23.06550, saving model to model_weights/model_2020-01-28_07-56-37.h5
 - 92s - loss: 23.0578 - val_loss: 23.0655
Epoch 4/8000

Epoch 00004: val_loss improved from 23.06550 to 21.43329, saving model to model_weights/model_2020-01-28_07-56-37.h5
 - 92s - loss: 21.1486 - val_loss: 21.4333
Epoch 5/8000

Epoch 00005: val_loss improved from 21.43329 to 20.62341, saving model to model_weights/model_2020-01-28_07-56-37.h5
 - 92s - loss: 20.1240 - val_loss: 20.6234
Epoch 6/8000

Epoch 00006: val_loss did not improve from 20.62341
 - 92s - loss: 19.1466 - val_loss: 21.6047
Epoch 7/8000

Epoch 00007: val_loss improved from 20.62341 to 19.24559, saving model to model_weights/model_2020-01-28_07-56-37.h5
 - 93s - loss: 20.2253 - val_loss: 19.2456
Epoch 8/8000

Epoch 00008: val_loss improved from 19.24559 to 18.74596, saving model to model_weights/model_2020-01-28_07-56-37.h5
 - 93s - loss: 18.1947 - val_loss: 18.7460
Epoch 9/8000

Epoch 00009: val_loss did not improve from 18.74596
 - 92s - loss: 18.1502 - val_loss: 18.8126
Epoch 10/8000

Epoch 00010: val_loss improved from 18.74596 to 18.50147, saving model to model_weights/model_2020-01-28_07-56-37.h5
 - 92s - loss: 18.0301 - val_loss: 18.5015
Epoch 11/8000

Epoch 00011: val_loss improved from 18.50147 to 17.80688, saving model to model_weights/model_2020-01-28_07-56-37.h5
 - 92s - loss: 17.2382 - val_loss: 17.8069
Epoch 12/8000

Epoch 00012: val_loss improved from 17.80688 to 17.48823, saving model to model_weights/model_2020-01-28_07-56-37.h5
 - 92s - loss: 16.9957 - val_loss: 17.4882
Epoch 13/8000

Epoch 00013: val_loss improved from 17.48823 to 17.13158, saving model to model_weights/model_2020-01-28_07-56-37.h5
 - 92s - loss: 16.7659 - val_loss: 17.1316
Epoch 14/8000

Epoch 00014: val_loss did not improve from 17.13158
 - 93s - loss: 16.3924 - val_loss: 17.7290
Epoch 15/8000

Epoch 00015: val_loss did not improve from 17.13158
 - 93s - loss: 16.3468 - val_loss: 18.1596
Epoch 16/8000

Epoch 00016: val_loss improved from 17.13158 to 16.03871, saving model to model_weights/model_2020-01-28_07-56-37.h5
 - 93s - loss: 16.2609 - val_loss: 16.0387
Epoch 17/8000

Epoch 00017: val_loss improved from 16.03871 to 15.61536, saving model to model_weights/model_2020-01-28_07-56-37.h5
 - 93s - loss: 15.8083 - val_loss: 15.6154
Epoch 18/8000

Epoch 00018: val_loss did not improve from 15.61536
 - 93s - loss: 15.9121 - val_loss: 16.1043
Epoch 19/8000

Epoch 00019: val_loss did not improve from 15.61536
 - 92s - loss: 15.6953 - val_loss: 15.8052
Epoch 20/8000

Epoch 00020: val_loss did not improve from 15.61536
 - 92s - loss: 15.9141 - val_loss: 15.8362
Epoch 21/8000

Epoch 00021: val_loss did not improve from 15.61536
 - 93s - loss: 29.7809 - val_loss: 24.1025
Epoch 22/8000

Epoch 00022: val_loss did not improve from 15.61536
 - 93s - loss: 21.2913 - val_loss: 20.8905
Epoch 23/8000

Epoch 00023: val_loss did not improve from 15.61536
 - 92s - loss: 20.0518 - val_loss: 20.4005
Epoch 24/8000

Epoch 00024: val_loss did not improve from 15.61536
 - 92s - loss: 19.3167 - val_loss: 19.6706
Epoch 25/8000

Epoch 00025: val_loss did not improve from 15.61536
 - 92s - loss: 18.7908 - val_loss: 19.3667
Epoch 26/8000

Epoch 00026: val_loss did not improve from 15.61536
 - 92s - loss: 18.2797 - val_loss: 18.2315
Epoch 27/8000

Epoch 00027: val_loss did not improve from 15.61536
 - 92s - loss: 17.7302 - val_loss: 18.2989
Epoch 28/8000

Epoch 00028: val_loss did not improve from 15.61536
 - 93s - loss: 17.3817 - val_loss: 17.6789
Epoch 29/8000

Epoch 00029: val_loss did not improve from 15.61536
 - 93s - loss: 17.0900 - val_loss: 17.2680
Epoch 30/8000

Epoch 00030: val_loss did not improve from 15.61536
 - 93s - loss: 17.1473 - val_loss: 16.9969
Epoch 31/8000

Epoch 00031: val_loss did not improve from 15.61536
 - 93s - loss: 16.5048 - val_loss: 17.1068
Epoch 32/8000

Epoch 00032: val_loss did not improve from 15.61536
 - 93s - loss: 16.5712 - val_loss: 17.0480
Epoch 33/8000

Epoch 00033: val_loss did not improve from 15.61536
 - 92s - loss: 16.3542 - val_loss: 16.7213
Epoch 34/8000

Epoch 00034: val_loss did not improve from 15.61536
 - 92s - loss: 16.0372 - val_loss: 16.5608
Epoch 35/8000

Epoch 00035: val_loss did not improve from 15.61536
 - 92s - loss: 16.1627 - val_loss: 16.3404
Epoch 36/8000

Epoch 00036: val_loss did not improve from 15.61536
 - 93s - loss: 16.3131 - val_loss: 16.9719
Epoch 37/8000

Epoch 00037: val_loss did not improve from 15.61536
 - 92s - loss: 15.9323 - val_loss: 16.4733
Epoch 38/8000

Epoch 00038: val_loss did not improve from 15.61536
 - 92s - loss: 15.7341 - val_loss: 16.2471
Epoch 39/8000

Epoch 00039: val_loss did not improve from 15.61536
 - 92s - loss: 15.8340 - val_loss: 15.8363
Epoch 40/8000

Epoch 00040: val_loss did not improve from 15.61536
 - 92s - loss: 15.7539 - val_loss: 16.3550
Epoch 41/8000

Epoch 00041: val_loss did not improve from 15.61536
 - 93s - loss: 15.4198 - val_loss: 15.8797
Epoch 42/8000

Epoch 00042: val_loss did not improve from 15.61536
 - 93s - loss: 15.4550 - val_loss: 16.4901
Epoch 43/8000

Epoch 00043: val_loss did not improve from 15.61536
 - 93s - loss: 15.3484 - val_loss: 16.0233
Epoch 44/8000

Epoch 00044: val_loss improved from 15.61536 to 15.27605, saving model to model_weights/model_2020-01-28_07-56-37.h5
 - 93s - loss: 15.5513 - val_loss: 15.2760
Epoch 45/8000

Epoch 00045: val_loss did not improve from 15.27605
 - 93s - loss: 15.1055 - val_loss: 15.8483
Epoch 46/8000

Epoch 00046: val_loss did not improve from 15.27605
 - 93s - loss: 15.4904 - val_loss: 15.7631
Epoch 47/8000

Epoch 00047: val_loss did not improve from 15.27605
 - 92s - loss: 15.4398 - val_loss: 16.1126
Epoch 48/8000

Epoch 00048: val_loss did not improve from 15.27605
 - 93s - loss: 15.0386 - val_loss: 15.4857
Epoch 49/8000

Epoch 00049: val_loss did not improve from 15.27605
 - 93s - loss: 15.2974 - val_loss: 16.2823
Epoch 50/8000

Epoch 00050: val_loss did not improve from 15.27605
 - 93s - loss: 15.7945 - val_loss: 16.6684
Epoch 51/8000

Epoch 00051: val_loss did not improve from 15.27605
 - 92s - loss: 15.4408 - val_loss: 16.0762
Epoch 52/8000

Epoch 00052: val_loss did not improve from 15.27605
 - 92s - loss: 14.9302 - val_loss: 15.6140
Epoch 53/8000

Epoch 00053: val_loss did not improve from 15.27605
 - 92s - loss: 14.8196 - val_loss: 17.8405
Epoch 54/8000

Epoch 00054: val_loss did not improve from 15.27605
 - 92s - loss: 15.2329 - val_loss: 15.4120
Epoch 55/8000

Epoch 00055: val_loss did not improve from 15.27605
 - 93s - loss: 14.8561 - val_loss: 15.7230
Epoch 56/8000

Epoch 00056: val_loss improved from 15.27605 to 14.84658, saving model to model_weights/model_2020-01-28_07-56-37.h5
 - 93s - loss: 14.9326 - val_loss: 14.8466
Epoch 57/8000

Epoch 00057: val_loss did not improve from 14.84658
 - 93s - loss: 14.9150 - val_loss: 15.1354
Epoch 58/8000

Epoch 00058: val_loss did not improve from 14.84658
 - 93s - loss: 15.1580 - val_loss: 15.0784
Epoch 59/8000

Epoch 00059: val_loss did not improve from 14.84658
 - 93s - loss: 14.8222 - val_loss: 16.1274
Epoch 60/8000

Epoch 00060: val_loss did not improve from 14.84658
 - 93s - loss: 14.7387 - val_loss: 15.2773
Epoch 61/8000

Epoch 00061: val_loss did not improve from 14.84658
 - 92s - loss: 15.0817 - val_loss: 15.5533
Epoch 62/8000

Epoch 00062: val_loss did not improve from 14.84658
 - 92s - loss: 14.9127 - val_loss: 16.0404
Epoch 63/8000

Epoch 00063: val_loss did not improve from 14.84658
 - 92s - loss: 14.4675 - val_loss: 14.9457
Epoch 64/8000

Epoch 00064: val_loss did not improve from 14.84658
 - 93s - loss: 15.2171 - val_loss: 17.5195
Epoch 65/8000

Epoch 00065: val_loss did not improve from 14.84658
 - 93s - loss: 15.3848 - val_loss: 15.2317
Epoch 66/8000

Epoch 00066: val_loss improved from 14.84658 to 14.75724, saving model to model_weights/model_2020-01-28_07-56-37.h5
 - 93s - loss: 14.6770 - val_loss: 14.7572
Epoch 67/8000

Epoch 00067: val_loss did not improve from 14.75724
 - 92s - loss: 14.8829 - val_loss: 15.0890
Epoch 68/8000

Epoch 00068: val_loss did not improve from 14.75724
 - 92s - loss: 14.5367 - val_loss: 14.9981
Epoch 69/8000

Epoch 00069: val_loss improved from 14.75724 to 14.72500, saving model to model_weights/model_2020-01-28_07-56-37.h5
 - 92s - loss: 14.2825 - val_loss: 14.7250
Epoch 70/8000

Epoch 00070: val_loss did not improve from 14.72500
 - 93s - loss: 14.3042 - val_loss: 15.0112
Epoch 71/8000

Epoch 00071: val_loss improved from 14.72500 to 14.51776, saving model to model_weights/model_2020-01-28_07-56-37.h5
 - 93s - loss: 14.3338 - val_loss: 14.5178
Epoch 72/8000

Epoch 00072: val_loss did not improve from 14.51776
 - 93s - loss: 14.3784 - val_loss: 15.5566
Epoch 73/8000

Epoch 00073: val_loss did not improve from 14.51776
 - 93s - loss: 14.0613 - val_loss: 14.5799
Epoch 74/8000

Epoch 00074: val_loss did not improve from 14.51776
 - 92s - loss: 14.1383 - val_loss: 14.6551
Epoch 75/8000

Epoch 00075: val_loss improved from 14.51776 to 14.07751, saving model to model_weights/model_2020-01-28_07-56-37.h5
 - 92s - loss: 14.0135 - val_loss: 14.0775
Epoch 76/8000

Epoch 00076: val_loss did not improve from 14.07751
 - 92s - loss: 14.1809 - val_loss: 14.3799
Epoch 77/8000

Epoch 00077: val_loss did not improve from 14.07751
 - 93s - loss: 14.0950 - val_loss: 14.2958
Epoch 78/8000

Epoch 00078: val_loss did not improve from 14.07751
 - 93s - loss: 14.1906 - val_loss: 14.2790
Epoch 79/8000

Epoch 00079: val_loss improved from 14.07751 to 13.91888, saving model to model_weights/model_2020-01-28_07-56-37.h5
 - 93s - loss: 13.7970 - val_loss: 13.9189
Epoch 80/8000

Epoch 00080: val_loss did not improve from 13.91888
 - 93s - loss: 13.7209 - val_loss: 14.7415
Epoch 81/8000

Epoch 00081: val_loss did not improve from 13.91888
 - 92s - loss: 14.0575 - val_loss: 13.9563
Epoch 82/8000

Epoch 00082: val_loss did not improve from 13.91888
 - 92s - loss: 13.9404 - val_loss: 14.5054
Epoch 83/8000

Epoch 00083: val_loss did not improve from 13.91888
 - 93s - loss: 16.2094 - val_loss: 15.6636
Epoch 84/8000

Epoch 00084: val_loss did not improve from 13.91888
 - 93s - loss: 14.2894 - val_loss: 16.4394
Epoch 85/8000

Epoch 00085: val_loss did not improve from 13.91888
 - 93s - loss: 14.5169 - val_loss: 15.1521
Epoch 86/8000

Epoch 00086: val_loss did not improve from 13.91888
 - 92s - loss: 14.4203 - val_loss: 14.2563
Epoch 87/8000

Epoch 00087: val_loss improved from 13.91888 to 13.65831, saving model to model_weights/model_2020-01-28_07-56-37.h5
 - 92s - loss: 13.8650 - val_loss: 13.6583
Epoch 88/8000

Epoch 00088: val_loss did not improve from 13.65831
 - 92s - loss: 13.8682 - val_loss: 14.6075
Epoch 89/8000

Epoch 00089: val_loss did not improve from 13.65831
 - 92s - loss: 13.7727 - val_loss: 14.9247
Epoch 90/8000

Epoch 00090: val_loss did not improve from 13.65831
 - 93s - loss: 13.5939 - val_loss: 13.7369
Epoch 91/8000

Epoch 00091: val_loss improved from 13.65831 to 13.53157, saving model to model_weights/model_2020-01-28_07-56-37.h5
 - 93s - loss: 13.4940 - val_loss: 13.5316
Epoch 92/8000

Epoch 00092: val_loss did not improve from 13.53157
 - 93s - loss: 13.7832 - val_loss: 14.4605
Epoch 93/8000

Epoch 00093: val_loss did not improve from 13.53157
 - 93s - loss: 13.4769 - val_loss: 13.8523
Epoch 94/8000

Epoch 00094: val_loss did not improve from 13.53157
 - 93s - loss: 13.4226 - val_loss: 14.0466
Epoch 95/8000

Epoch 00095: val_loss did not improve from 13.53157
 - 93s - loss: 13.5986 - val_loss: 14.5724
Epoch 96/8000

Epoch 00096: val_loss did not improve from 13.53157
 - 92s - loss: 13.6174 - val_loss: 13.6742
Epoch 97/8000

Epoch 00097: val_loss did not improve from 13.53157
 - 92s - loss: 13.6881 - val_loss: 13.6794
Epoch 98/8000

Epoch 00098: val_loss improved from 13.53157 to 13.33798, saving model to model_weights/model_2020-01-28_07-56-37.h5
 - 93s - loss: 13.4943 - val_loss: 13.3380
Epoch 99/8000

Epoch 00099: val_loss did not improve from 13.33798
 - 93s - loss: 13.3633 - val_loss: 13.7036
Epoch 100/8000

Epoch 00100: val_loss did not improve from 13.33798
 - 92s - loss: 13.6086 - val_loss: 13.7756
Epoch 101/8000

Epoch 00101: val_loss did not improve from 13.33798
 - 92s - loss: 14.4245 - val_loss: 13.3410
Epoch 102/8000

Epoch 00102: val_loss did not improve from 13.33798
 - 92s - loss: 13.4608 - val_loss: 13.7993
Epoch 103/8000

Epoch 00103: val_loss did not improve from 13.33798
 - 92s - loss: 19.4812 - val_loss: 16.2188
Epoch 104/8000

Epoch 00104: val_loss did not improve from 13.33798
 - 93s - loss: 15.2299 - val_loss: 15.2628
Epoch 105/8000

Epoch 00105: val_loss did not improve from 13.33798
 - 93s - loss: 14.5211 - val_loss: 15.1664
Epoch 106/8000

Epoch 00106: val_loss did not improve from 13.33798
 - 93s - loss: 14.9853 - val_loss: 14.3063
Epoch 107/8000

Epoch 00107: val_loss did not improve from 13.33798
 - 93s - loss: 14.0512 - val_loss: 13.8497
Epoch 108/8000

Epoch 00108: val_loss did not improve from 13.33798
 - 93s - loss: 13.8063 - val_loss: 14.9887
Epoch 109/8000

Epoch 00109: val_loss did not improve from 13.33798
 - 92s - loss: 13.7932 - val_loss: 14.2756
Epoch 110/8000

Epoch 00110: val_loss did not improve from 13.33798
 - 92s - loss: 13.9829 - val_loss: 13.7806
Epoch 111/8000

Epoch 00111: val_loss did not improve from 13.33798
 - 92s - loss: 13.7652 - val_loss: 14.2550
Epoch 112/8000

Epoch 00112: val_loss did not improve from 13.33798
 - 93s - loss: 13.7149 - val_loss: 13.8945
Epoch 113/8000

Epoch 00113: val_loss did not improve from 13.33798
 - 93s - loss: 13.3939 - val_loss: 14.0562
Epoch 114/8000

Epoch 00114: val_loss did not improve from 13.33798
 - 93s - loss: 13.5911 - val_loss: 13.8613
Epoch 115/8000

Epoch 00115: val_loss did not improve from 13.33798
 - 93s - loss: 13.6928 - val_loss: 13.9414
Epoch 116/8000

Epoch 00116: val_loss did not improve from 13.33798
 - 92s - loss: 17.9578 - val_loss: 18.7504
Epoch 117/8000

Epoch 00117: val_loss did not improve from 13.33798
 - 92s - loss: 16.9913 - val_loss: 16.5505
Epoch 118/8000

Epoch 00118: val_loss did not improve from 13.33798
 - 92s - loss: 15.5794 - val_loss: 15.5988
Epoch 119/8000

Epoch 00119: val_loss did not improve from 13.33798
 - 93s - loss: 14.6986 - val_loss: 14.8173
Epoch 120/8000

Epoch 00120: val_loss did not improve from 13.33798
 - 93s - loss: 14.2961 - val_loss: 13.7907
Epoch 121/8000

Epoch 00121: val_loss did not improve from 13.33798
 - 92s - loss: 14.1550 - val_loss: 14.9835
Epoch 122/8000

Epoch 00122: val_loss did not improve from 13.33798
 - 92s - loss: 13.9076 - val_loss: 14.4579
Epoch 123/8000

Epoch 00123: val_loss did not improve from 13.33798
 - 92s - loss: 13.7738 - val_loss: 13.7173
Epoch 124/8000

Epoch 00124: val_loss did not improve from 13.33798
 - 92s - loss: 13.5919 - val_loss: 15.0683
Epoch 125/8000

Epoch 00125: val_loss did not improve from 13.33798
 - 92s - loss: 13.5013 - val_loss: 13.5102
Epoch 126/8000

Epoch 00126: val_loss did not improve from 13.33798
 - 93s - loss: 15.8254 - val_loss: 17.5312
Epoch 127/8000

Epoch 00127: val_loss did not improve from 13.33798
 - 93s - loss: 15.4947 - val_loss: 14.9294
Epoch 128/8000

Epoch 00128: val_loss did not improve from 13.33798
 - 93s - loss: 14.2447 - val_loss: 13.9466
Epoch 129/8000

Epoch 00129: val_loss did not improve from 13.33798
 - 93s - loss: 13.8306 - val_loss: 13.9440
Epoch 130/8000

Epoch 00130: val_loss did not improve from 13.33798
 - 92s - loss: 13.5620 - val_loss: 13.9802
Epoch 131/8000

Epoch 00131: val_loss improved from 13.33798 to 13.31277, saving model to model_weights/model_2020-01-28_07-56-37.h5
 - 92s - loss: 13.5497 - val_loss: 13.3128
Epoch 132/8000

Epoch 00132: val_loss improved from 13.31277 to 13.31026, saving model to model_weights/model_2020-01-28_07-56-37.h5
 - 92s - loss: 13.2491 - val_loss: 13.3103
Epoch 133/8000

Epoch 00133: val_loss did not improve from 13.31026
 - 93s - loss: 13.3019 - val_loss: 13.4769
Epoch 134/8000

Epoch 00134: val_loss improved from 13.31026 to 13.18466, saving model to model_weights/model_2020-01-28_07-56-37.h5
 - 93s - loss: 13.3461 - val_loss: 13.1847
Epoch 135/8000

Epoch 00135: val_loss did not improve from 13.18466
 - 93s - loss: 13.0991 - val_loss: 13.5294
Epoch 136/8000

Epoch 00136: val_loss did not improve from 13.18466
 - 92s - loss: 13.0959 - val_loss: 13.3403
Epoch 137/8000

Epoch 00137: val_loss did not improve from 13.18466
 - 92s - loss: 12.9545 - val_loss: 13.3240
Epoch 138/8000

Epoch 00138: val_loss improved from 13.18466 to 13.10534, saving model to model_weights/model_2020-01-28_07-56-37.h5
 - 92s - loss: 12.9128 - val_loss: 13.1053
Epoch 139/8000

Epoch 00139: val_loss improved from 13.10534 to 12.91912, saving model to model_weights/model_2020-01-28_07-56-37.h5
 - 92s - loss: 13.0341 - val_loss: 12.9191
Epoch 140/8000

Epoch 00140: val_loss did not improve from 12.91912
 - 93s - loss: 12.8143 - val_loss: 13.0680
Epoch 141/8000

Epoch 00141: val_loss did not improve from 12.91912
 - 93s - loss: 13.1389 - val_loss: 14.1283
Epoch 142/8000

Epoch 00142: val_loss improved from 12.91912 to 12.81403, saving model to model_weights/model_2020-01-28_07-56-37.h5
 - 93s - loss: 12.8015 - val_loss: 12.8140
Epoch 143/8000

Epoch 00143: val_loss improved from 12.81403 to 12.80469, saving model to model_weights/model_2020-01-28_07-56-37.h5
 - 93s - loss: 12.8790 - val_loss: 12.8047
Epoch 144/8000

Epoch 00144: val_loss did not improve from 12.80469
 - 93s - loss: 13.2860 - val_loss: 13.1505
Epoch 145/8000

Epoch 00145: val_loss did not improve from 12.80469
 - 92s - loss: 12.7527 - val_loss: 12.9403
Epoch 146/8000

Epoch 00146: val_loss did not improve from 12.80469
 - 92s - loss: 12.6441 - val_loss: 13.0861
Epoch 147/8000

Epoch 00147: val_loss did not improve from 12.80469
 - 93s - loss: 12.7985 - val_loss: 13.1526
Epoch 148/8000

Epoch 00148: val_loss did not improve from 12.80469
 - 93s - loss: 12.5121 - val_loss: 12.8702
Epoch 149/8000

Epoch 00149: val_loss did not improve from 12.80469
 - 92s - loss: 12.6719 - val_loss: 13.0173
Epoch 150/8000

Epoch 00150: val_loss improved from 12.80469 to 12.78166, saving model to model_weights/model_2020-01-28_07-56-37.h5
 - 92s - loss: 12.7105 - val_loss: 12.7817
Epoch 151/8000

Epoch 00151: val_loss improved from 12.78166 to 12.64674, saving model to model_weights/model_2020-01-28_07-56-37.h5
 - 92s - loss: 12.7824 - val_loss: 12.6467
Epoch 152/8000

Epoch 00152: val_loss did not improve from 12.64674
 - 92s - loss: 12.5504 - val_loss: 13.8381
Epoch 153/8000

Epoch 00153: val_loss did not improve from 12.64674
 - 92s - loss: 12.6060 - val_loss: 12.7014
Epoch 154/8000

Epoch 00154: val_loss did not improve from 12.64674
 - 93s - loss: 12.5440 - val_loss: 13.1264
Epoch 155/8000

Epoch 00155: val_loss did not improve from 12.64674
 - 93s - loss: 12.4379 - val_loss: 13.0166
Epoch 156/8000

Epoch 00156: val_loss did not improve from 12.64674
 - 93s - loss: 12.5543 - val_loss: 12.7950
Epoch 157/8000

Epoch 00157: val_loss improved from 12.64674 to 12.62196, saving model to model_weights/model_2020-01-28_07-56-37.h5
 - 93s - loss: 12.5000 - val_loss: 12.6220
Epoch 158/8000

Epoch 00158: val_loss did not improve from 12.62196
 - 93s - loss: 12.4597 - val_loss: 14.6320
Epoch 159/8000

Epoch 00159: val_loss did not improve from 12.62196
 - 92s - loss: 12.5877 - val_loss: 12.8991
Epoch 160/8000

Epoch 00160: val_loss improved from 12.62196 to 12.47432, saving model to model_weights/model_2020-01-28_07-56-37.h5
 - 92s - loss: 12.4067 - val_loss: 12.4743
Epoch 161/8000

Epoch 00161: val_loss did not improve from 12.47432
 - 92s - loss: 12.7406 - val_loss: 14.5287
Epoch 162/8000

Epoch 00162: val_loss improved from 12.47432 to 12.45183, saving model to model_weights/model_2020-01-28_07-56-37.h5
 - 93s - loss: 12.6752 - val_loss: 12.4518
Epoch 163/8000

Epoch 00163: val_loss did not improve from 12.45183
 - 92s - loss: 12.5542 - val_loss: 13.3468
Epoch 164/8000

Epoch 00164: val_loss did not improve from 12.45183
 - 93s - loss: 12.3978 - val_loss: 13.7259
Epoch 165/8000

Epoch 00165: val_loss did not improve from 12.45183
 - 93s - loss: 12.4514 - val_loss: 12.7820
Epoch 166/8000

Epoch 00166: val_loss did not improve from 12.45183
 - 92s - loss: 12.9692 - val_loss: 12.9684
Epoch 167/8000

Epoch 00167: val_loss did not improve from 12.45183
 - 92s - loss: 12.1632 - val_loss: 12.5743
Epoch 168/8000

Epoch 00168: val_loss improved from 12.45183 to 12.38550, saving model to model_weights/model_2020-01-28_07-56-37.h5
 - 93s - loss: 12.4010 - val_loss: 12.3855
Epoch 169/8000

Epoch 00169: val_loss did not improve from 12.38550
 - 93s - loss: 13.2173 - val_loss: 12.4149
Epoch 170/8000

Epoch 00170: val_loss did not improve from 12.38550
 - 93s - loss: 12.3802 - val_loss: 12.9400
Epoch 171/8000

Epoch 00171: val_loss did not improve from 12.38550
 - 92s - loss: 12.3373 - val_loss: 12.4618
Epoch 172/8000

Epoch 00172: val_loss improved from 12.38550 to 12.14247, saving model to model_weights/model_2020-01-28_07-56-37.h5
 - 92s - loss: 12.1128 - val_loss: 12.1425
Epoch 173/8000

Epoch 00173: val_loss did not improve from 12.14247
 - 92s - loss: 11.9358 - val_loss: 12.3438
Epoch 174/8000

Epoch 00174: val_loss did not improve from 12.14247
 - 92s - loss: 12.4056 - val_loss: 12.4323
Epoch 175/8000

Epoch 00175: val_loss did not improve from 12.14247
 - 93s - loss: 12.1897 - val_loss: 12.9687
Epoch 176/8000

Epoch 00176: val_loss did not improve from 12.14247
 - 93s - loss: 13.9035 - val_loss: 26.7405
Epoch 177/8000

Epoch 00177: val_loss did not improve from 12.14247
 - 93s - loss: 20.5846 - val_loss: 19.4251
Epoch 178/8000

Epoch 00178: val_loss did not improve from 12.14247
 - 93s - loss: 17.8439 - val_loss: 17.4577
Epoch 179/8000

Epoch 00179: val_loss did not improve from 12.14247
 - 92s - loss: 16.5409 - val_loss: 17.2748
Epoch 180/8000

Epoch 00180: val_loss did not improve from 12.14247
 - 92s - loss: 16.0868 - val_loss: 16.3545
Epoch 181/8000

Epoch 00181: val_loss did not improve from 12.14247
 - 92s - loss: 15.7821 - val_loss: 16.2104
Epoch 182/8000

Epoch 00182: val_loss did not improve from 12.14247
 - 93s - loss: 15.4141 - val_loss: 15.3976
Epoch 183/8000

Epoch 00183: val_loss did not improve from 12.14247
 - 93s - loss: 14.9633 - val_loss: 15.2161
Epoch 184/8000

Epoch 00184: val_loss did not improve from 12.14247
 - 92s - loss: 14.7408 - val_loss: 15.7045
Epoch 185/8000

Epoch 00185: val_loss did not improve from 12.14247
 - 92s - loss: 14.5919 - val_loss: 15.2821
Epoch 186/8000

Epoch 00186: val_loss did not improve from 12.14247
 - 92s - loss: 14.4675 - val_loss: 14.7654
Epoch 187/8000

Epoch 00187: val_loss did not improve from 12.14247
 - 92s - loss: 14.4174 - val_loss: 14.2915
Epoch 188/8000

Epoch 00188: val_loss did not improve from 12.14247
 - 92s - loss: 14.1075 - val_loss: 14.7823
Epoch 189/8000

Epoch 00189: val_loss did not improve from 12.14247
 - 93s - loss: 14.1392 - val_loss: 13.9376
Epoch 190/8000

Epoch 00190: val_loss did not improve from 12.14247
 - 93s - loss: 13.9339 - val_loss: 13.9876
Epoch 191/8000

Epoch 00191: val_loss did not improve from 12.14247
 - 93s - loss: 13.9431 - val_loss: 14.0559
Epoch 192/8000

Epoch 00192: val_loss did not improve from 12.14247
 - 93s - loss: 13.8831 - val_loss: 14.0921
Epoch 193/8000

Epoch 00193: val_loss did not improve from 12.14247
 - 92s - loss: 13.5673 - val_loss: 13.8691
Epoch 194/8000

Epoch 00194: val_loss did not improve from 12.14247
 - 92s - loss: 13.8739 - val_loss: 14.1495
Epoch 195/8000

Epoch 00195: val_loss did not improve from 12.14247
 - 92s - loss: 13.8343 - val_loss: 13.8862
Epoch 196/8000

Epoch 00196: val_loss did not improve from 12.14247
 - 94s - loss: 13.7014 - val_loss: 13.1284
Epoch 197/8000

Epoch 00197: val_loss did not improve from 12.14247
 - 93s - loss: 13.3738 - val_loss: 14.3410
Epoch 198/8000

Epoch 00198: val_loss did not improve from 12.14247
 - 92s - loss: 13.3697 - val_loss: 13.4682
Epoch 199/8000

Epoch 00199: val_loss did not improve from 12.14247
 - 93s - loss: 13.1753 - val_loss: 13.4530
Epoch 200/8000

Epoch 00200: val_loss did not improve from 12.14247
 - 92s - loss: 13.0964 - val_loss: 13.3455
Epoch 201/8000

Epoch 00201: val_loss did not improve from 12.14247
 - 92s - loss: 16.7988 - val_loss: 22.5128
Epoch 202/8000

Epoch 00202: val_loss did not improve from 12.14247
 - 92s - loss: 19.5123 - val_loss: 19.1085
Epoch 203/8000

Epoch 00203: val_loss did not improve from 12.14247
 - 93s - loss: 17.8994 - val_loss: 18.0378
Epoch 204/8000

Epoch 00204: val_loss did not improve from 12.14247
 - 93s - loss: 17.2429 - val_loss: 17.7559
Epoch 205/8000

Epoch 00205: val_loss did not improve from 12.14247
 - 92s - loss: 16.7953 - val_loss: 17.2002
Epoch 206/8000

Epoch 00206: val_loss did not improve from 12.14247
 - 92s - loss: 16.4803 - val_loss: 16.5194
Epoch 207/8000

Epoch 00207: val_loss did not improve from 12.14247
 - 92s - loss: 16.1377 - val_loss: 17.6140
Epoch 208/8000

Epoch 00208: val_loss did not improve from 12.14247
 - 92s - loss: 15.9408 - val_loss: 16.3495
Epoch 209/8000

Epoch 00209: val_loss did not improve from 12.14247
 - 92s - loss: 15.7063 - val_loss: 15.6962
Epoch 210/8000

Epoch 00210: val_loss did not improve from 12.14247
 - 92s - loss: 15.4656 - val_loss: 15.6286
Epoch 211/8000

Epoch 00211: val_loss did not improve from 12.14247
 - 93s - loss: 15.3135 - val_loss: 15.9779
Epoch 212/8000

Epoch 00212: val_loss did not improve from 12.14247
 - 92s - loss: 15.0588 - val_loss: 15.3178
Epoch 213/8000

Epoch 00213: val_loss did not improve from 12.14247
 - 93s - loss: 15.0298 - val_loss: 15.2122
Epoch 214/8000

Epoch 00214: val_loss did not improve from 12.14247
 - 92s - loss: 14.7602 - val_loss: 15.3125
Epoch 215/8000

Epoch 00215: val_loss did not improve from 12.14247
 - 92s - loss: 14.7490 - val_loss: 15.0080
Epoch 216/8000

Epoch 00216: val_loss did not improve from 12.14247
 - 92s - loss: 14.4800 - val_loss: 14.5734
Epoch 217/8000

Epoch 00217: val_loss did not improve from 12.14247
 - 93s - loss: 14.6507 - val_loss: 14.6068
Epoch 218/8000

Epoch 00218: val_loss did not improve from 12.14247
 - 93s - loss: 14.3615 - val_loss: 14.5471
Epoch 219/8000

Epoch 00219: val_loss did not improve from 12.14247
 - 93s - loss: 14.2035 - val_loss: 14.2678
Epoch 220/8000

Epoch 00220: val_loss did not improve from 12.14247
 - 93s - loss: 14.0990 - val_loss: 14.6240
Epoch 221/8000

Epoch 00221: val_loss did not improve from 12.14247
 - 92s - loss: 14.1760 - val_loss: 14.1387
Epoch 222/8000

Epoch 00222: val_loss did not improve from 12.14247
 - 92s - loss: 13.8784 - val_loss: 14.4756
Epoch 223/8000

Epoch 00223: val_loss did not improve from 12.14247
 - 92s - loss: 13.8630 - val_loss: 14.0682
Epoch 224/8000

Epoch 00224: val_loss did not improve from 12.14247
 - 92s - loss: 13.8761 - val_loss: 13.9135
Epoch 225/8000

Epoch 00225: val_loss did not improve from 12.14247
 - 93s - loss: 13.9022 - val_loss: 13.9108
Epoch 226/8000

Epoch 00226: val_loss did not improve from 12.14247
 - 93s - loss: 13.6690 - val_loss: 14.1964
Epoch 227/8000

Epoch 00227: val_loss did not improve from 12.14247
 - 93s - loss: 13.6247 - val_loss: 14.6177
Epoch 228/8000

Epoch 00228: val_loss did not improve from 12.14247
 - 92s - loss: 13.8710 - val_loss: 14.3069
Epoch 229/8000

Epoch 00229: val_loss did not improve from 12.14247
 - 92s - loss: 13.6286 - val_loss: 13.5092
Epoch 230/8000

Epoch 00230: val_loss did not improve from 12.14247
 - 92s - loss: 13.3845 - val_loss: 13.8626
Epoch 231/8000

Epoch 00231: val_loss did not improve from 12.14247
 - 93s - loss: 13.4715 - val_loss: 13.8402
Epoch 232/8000

Epoch 00232: val_loss did not improve from 12.14247
 - 93s - loss: 13.3765 - val_loss: 13.9872
Epoch 233/8000

Epoch 00233: val_loss did not improve from 12.14247
 - 92s - loss: 13.4285 - val_loss: 14.6250
Epoch 234/8000

Epoch 00234: val_loss did not improve from 12.14247
 - 92s - loss: 13.4368 - val_loss: 13.4026
Epoch 235/8000

Epoch 00235: val_loss did not improve from 12.14247
 - 92s - loss: 13.2344 - val_loss: 13.6714
Epoch 236/8000

Epoch 00236: val_loss did not improve from 12.14247
 - 92s - loss: 13.3436 - val_loss: 13.4674
Epoch 237/8000

Epoch 00237: val_loss did not improve from 12.14247
 - 92s - loss: 13.0113 - val_loss: 13.4755
Epoch 238/8000

Epoch 00238: val_loss did not improve from 12.14247
 - 93s - loss: 13.1755 - val_loss: 14.0024
Epoch 239/8000

Epoch 00239: val_loss did not improve from 12.14247
 - 93s - loss: 13.0555 - val_loss: 13.4935
Epoch 240/8000

Epoch 00240: val_loss did not improve from 12.14247
 - 93s - loss: 15.4823 - val_loss: 15.1187
Epoch 241/8000

Epoch 00241: val_loss did not improve from 12.14247
 - 93s - loss: 14.3923 - val_loss: 14.5154
Epoch 242/8000

Epoch 00242: val_loss did not improve from 12.14247
 - 93s - loss: 13.9179 - val_loss: 13.9353
Epoch 243/8000

Epoch 00243: val_loss did not improve from 12.14247
 - 92s - loss: 13.7124 - val_loss: 13.6446
Epoch 244/8000

Epoch 00244: val_loss did not improve from 12.14247
 - 92s - loss: 13.7238 - val_loss: 14.1393
Epoch 245/8000

Epoch 00245: val_loss did not improve from 12.14247
 - 92s - loss: 13.5724 - val_loss: 13.7605
Epoch 246/8000

Epoch 00246: val_loss did not improve from 12.14247
 - 93s - loss: 13.3805 - val_loss: 13.4620
Epoch 247/8000

Epoch 00247: val_loss did not improve from 12.14247
 - 92s - loss: 13.3193 - val_loss: 13.4504
Epoch 248/8000

Epoch 00248: val_loss did not improve from 12.14247
 - 93s - loss: 13.2244 - val_loss: 13.5402
Epoch 249/8000

Epoch 00249: val_loss did not improve from 12.14247
 - 92s - loss: 13.0939 - val_loss: 13.2541
Epoch 250/8000

Epoch 00250: val_loss did not improve from 12.14247
 - 92s - loss: 13.1214 - val_loss: 13.5642
Epoch 251/8000

Epoch 00251: val_loss did not improve from 12.14247
 - 92s - loss: 13.0523 - val_loss: 13.2629
Epoch 252/8000

Epoch 00252: val_loss did not improve from 12.14247
 - 93s - loss: 13.0308 - val_loss: 13.3950
Epoch 253/8000

Epoch 00253: val_loss did not improve from 12.14247
 - 93s - loss: 12.8848 - val_loss: 13.1624
Epoch 254/8000

Epoch 00254: val_loss did not improve from 12.14247
 - 92s - loss: 13.0817 - val_loss: 13.8290
Epoch 255/8000

Epoch 00255: val_loss did not improve from 12.14247
 - 92s - loss: 13.1030 - val_loss: 13.7639
Epoch 256/8000

Epoch 00256: val_loss did not improve from 12.14247
 - 92s - loss: 12.8381 - val_loss: 13.4770
Epoch 257/8000

Epoch 00257: val_loss did not improve from 12.14247
 - 92s - loss: 12.9850 - val_loss: 14.0839
Epoch 258/8000

Epoch 00258: val_loss did not improve from 12.14247
 - 92s - loss: 12.8951 - val_loss: 12.7847
Epoch 259/8000

Epoch 00259: val_loss did not improve from 12.14247
 - 93s - loss: 12.6444 - val_loss: 13.0179
Epoch 260/8000

Epoch 00260: val_loss did not improve from 12.14247
 - 93s - loss: 12.8650 - val_loss: 12.9189
Epoch 261/8000

Epoch 00261: val_loss did not improve from 12.14247
 - 92s - loss: 12.7305 - val_loss: 13.1555
Epoch 262/8000

Epoch 00262: val_loss did not improve from 12.14247
 - 93s - loss: 12.6246 - val_loss: 13.0236
Epoch 263/8000

Epoch 00263: val_loss did not improve from 12.14247
 - 92s - loss: 12.6820 - val_loss: 13.2148
Epoch 264/8000

Epoch 00264: val_loss did not improve from 12.14247
 - 92s - loss: 12.6253 - val_loss: 12.9450
Epoch 265/8000

Epoch 00265: val_loss did not improve from 12.14247
 - 93s - loss: 12.5539 - val_loss: 13.4163
Epoch 266/8000

Epoch 00266: val_loss did not improve from 12.14247
 - 93s - loss: 12.7423 - val_loss: 12.6241
Epoch 267/8000

Epoch 00267: val_loss did not improve from 12.14247
 - 93s - loss: 12.6679 - val_loss: 12.7828
Epoch 268/8000

Epoch 00268: val_loss did not improve from 12.14247
 - 92s - loss: 12.7320 - val_loss: 13.0911
Epoch 269/8000

Epoch 00269: val_loss did not improve from 12.14247
 - 92s - loss: 12.7163 - val_loss: 12.7314
Epoch 270/8000

Epoch 00270: val_loss did not improve from 12.14247
 - 92s - loss: 12.3748 - val_loss: 12.5087
Epoch 271/8000

Epoch 00271: val_loss did not improve from 12.14247
 - 92s - loss: 12.4914 - val_loss: 12.9327
Epoch 272/8000

Epoch 00272: val_loss did not improve from 12.14247
 - 92s - loss: 12.6444 - val_loss: 14.1170
Epoch 00272: early stopping
