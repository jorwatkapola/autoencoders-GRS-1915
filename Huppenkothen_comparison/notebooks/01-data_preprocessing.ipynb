{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating a data set for the comparison with Huppenkothen+2017\n",
    "\n",
    "Main classificatin experiments described in paper Orwat-Kapola+2021 were not directly comparable with the work of Daniela Huppenkothen, because the former classified whole light curves instead of 1024 s segments, and the cadence of the data was 1s or 4s instead of 0.125 s.\n",
    "\n",
    "Here we prepare a data set of 1024 s overlapping segments which are further segmented into 16 s segments with cadence of 0.125s to make a direct comparison with the supervised classification of Huppenkothen+2017.\n",
    "\n",
    "In order to reduce the amount of generated data, the 1024 s segments created with a stride of 256 s and the 16 s segments are created with a stride of 16 s. The fact that light curve features are observed in only one phase shift position within those 16 s segments can affect the result. \n",
    "\n",
    "Huppenkothen+2017 used a total of 1884 previously classified samples (1024 s segments), with 885 classified samples in the training set, 480 samples in the validation set and 519 samples in the test set, respectively.\n",
    "\n",
    "## Edit the paths below as required and run all cells to generate light curve segments.\n",
    "\n",
    "## segments are then used to train the VAE-LSTM model. this is done using the scrip Huppenkothen_comparison/scripts/train_VAE-LSTM.py. \n",
    "\n",
    "## when the light curve segments are ready, use the script to train the model and move on to notebook 02"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_path = \"../\" # parent folder path where /src folder is\n",
    "# data_dir = \"../data/\" # path to the light curve data saved in txt files.\n",
    "raw_data_dir = \"../../../data_GRS1915/std1/\" # directory path to where lightcurve files are located. could be changed to \"../data/raw/\" for example\n",
    "raw_file_name_suffix = \"_std1_lc.txt\"   # suffix of light curve txt files. all of the files from the \n",
    "                                        # raw_data_dir ending with this suffix will be read in. they need to be directly interpretable by numpy\n",
    "\n",
    "labels_dir = '../data/1915Belloniclass_updated.dat' # path to the file containing classifications of the light curves (labels used in Huppenkothen+2017)\n",
    "\n",
    "\n",
    "### OUTPUTS\n",
    "segment_data_dir = \"../data/segments/\" # where to save the segmented data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys; sys.path.insert(0, src_path) # add parent folder path where /src folder is\n",
    "from src import data_preprocessing\n",
    "\n",
    "from IPython.display import clear_output\n",
    "import os\n",
    "import fnmatch\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pickle\n",
    "import pandas as pd\n",
    "# import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished\n"
     ]
    }
   ],
   "source": [
    "# load light curves from text files\n",
    "\n",
    "lcs=[] # light curves (time stamps, count rate, uncertainty)\n",
    "lc_ids=[] # observation ids\n",
    "\n",
    "for root, dirnames, filenames in os.walk(raw_data_dir): #Std1_PCU2\n",
    "    for filename in fnmatch.filter(filenames, \"*{}\".format(raw_file_name_suffix)):\n",
    "        lc = os.path.join(root, filename)\n",
    "        lc_ids.append(filename.split(\"_\")[0])\n",
    "        f=np.loadtxt(lc)\n",
    "        f=np.transpose(f)\n",
    "        lcs.append(f)\n",
    "        clear_output(wait=True)\n",
    "        print(\"Loaded {} lightcurves\".format(len(lcs)))\n",
    "print(\"Finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 1776/1776 light curves.\n",
      "Successfully segmented 1442 light curves.\n",
      "Prepared 11028 segments.\n"
     ]
    }
   ],
   "source": [
    "# segmentation of light curves\n",
    "\n",
    "seg_ids=[]\n",
    "segments_list = []\n",
    "\n",
    "for lc_index, lc in enumerate(lcs):\n",
    "    segments = data_preprocessing.segmentation(time_series = lc, \n",
    "                                       segment_length_sec = 1024, \n",
    "                                       stride_sec = 256, \n",
    "                                       keep_time_stamps = True, \n",
    "                                       input_cadence_sec = 0.125)\n",
    "    if len(segments) > 0:\n",
    "        segments_list.append(segments)\n",
    "        for seg_index, seg in enumerate(segments):\n",
    "            seg_ids.append(lc_ids[lc_index]+\"_{}\".format(seg_index))\n",
    "            \n",
    "    clear_output(wait=True)\n",
    "    print(\"Processed {}/{} light curves.\".format(lc_index+1, len(lcs)))\n",
    "print(\"Successfully segmented {} light curves.\".format(len(segments_list)))\n",
    "\n",
    "segments_list =  [item for sublist in segments_list for item in sublist] # vstack the list of lists of data segments\n",
    "\n",
    "print(\"Prepared {} segments.\".format(len(segments_list)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load human-labelled classifications from Huppenkothen+2017\n",
    "\n",
    "clean_belloni = open(labels_dir)\n",
    "lines = clean_belloni.readlines()\n",
    "states = lines[0].split()\n",
    "belloni_clean = {}\n",
    "for h,l in zip(states, lines[1:]):\n",
    "    belloni_clean[h] = l.split()\n",
    "    #state: obsID1, obsID2...\n",
    "ob_state = {}\n",
    "for state, obs in belloni_clean.items():\n",
    "    if state == \"chi1\" or state == \"chi2\" or state == \"chi3\" or state == \"chi4\": state = \"chi\"\n",
    "    for ob in obs:\n",
    "        ob_state[ob] = state\n",
    "        \n",
    "ob_state[\"10258-01-10-00\"] = \"mu\" # this one seems to be misclassified in Huppenkothen+2017\n",
    "        \n",
    "# inverse the ob_state dictionary, so that inv_ob_state contains {\"state name\" : [list of observation IDs], ...}\n",
    "\n",
    "inv_ob_state = {}\n",
    "for k, v in ob_state.items():\n",
    "    inv_ob_state[v] = inv_ob_state.get(v, [])\n",
    "    inv_ob_state[v].append(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure that the data labelled data does not allow for any more segments to be extracted\n",
    "\n",
    "# data_preprocessing.verify_not_segmented_light_curves(lcs, ob_state, seg_ids, lc_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alpha\t6\t66\n",
      "beta\t14\t142\n",
      "chi\t70\t888\n",
      "delta\t13\t66\n",
      "eta\t2\t46\n",
      "gamma\t12\t163\n",
      "kappa\t9\t117\n",
      "lambda\t2\t34\n",
      "mu\t7\t78\n",
      "nu\t3\t47\n",
      "omega\t3\t16\n",
      "phi\t9\t107\n",
      "rho\t17\t159\n",
      "theta\t17\t212\n",
      "\n",
      "sum\t184\t2141\n"
     ]
    }
   ],
   "source": [
    "# check how many labelled light curves were segmented and count the number for each class\n",
    "# count how many labelled segments there are for each class\n",
    "\n",
    "labelled_with_segments = []\n",
    "for k, v in ob_state.items():\n",
    "    if k in [x.split(\"_\")[0] for x in seg_ids]:\n",
    "        labelled_with_segments.append(v)\n",
    "class_names, class_obs_counts = np.unique(np.array(labelled_with_segments), return_counts=True)\n",
    "\n",
    "labelled_segments = []\n",
    "for x in [x.split(\"_\")[0] for x in seg_ids]:\n",
    "    if x in ob_state.keys():\n",
    "        labelled_segments.append(ob_state[x])\n",
    "class_names, class_seg_counts = np.unique(np.array(labelled_segments), return_counts=True)\n",
    "\n",
    "\n",
    "for class_name, class_obs_count, class_seg_count  in zip(class_names,class_obs_counts,class_seg_counts):\n",
    "    print(\"{}\\t{}\\t{}\".format(class_name, class_obs_count, class_seg_count))\n",
    "print(\"\\nsum\\t{}\\t{}\".format(np.sum(class_obs_counts), np.sum(class_seg_counts)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# split data into training, validation and test sets\n",
    "50/25/25 split ratio\n",
    "\n",
    "Huppenkothen+2017 used a total of 1884 previously classified samples, with 885 classified samples in the training set, 480 samples in the validation set and 519 samples in the test set, respectively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counts of labelled/unlabelled segments:\n",
      "(array(['alpha', 'beta', 'chi', 'delta', 'eta', 'gamma', 'kappa', 'lambda',\n",
      "       'mu', 'no_label', 'nu', 'omega', 'phi', 'rho', 'theta'],\n",
      "      dtype='<U8'), array([   6,   14,   70,   13,    2,   12,    9,    2,    7, 1258,    3,\n",
      "          3,    9,   17,   17]))\n"
     ]
    }
   ],
   "source": [
    "# get rid of the within-observation segment indices and create a degenerate list of observation IDs\n",
    "seg_ob_IDs_unique = np.unique([seg.split(\"_\")[0] for seg in seg_ids])\n",
    "\n",
    "# create list of observation classifications including those without a label\n",
    "classes = np.array([\"alpha\", \"beta\", \"gamma\", \"delta\", \"theta\", \"kappa\", \"lambda\", \"mu\", \"nu\", \"rho\", \"phi\", \"chi\", \"eta\", \"omega\"])\n",
    "labelled_obs_class = []\n",
    "labelled_obs_id = []\n",
    "for seg in seg_ob_IDs_unique:\n",
    "    if seg in ob_state.keys():\n",
    "        labelled_obs_class.append(ob_state[seg])\n",
    "        labelled_obs_id.append(seg)\n",
    "    else:\n",
    "        labelled_obs_class.append(\"no_label\")\n",
    "        labelled_obs_id.append(seg)\n",
    "        \n",
    "print(\"Counts of labelled/unlabelled segments:\")\n",
    "print(np.unique(labelled_obs_class, return_counts=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split observations between training and test sets\n",
    "\n",
    "ob_ids_train, ob_ids_test, training_classes_ob, test_classes_ob = train_test_split(\n",
    "    labelled_obs_id,\n",
    "    labelled_obs_class,\n",
    "    test_size=0.25, random_state=123456, stratify=labelled_obs_class)\n",
    "\n",
    "# split observations between training and validation sets\n",
    "\n",
    "ob_ids_train, ob_ids_valid, training_classes_ob, validation_classes_ob = train_test_split(\n",
    "    ob_ids_train,\n",
    "    training_classes_ob,\n",
    "    test_size=0.33, random_state=123456, stratify=training_classes_ob)\n",
    "\n",
    "# no_label segments are not needed in the test set, so they will be moved to the training set.\n",
    "\n",
    "to_be_moved = []\n",
    "for ob_index, ob_class in enumerate(test_classes_ob):\n",
    "    if ob_class == \"no_label\":\n",
    "        to_be_moved.append(True)\n",
    "    else:\n",
    "        to_be_moved.append(False)    \n",
    "to_be_moved = np.array(to_be_moved)\n",
    "\n",
    "ob_ids_train = np.concatenate((ob_ids_train, np.array(ob_ids_test)[to_be_moved]))\n",
    "training_classes_ob = np.concatenate((training_classes_ob, np.array(test_classes_ob)[to_be_moved]))\n",
    "\n",
    "ob_ids_test = np.array(ob_ids_test)[~to_be_moved]\n",
    "test_classes_ob = np.array(test_classes_ob)[~to_be_moved]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          train set  valid set  test set\n",
      "alpha            26          6        34\n",
      "beta             84         25        33\n",
      "chi             367        273       248\n",
      "delta            11         12        43\n",
      "eta               6         40         0\n",
      "gamma           108         38        17\n",
      "kappa            76          5        36\n",
      "lambda           18         16         0\n",
      "mu               38          7        33\n",
      "no_label       6573       2314         0\n",
      "nu               18         20         9\n",
      "omega             2          3        11\n",
      "phi              62         36         9\n",
      "rho              98         29        32\n",
      "theta            69         82        61\n",
      "\n",
      "              train set  valid set  test set\n",
      "sum labelled        983        592       566\n",
      "sum total          7556       2906       566\n",
      "\n",
      "            train set  valid set   test set\n",
      "% labelled  45.913125  27.650631  26.436245\n"
     ]
    }
   ],
   "source": [
    "# find the number of segments of each class in training, validation and test sets\n",
    "\n",
    "seg_ob_IDs = [seg.split(\"_\")[0] for seg in seg_ids]\n",
    "\n",
    "training_classes_seg = []\n",
    "for seg_ob_ID in seg_ob_IDs:\n",
    "    if seg_ob_ID in ob_ids_train:\n",
    "        if seg_ob_ID in ob_state.keys():\n",
    "            training_classes_seg.append(ob_state[seg_ob_ID])\n",
    "        else:\n",
    "            training_classes_seg.append(\"no_label\")\n",
    "        \n",
    "validation_classes_seg = []\n",
    "for seg_ob_ID in seg_ob_IDs:\n",
    "    if seg_ob_ID in ob_ids_valid:\n",
    "        if seg_ob_ID in ob_state.keys():\n",
    "            validation_classes_seg.append(ob_state[seg_ob_ID])\n",
    "        else:\n",
    "            validation_classes_seg.append(\"no_label\")\n",
    "            \n",
    "test_classes_seg = []\n",
    "for seg_ob_ID in seg_ob_IDs:\n",
    "    if seg_ob_ID in ob_ids_test:\n",
    "        if seg_ob_ID in ob_state.keys():\n",
    "            test_classes_seg.append(ob_state[seg_ob_ID])\n",
    "        else:\n",
    "            test_classes_seg.append(\"no_label\")\n",
    "\n",
    "\n",
    "train_seg_class_names, train_seg_class_counts = np.unique(training_classes_seg, return_counts=True)\n",
    "validation_seg_class_names, validation_seg_class_counts = np.unique(validation_classes_seg, return_counts=True)\n",
    "test_seg_class_names, test_seg_class_counts = np.unique(test_classes_seg, return_counts=True)\n",
    "\n",
    "training_pd_series = pd.DataFrame(index = train_seg_class_names, data=train_seg_class_counts, columns=[\"train set\"])\n",
    "validation_pd_series = pd.DataFrame(index = validation_seg_class_names, data=validation_seg_class_counts, columns=[\"valid set\"])\n",
    "test_pd_series = pd.DataFrame(index = test_seg_class_names, data=test_seg_class_counts, columns=[\"test set\"])\n",
    "\n",
    "seg_counts_df = training_pd_series.join(validation_pd_series).join(test_pd_series).fillna(0).astype(int)\n",
    "seg_counts_df_sums = pd.DataFrame({'sum labelled': seg_counts_df.sum()-seg_counts_df.loc['no_label'],\n",
    "                                           'sum total': seg_counts_df.sum()})\n",
    "\n",
    "# seg_counts_df.loc['sum labelled'] = seg_counts_df.sum()\n",
    "# seg_counts_df.loc['sum labelled'] -= seg_counts_df.loc['no_label']\n",
    "seg_counts_df_perc_labelled = pd.DataFrame({'% labelled': (seg_counts_df_sums['sum labelled']/seg_counts_df_sums['sum labelled'].sum()*100)})\n",
    "print(seg_counts_df)\n",
    "print()\n",
    "print(seg_counts_df_sums.T)\n",
    "print()\n",
    "print(seg_counts_df_perc_labelled.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7556 2906 566\n"
     ]
    }
   ],
   "source": [
    "# create separate segment list for each subset\n",
    "\n",
    "segments_train = []\n",
    "segments_valid = []\n",
    "segments_test = []\n",
    "\n",
    "for seg_id, segment in zip(seg_ids,segments_list):\n",
    "    seg_ob_id = seg_id.split(\"_\")[0]\n",
    "    if seg_ob_id in ob_ids_train:\n",
    "        if seg_ob_id in ob_state.keys():\n",
    "            segments_train.append((seg_id, ob_state[seg_ob_id], segment))\n",
    "        else:\n",
    "            segments_train.append((seg_id, \"no_label\", segment))\n",
    "    elif seg_ob_id in ob_ids_valid:\n",
    "        if seg_ob_id in ob_state.keys():\n",
    "            segments_valid.append((seg_id, ob_state[seg_ob_id], segment))\n",
    "        else:\n",
    "            segments_valid.append((seg_id, \"no_label\", segment))\n",
    "    elif seg_ob_id in ob_ids_test:\n",
    "        if seg_ob_id in ob_state.keys():\n",
    "            segments_test.append((seg_id, ob_state[seg_ob_id], segment))\n",
    "        else:\n",
    "            segments_test.append((seg_id, \"no_label\", segment))\n",
    "\n",
    "print(len(segments_train),len(segments_valid),len(segments_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save segments for later use\n",
    "\n",
    "with open('{}/segments_1024s_256stride_0125cad_train.pkl'.format(segment_data_dir), 'wb') as f:\n",
    "    pickle.dump(segments_train, f)\n",
    "\n",
    "with open('{}/segments_1024s_256stride_0125cad_valid.pkl'.format(segment_data_dir), 'wb') as f:\n",
    "    pickle.dump(segments_valid, f)\n",
    "\n",
    "with open('{}/segments_1024s_256stride_0125cad_test.pkl'.format(segment_data_dir), 'wb') as f:\n",
    "    pickle.dump(segments_test, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# segmentation of 1024 second segments\n",
    "Here we're making smaller segments which will be fed to the neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def segment_segments(lcs_to_be_segmented):\n",
    "    \"\"\"\n",
    "    Custom function for the second segmentation of data. 1024 second segments are segmented to the size of 16 seconds (128 data points at 0.125 second cadence)\n",
    "    which is compatible with the VAE-LSTM\n",
    "    \"\"\"\n",
    "    \n",
    "    new_segments_list = []\n",
    "    good_lcs = 0\n",
    "    good_segs = 0\n",
    "    \n",
    "    for lc_index, (lc_id, lc_label, lc) in enumerate(lcs_to_be_segmented):\n",
    "        segments = data_preprocessing.segmentation(time_series = lc, \n",
    "                                           segment_length_sec = 128*0.125, \n",
    "                                           stride_sec = 128*0.125, \n",
    "                                           keep_time_stamps = True, \n",
    "                                           input_cadence_sec = 0.125)\n",
    "        if len(segments) > 0:\n",
    "            good_lcs += 1\n",
    "            for seg_index, seg in enumerate(segments):\n",
    "                good_segs += 1\n",
    "                new_segments_list.append((lc_id+\"_{}\".format(seg_index), lc_label, seg))\n",
    "\n",
    "        if lc_index%100 == 0 or lc_index+1 == len(lcs_to_be_segmented) or lc_index == 0:\n",
    "            clear_output(wait=True)\n",
    "            print(\"Processed {}/{} light curves.\".format(lc_index+1, len(lcs_to_be_segmented)))\n",
    "    print(\"Successfully segmented {} light curves.\".format(good_lcs))\n",
    "    print(\"Prepared {} segments.\".format(good_segs))\n",
    "    \n",
    "    return new_segments_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 7556/7556 light curves.\n",
      "Successfully segmented 7556 light curves.\n",
      "Prepared 483584 segments.\n",
      "Saved to disc\n"
     ]
    }
   ],
   "source": [
    "#training data \n",
    "# segmentation of training 1024 second segments\n",
    "\n",
    "segments128_list = segment_segments(segments_train)\n",
    "\n",
    "with open('{}/segments_1024s_256stride_0125cad_segmented_to64_train.pkl'.format(segment_data_dir), 'wb') as f:\n",
    "    pickle.dump(segments128_list, f)\n",
    "\n",
    "print(\"Saved to disc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 2906/2906 light curves.\n",
      "Successfully segmented 2906 light curves.\n",
      "Prepared 185984 segments.\n",
      "Saved to disc\n"
     ]
    }
   ],
   "source": [
    "#validation data \n",
    "# segmentation of training 1024 second segments\n",
    "\n",
    "segments128_list = segment_segments(segments_valid)\n",
    "\n",
    "with open('{}/segments_1024s_256stride_0125cad_segmented_to64_valid.pkl'.format(segment_data_dir), 'wb') as f:\n",
    "    pickle.dump(segments128_list, f)\n",
    "\n",
    "print(\"Saved to disc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 566/566 light curves.\n",
      "Successfully segmented 566 light curves.\n",
      "Prepared 36224 segments.\n",
      "Saved to disc\n"
     ]
    }
   ],
   "source": [
    "#test data \n",
    "# segmentation of training 1024 second segments\n",
    "\n",
    "segments128_list = segment_segments(segments_test)\n",
    "\n",
    "with open('{}/segments_1024s_256stride_0125cad_segmented_to64_test.pkl'.format(segment_data_dir), 'wb') as f:\n",
    "    pickle.dump(segments128_list, f)\n",
    "\n",
    "print(\"Saved to disc\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# above data is used to train the VAE-LSTM model. this is done using the scrip /scripts/Huppenkothen_comparison/train_VAE-LSTM.py"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jakub-tf",
   "language": "python",
   "name": "jakub-tf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
