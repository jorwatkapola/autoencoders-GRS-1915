{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from IPython.display import clear_output\n",
    "import os\n",
    "import fnmatch\n",
    "import numpy as np\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import umap\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from scipy import stats\n",
    "\n",
    "from scipy.stats import zscore\n",
    "\n",
    "from numpy.random import multivariate_normal\n",
    "from scipy.spatial.distance import mahalanobis\n",
    "from scipy.spatial.distance import euclidean\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (5.0, 5.0)\n",
    "plt.rcParams.update({'font.size': 12})\n",
    "plt.rcParams.update(plt.rcParamsDefault)\n",
    "\n",
    "np.random.seed(seed=11)\n",
    "\n",
    "\n",
    "cwd = os.getcwd()\n",
    "\n",
    "if cwd.split(\"/\")[1] == \"export\":\n",
    "    data_dir = \"../../../files_from_snuffy\"\n",
    "else:\n",
    "    data_dir = \"../../../data_GRS1915\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data and Gaussian mixture model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('{}/468202_len128_s2_4cad_counts_errorfix.pkl'.format(data_dir), 'rb') as f:\n",
    "    segments_counts = pickle.load(f)\n",
    "# with open('../../../data_GRS1915/468202_len128_s2_4cad_errors_errorfix.pkl', 'rb') as f:\n",
    "#     segments_errors = pickle.load(f)\n",
    "# with open('../../../data_GRS1915/468202_len128_s2_4cad_ids_errorfix.pkl', 'rb') as f:\n",
    "#     id_per_seg = pickle.load(f)\n",
    "\n",
    "weights_dir = \"../../../model_weights/model_2020-04-29_09-12-23.h5\"\n",
    "segments_dir = '../../../data_GRS1915/468202_len128_s2_4cad_counts_errorfix.pkl'\n",
    "segment_encoding_dir = '{}/segment_encoding_{}_segments_{}.pkl'.format(data_dir, weights_dir.split(\"/\")[-1].split(\".\")[0], segments_dir.split(\"/\")[-1].split(\".\")[0])\n",
    "\n",
    "with open(segment_encoding_dir, 'rb') as f:\n",
    "    segment_encoding = pickle.load(f)\n",
    "    \n",
    "segment_encoding_scaled_means = zscore(segment_encoding[:,0,:], axis=0).astype(np.float32)  # standardize per feature\n",
    "\n",
    "\n",
    "desc_stats = np.zeros((len(segments_counts), 4)) #mean, std, skew, kurt\n",
    "# desc_stats[:,0] = np.median(segments_counts, axis=1).flatten()\n",
    "desc_stats[:,0] = np.mean(segments_counts, axis=1).flatten()\n",
    "desc_stats[:,1] = np.std(segments_counts, axis=1).flatten()\n",
    "desc_stats[:,2] = stats.skew(segments_counts, axis=1).flatten()\n",
    "desc_stats[:,3] = stats.kurtosis(segments_counts, axis=1).flatten()\n",
    "zscore_desc_stats = zscore(desc_stats, axis=0)\n",
    "\n",
    "# desc_GM = np.hstack((zscore(desc_stats, axis=0), GMM_bics))\n",
    "\n",
    "shape_moments = np.hstack((segment_encoding_scaled_means, zscore_desc_stats)) # every column is standardized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('{}/GMM_shape16_moments4_components500_alldata.pkl'.format(data_dir), 'rb') as f: # 500 component Gausssian mixture model fit to the 468202 20d samples\n",
    "    clf_GM500 = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('{}/shape_moments_GM500_labels.pkl'.format(data_dir), 'rb') as f: # 500 component Gausssian mixture model fit to the 468202 20d samples\n",
    "    shape_moments_GM500_labels = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('{}/shape_moments_GM500_proba.pkl'.format(data_dir), 'rb') as f: # 500 component Gausssian mixture model fit to the 468202 20d samples\n",
    "    shape_moments_GM500_proba = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"{}/reconstructions_from_model_2020-04-29_09-12-23.pkl\".format(data_dir), 'rb') as f: # output of LSTM autoencoder's decoder\n",
    "    segment_reconstructions = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shape_moments_GM500_labels = clf_GM500.predict(shape_moments)\n",
    "# with open('{}/shape_moments_GM500_labels.pkl'.format(data_dir), 'wb') as f:\n",
    "#     pickle.dump(shape_moments_GM500_labels, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shape_moments_GM500_proba = clf_GM500.predict_proba(shape_moments)\n",
    "# with open('{}/shape_moments_GM500_proba.pkl'.format(data_dir), 'wb') as f:\n",
    "#     pickle.dump(shape_moments_GM500_proba, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merge Gaussian Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_entropy(probs, comps):\n",
    "    \"\"\"\n",
    "    probs : array of data point probability under the components of the mixture model (output of predict_proba method), shape (n_observations, n_components)\n",
    "    comps : list of indices of components whose entropy is to be calculated\n",
    "    \"\"\"\n",
    "    return -np.nansum(probs.T[comps]*np.log10(probs.T[comps]))\n",
    "\n",
    "def calculate_information_gain(probs, comps):\n",
    "    \"\"\"\n",
    "    probs : array of data point probability under the components of the mixture model (output of predict_proba method), shape (n_observations, n_components)\n",
    "    comps : two indicies of components within probs whose merger entropy difference is to be calculated\n",
    "    \"\"\"\n",
    "    np.seterr(all = 'ignore') # ignore divide by zero coming from tiny probability values\n",
    "    seperate_ent = calculate_entropy(probs, comps)\n",
    "    merged_prob = probs.T[comps[0]]+probs.T[comps[1]]\n",
    "    merged_ent = -np.nansum(merged_prob*np.log10(merged_prob))\n",
    "    np.seterr(all = 'warn') \n",
    "    return seperate_ent-merged_ent\n",
    "\n",
    "def entropy_Gaussian_component_clustering(init_probability, data_classification, normalize_info_gain = True):\n",
    "    \"\"\"\n",
    "        Inputs :\n",
    "    init_probability : float numpy array of data point probability under the components of the mixture model (output of predict_proba method), with shape \n",
    "                        [n_observations, n_components]\n",
    "    data_classification : interger numpy array of indices that correspond to the Gaussian component assignment (output of predict method), with size [n_observations]\n",
    "    normalize_info_gain : True if information gain values calculated for possible merge pairs are to be divided by the number of data points involved. When False, the order \n",
    "                            of mergers tends to be biased towards components with large populations.  \n",
    "                            \n",
    "        Returns :\n",
    "    information_gain : dense matrix with shape [2*n_components-1, 2*n_components-1], containing difference in entropy caused by the merger of Gaussian mixture components\n",
    "                        (columns and rows up to n_components), and by the composit clusters (columns and rows beyond n_components). Only the upper triangle is utilised.\n",
    "                        Float -2.0 is placed in unused positions, Float -1.0 is placed in positions that require calculation.\n",
    "    merger_history : list of pairs of indices of components/clusters in the order of merging, size [n_components-1]\n",
    "    \"\"\"\n",
    "    # probability of data under the original 114 components, and under merged components\n",
    "    probs = np.hstack((init_probability, np.zeros((init_probability.shape[0], init_probability.shape[1]-1)))) # (468202, 227) for 114 components\n",
    "    # information gain from the merger of each pair of components\n",
    "    # only positions with -1 will need to be calculated (upper triangle of the symmetric matrix)\n",
    "    len_info_gains = 2*init_probability.shape[1]-1\n",
    "    info_gains = np.ones((len_info_gains,len_info_gains))*-1 \n",
    "    info_gains[np.tril_indices(len_info_gains, k=0, m=None)[0], np.tril_indices(len_info_gains, k=0, m=None)[1]] = -2. #-2 will be skipped\n",
    "    \n",
    "    if  normalize_info_gain == True:\n",
    "        #number of data points involved in the mergers\n",
    "        no_datapoints_components = np.unique(data_classification, return_counts=1)[1]\n",
    "        no_datapoints = np.hstack((no_datapoints_components, np.zeros(init_probability.shape[1]-1))) # components nad composite clusters\n",
    "\n",
    "    # for keeping track of indices of components that can still be merged\n",
    "    current_comps = list(range(init_probability.shape[1])) \n",
    "    merger_history = [] # indices of merged clusters\n",
    "    \n",
    "    for merger_ind in range(init_probability.shape[1]-1):\n",
    "        merger_ind += init_probability.shape[1] # this will be the index of next merged cluster's column within probability array \"probs\"\n",
    "        \n",
    "        if normalize_info_gain == True:\n",
    "            for k1 in current_comps:\n",
    "                for k2 in current_comps: # calculations proceed in rows\n",
    "                    if k1 == k2: continue # skip mergers with self\n",
    "                    if not info_gains[k1,k2] == -1.: continue # only evaluate for -1 (place-holder values)\n",
    "\n",
    "                    info_gains[k1,k2] = calculate_information_gain(probs, [k1,k2]) / np.sum((no_datapoints[k1],no_datapoints[k2]))\n",
    "        else:\n",
    "            for k1 in current_comps:\n",
    "                for k2 in current_comps: # calculations proceed in rows\n",
    "                    if k1 == k2: continue # skip mergers with self\n",
    "                    if not info_gains[k1,k2] == -1.: continue # only evaluate for -1 (place-holder values)\n",
    "\n",
    "                    info_gains[k1,k2] = calculate_information_gain(probs, [k1,k2])\n",
    "\n",
    "        # select the pair of currently available component indices that produces the highest gain value\n",
    "        sorted_gains = np.argsort(info_gains.flatten())[::-1]# descending list of indices of sorted gain values\n",
    "        max_gain_pos = np.unravel_index(sorted_gains[0], info_gains.shape)#  \n",
    "        gain_val_rank = 0\n",
    "        while (max_gain_pos[0] not in current_comps) or (max_gain_pos[1] not in current_comps):# find top info gain value for existing components\n",
    "            gain_val_rank+=1\n",
    "            max_gain_pos = np.unravel_index(sorted_gains[gain_val_rank], info_gains.shape)#(np.where(sorted_gains[gain_val_rank] ==info_gains)[0][0], np.where(sorted_gains[gain_val_rank] ==info_gains)[1][0])#np.unravel_index(sorted_gains[int(gain_val_rank)], info_gains.shape)\n",
    "\n",
    "        probs[:, merger_ind] = probs[:, max_gain_pos[0]]+probs[:, max_gain_pos[1]] # calculate data probability for the merged component\n",
    "        merger_history.append(max_gain_pos) # save component indices for this merger \n",
    "        if normalize_info_gain == True:\n",
    "            no_datapoints[merger_ind] = np.sum((no_datapoints[max_gain_pos[0]], no_datapoints[max_gain_pos[1]])) # record the number of data points in the new cluster\n",
    "\n",
    "        #remove indices of the merged components from the list and add the component's index\n",
    "        current_comps.remove(max_gain_pos[0])\n",
    "        current_comps.remove(max_gain_pos[1])\n",
    "        current_comps.append(merger_ind)\n",
    "\n",
    "\n",
    "        print(merger_ind-init_probability.shape[1], \"/{} mergers performed\".format(init_probability.shape[1]))\n",
    "        clear_output(wait=1)\n",
    "        \n",
    "    print(\"Finished.\")\n",
    "    return info_gains, merger_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "info_gains, merger_history = entropy_Gaussian_component_clustering(shape_moments_GM500_proba, shape_moments_GM500_labels, normalize_info_gain = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pwlf\n",
    "plt.close()\n",
    "\n",
    "chrono_gains = [info_gains[x,y] for x,y in merger_history]\n",
    "\n",
    "\n",
    "x = np.array(list(range(len(merger_history))), dtype=float)+1\n",
    "y = np.array(chrono_gains)\n",
    "\n",
    "my_pwlf = pwlf.PiecewiseLinFit(x, y)\n",
    "breaks = my_pwlf.fit(2)\n",
    "print(breaks)\n",
    "\n",
    "x_hat = np.linspace(x.min(), x.max(), 100)\n",
    "y_hat = my_pwlf.predict(x_hat)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(x, y, 'o')\n",
    "plt.plot(x_hat, y_hat, '-')\n",
    "plt.ylabel(\"Difference in entropy per data point\")\n",
    "plt.xlabel(\"Number of mergers performed\")\n",
    "plt.text(x=60, y=0.08, s=\"breakpoints: \"+str([str(break1)[:5] for break1 in breaks[1:3]]))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a 468202 list of classifications for the new clusters\n",
    "\n",
    "import copy\n",
    "\n",
    "no_comps =500\n",
    "\n",
    "starting_components = list(range(no_comps)) \n",
    "merger_state = dict((\"merger \"+str(merger_no), dict()) for merger_no in list(range(no_comps)))\n",
    "merger_state[\"merger 0\"]=dict((\"cluster \"+str(component),[component]) for component in starting_components)\n",
    "for merger_no, merger_comps in enumerate(merger_history):\n",
    "    merger_state[\"merger \"+str(merger_no+1)] = copy.deepcopy(merger_state[\"merger \"+str(merger_no)])\n",
    "    merged_components = copy.deepcopy([merger_state[\"merger \"+str(merger_no+1)][\"cluster \"+str(merger_comps[0])], merger_state[\"merger \"+str(merger_no+1)][\"cluster \"+str(merger_comps[1])]])\n",
    "    del merger_state[\"merger \"+str(merger_no+1)][\"cluster \"+str(merger_comps[0])]\n",
    "    del merger_state[\"merger \"+str(merger_no+1)][\"cluster \"+str(merger_comps[1])]\n",
    "    merger_state[\"merger \"+str(merger_no+1)][\"cluster \"+str(merger_no+no_comps)] = [item for sublist in merged_components for item in sublist]\n",
    "\n",
    "# component_datapoints = np.zeros(no_comps+len(merger_history))\n",
    "# component_datapoints[:no_comps] = component_vol_members_density_df.members.values\n",
    "\n",
    "# for merger_ind, merger in enumerate(merger_history):\n",
    "#     merger_ind+=no_comps\n",
    "#     component_datapoints[merger_ind] = component_datapoints[merger[0]]+component_datapoints[merger[1]]\n",
    "\n",
    "# for merger_key,merger_vals in merger_state.items():\n",
    "#     # count number of components that make up each cluster, then find how common the counts are\n",
    "#     no_unique_component_counts = np.unique([len(comp_vals) for comp_key, comp_vals in merger_vals.items()], return_counts=True) \n",
    "#     comps_involved = [int(cluster_str[8:]) for cluster_str in merger_vals.keys()]\n",
    "#     print(merger_key,\"\\tnumber of clusters: \", len(merger_vals),\"\\t sizes of clusters, number of each size of cluster\", no_unique_component_counts, \"\\tmax cluster size: \",\n",
    "#           np.max([component_datapoints[comp_ind] for comp_ind in comps_involved]))\n",
    "\n",
    "\n",
    "no_mergers_to_perform = x\n",
    "\n",
    "composite_clusters = merger_state[\"merger {}\".format(no_mergers_to_perform)]\n",
    "\n",
    "new_classification = []\n",
    "\n",
    "for label in shape_moments_GM500_labels:\n",
    "    for cluster, components in composite_clusters.items():\n",
    "    if label in components:\n",
    "        new_classification.append(int(cluster.split(\" \")[1]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare clusters with Tomaso's classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load observation classifications from Huppenkothen 2017\n",
    "clean_belloni = open('{}/1915Belloniclass_updated.dat'.format(data_dir))\n",
    "lines = clean_belloni.readlines()\n",
    "states = lines[0].split()\n",
    "belloni_clean = {}\n",
    "for h,l in zip(states, lines[1:]):\n",
    "    belloni_clean[h] = l.split()\n",
    "    #state: obsID1, obsID2...\n",
    "ob_state = {}\n",
    "for state, obs in belloni_clean.items():\n",
    "    if state == \"chi1\" or state == \"chi2\" or state == \"chi3\" or state == \"chi4\": state = \"chi\"\n",
    "    for ob in obs:\n",
    "        ob_state[ob] = state\n",
    "        \n",
    "        \n",
    "# load IDs of segmented light curves: observationsID_segmentIndex\n",
    "with open('{}/468202_len128_s2_4cad_ids_errorfix.pkl'.format(data_dir), 'rb') as f:\n",
    "    seg_ids = pickle.load(f)\n",
    "\n",
    "        \n",
    "seg_ObIDs = [seg.split(\"_\")[0] for seg in seg_ids] # get rid of the within-observation segment indices and create a degenerate list of observation IDs\n",
    "\n",
    "classes = np.array([\"alpha\", \"beta\", \"gamma\", \"delta\", \"theta\", \"kappa\", \"lambda\", \"mu\", \"nu\", \"rho\", \"phi\", \"chi\", \"eta\", \"omega\"])\n",
    "scales = []\n",
    "segment_class = []\n",
    "for ob in seg_ObIDs:\n",
    "    if ob in ob_state:\n",
    "        segment_class.append(ob_state[ob])\n",
    "    else:\n",
    "        segment_class.append(\"Unknown\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pylab as plt\n",
    "\n",
    "uniform_data = np.random.rand(10, 12)\n",
    "ax = sns.heatmap(uniform_data, linewidth=0.5)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
