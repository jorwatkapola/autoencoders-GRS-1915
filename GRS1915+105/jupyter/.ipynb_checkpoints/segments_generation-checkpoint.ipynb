{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from IPython.display import clear_output\n",
    "# from segment_cluster import segmentation\n",
    "# import importlib\n",
    "import os\n",
    "import fnmatch\n",
    "import numpy as np\n",
    "# # from sklearn.model_selection import train_test_split\n",
    "# from collections import Counter\n",
    "# import csv\n",
    "# from sklearn import tree\n",
    "# import sys\n",
    "# sys.stdout.flush()\n",
    "# import math\n",
    "import pickle\n",
    "# from scipy.stats import zscore\n",
    "# import datetime\n",
    "# import pytz\n",
    "import matplotlib.pyplot as plt\n",
    "# import lc_functions as lcf\n",
    "\n",
    "# from matplotlib.table import Table\n",
    "\n",
    "\n",
    "# from tensorflow.keras.models import Sequential\n",
    "# from tensorflow.keras.layers import LSTM\n",
    "# from tensorflow.keras.layers import Dense\n",
    "# from tensorflow.keras.layers import RepeatVector\n",
    "# from tensorflow.keras.layers import TimeDistributed\n",
    "# from tensorflow.keras.callbacks import EarlyStopping\n",
    "# from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "# from tensorflow.keras.utils import plot_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "lcs=[]\n",
    "ids=[]\n",
    "\n",
    "for root, dirnames, filenames in os.walk(\"/data/jkok1g14/data_GRS1915/std1\"): #Std1_PCU2\n",
    "    for filename in fnmatch.filter(filenames, \"*_std1_lc.txt\"):\n",
    "        lc = os.path.join(root, filename)\n",
    "        ids.append(filename.split(\"_\")[0])\n",
    "        f=np.loadtxt(lc)\n",
    "        f=np.transpose(f)#,axis=1)\n",
    "        #f=f[0:2]\n",
    "        ###1s average and time check to eliminate points outside of GTIs\n",
    "        f8t = np.mean(f[0][:(len(f[0])//8)*8].reshape(-1, 8), axis=1)\n",
    "        f8c = np.mean(f[1][:(len(f[1])//8)*8].reshape(-1, 8), axis=1)\n",
    "        f8e = np.sqrt(np.sum(f[2][:(len(f[2])//8)*8].reshape(-1, 8)**2, axis=1))/8\n",
    "        rm_points = []\n",
    "        skip=False\n",
    "        for i in range(len(f8t)-1):\n",
    "            if skip==True:\n",
    "                skip=False\n",
    "                continue\n",
    "            delta = f8t[i+1]-f8t[i]\n",
    "            if delta > 1.0:\n",
    "                rm_points.append(i+1)\n",
    "                skip=True\n",
    "\n",
    "        times=np.delete(f8t,rm_points)\n",
    "        counts=np.delete(f8c,rm_points)\n",
    "        errors=np.delete(f8e,rm_points)\n",
    "        lcs.append(np.stack((times,counts, errors)))\n",
    "        break\n",
    "    if len(lcs)>0:\n",
    "        break\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('../../../data_GRS1915/1776_light_curves_1s_bin_errorfix.pkl', 'wb') as f:\n",
    "#     pickle.dump(lcs, f)\n",
    "# with open('../../../data_GRS1915/1776_light_curves_1s_bin_ids_errorfix.pkl', 'wb') as f:\n",
    "#     pickle.dump(ids, f)\n",
    "\n",
    "with open('../../../data_GRS1915/1776_light_curves_1s_bin_errorfix.pkl', 'rb') as f:\n",
    "    lcs = pickle.load(f)\n",
    "with open('../../../data_GRS1915/1776_light_curves_1s_bin_ids_errorfix.pkl', 'rb') as f:\n",
    "    ids = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_belloni = open('../../../data_GRS1915/1915Belloniclass_updated.dat')\n",
    "lines = clean_belloni.readlines()\n",
    "states = lines[0].split()\n",
    "belloni_clean = {}\n",
    "for h,l in zip(states, lines[1:]):\n",
    "    belloni_clean[h] = l.split()\n",
    "    #state: obsID1, obsID2...\n",
    "ob_state = {}\n",
    "for state, obs in belloni_clean.items():\n",
    "    if state == \"chi1\" or state == \"chi2\" or state == \"chi3\" or state == \"chi4\": state = \"chi\"\n",
    "    for ob in obs:\n",
    "        ob_state[ob] = state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def segmentation(time_series, seg_len, stride, keep_time_stamps=True, experimental = False):\n",
    "    \"\"\"\n",
    "    Create a list of 1D (when time_stamps=False) or 2D (when time_stamps=True) arrays, which are overlappig segments of ts. Incomplete fragments are rejected.\n",
    "    \n",
    "    time_series = time series to be segmented\n",
    "    seg_len = length of a segment, \n",
    "    stride = step size; difference in the starting position of the consecutive segments\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    segments=[]\n",
    "    for start in range(0, len(time_series[0])-seg_len, stride):\n",
    "        end=start+seg_len\n",
    "        ############################################# *4 because of the 4 second cadance \n",
    "        if time_series[0][end]-time_series[0][start] != seg_len: #don't allow temporally discontinous segments\n",
    "            continue\n",
    "        if keep_time_stamps==True:\n",
    "            segments.append(time_series[:,start:end])\n",
    "        else:\n",
    "            segments.append(time_series[1:,start:end])\n",
    "    return np.array(segments) # check why time stamps are kept \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1730 /1776\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "segments_counts=[]\n",
    "segments_errors=[]\n",
    "seg_ids=[]\n",
    "for lc_index, lc in enumerate(lcs):\n",
    "    if len(lc[1]) >= 512: \n",
    "        segments = segmentation(lc, 512, 40, keep_time_stamps=False, experimental = False)\n",
    "    else:\n",
    "        continue\n",
    "    if len(segments) > 0:\n",
    "        segments_counts.append(segments[:,0,:])\n",
    "        segments_errors.append(segments[:,1,:])\n",
    "        seg_ids.append(ids[lc_index])\n",
    "        print(lc_index+1, \"/{}\".format(len(lcs)))\n",
    "        clear_output(wait=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_per_seg = []  # for each light curve, copy the observation id for every segment of the light curve\n",
    "for lc_index, lc in enumerate(segments_counts):\n",
    "    for i in range(len(lc)):\n",
    "        id_per_seg.append(seg_ids[lc_index]+\"_{}\".format(i))\n",
    "        \n",
    "segments_counts=np.vstack(segments_counts)\n",
    "segments_errors=np.vstack(segments_errors)\n",
    "segments_counts = np.expand_dims(segments_counts, axis=-1)\n",
    "segments_errors = np.expand_dims(segments_errors, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng_state = np.random.get_state()\n",
    "np.random.shuffle(segments_counts)\n",
    "np.random.set_state(rng_state)\n",
    "np.random.shuffle(segments_errors)\n",
    "np.random.set_state(rng_state)\n",
    "np.random.shuffle(id_per_seg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('../../../data_GRS1915/94465_len512_s40_counts_errorfix.pkl', 'wb') as f:\n",
    "#     pickle.dump(segments_counts, f)\n",
    "    \n",
    "# with open('../../../data_GRS1915/94465_len512_s40_errors_errorfix.pkl', 'wb') as f:\n",
    "#     pickle.dump(segments_errors, f)\n",
    "    \n",
    "# with open('../../../data_GRS1915/94465_len512_s40_ids_errorfix.pkl', 'wb') as f:\n",
    "#     pickle.dump(id_per_seg, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "lcs=[]\n",
    "ids=[]\n",
    "\n",
    "cadence=4 # seconds\n",
    "binned_stamps = int(cadence/0.125)\n",
    "\n",
    "for root, dirnames, filenames in os.walk(\"/data/jkok1g14/data_GRS1915/std1\"): #Std1_PCU2\n",
    "    for filename in fnmatch.filter(filenames, \"*_std1_lc.txt\"):\n",
    "        lc = os.path.join(root, filename)\n",
    "        ids.append(filename.split(\"_\")[0])\n",
    "        f=np.loadtxt(lc)\n",
    "        f=np.transpose(f)#,axis=1)\n",
    "        #f=f[0:2]\n",
    "        ###1s average and time check to eliminate points outside of GTIs\n",
    "        fbinned_t = np.mean(f[0][:(len(f[0])//binned_stamps)*binned_stamps].reshape(-1, binned_stamps), axis=1)\n",
    "        fbinned_c = np.mean(f[1][:(len(f[1])//binned_stamps)*binned_stamps].reshape(-1, binned_stamps), axis=1)\n",
    "        fbinned_e = np.sqrt(np.sum(f[2][:(len(f[2])//binned_stamps)*binned_stamps].reshape(-1, binned_stamps)**2, axis=1))/binned_stamps\n",
    "        rm_points = []\n",
    "        skip=False\n",
    "        for i in range(len(fbinned_t)-1):\n",
    "            if skip==True:\n",
    "                skip=False\n",
    "                continue\n",
    "            delta = fbinned_t[i+1]-fbinned_t[i]\n",
    "            if delta > cadence:\n",
    "                rm_points.append(i+1)\n",
    "                skip=True\n",
    "\n",
    "        times=np.delete(fbinned_t,rm_points)\n",
    "        counts=np.delete(fbinned_c,rm_points)\n",
    "        errors=np.delete(fbinned_e,rm_points)\n",
    "        lcs.append(np.stack((times,counts, errors)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('../../../data_GRS1915/1776_light_curves_4s_bin_errorfix.pkl', 'wb') as f:\n",
    "#     pickle.dump(lcs, f)\n",
    "# with open('../../../data_GRS1915/1776_light_curves_4s_bin_ids_errorfix.pkl', 'wb') as f:\n",
    "#     pickle.dump(ids, f)\n",
    "\n",
    "# with open('../../../data_GRS1915/1776_light_curves_4s_bin_errorfix.pkl', 'rb') as f:\n",
    "#     lcs = pickle.load(f)\n",
    "# with open('../../../data_GRS1915/1776_light_curves_4s_bin_ids_errorfix.pkl', 'rb') as f:\n",
    "#     ids = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "def segmentation(time_series, seg_len, stride, keep_time_stamps=True, experimental = False, cadence=4):\n",
    "    \"\"\"\n",
    "    Create a list of 1D (when time_stamps=False) or 2D (when time_stamps=True) arrays, which are overlappig segments of ts. Incomplete fragments are rejected.\n",
    "    \n",
    "    time_series = time series to be segmented\n",
    "    seg_len = length of a segment, \n",
    "    stride = step size; difference in the starting position of the consecutive segments\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    segments=[]\n",
    "    for start in range(0, len(time_series[0])-seg_len, stride):\n",
    "        end=start+seg_len\n",
    "        ############################################# *4 because of the 4 second cadance \n",
    "        if time_series[0][end]-time_series[0][start] != seg_len*cadence: #don't allow temporally discontinous segments\n",
    "            continue\n",
    "        if keep_time_stamps==True:\n",
    "            segments.append(time_series[:,start:end])\n",
    "        else:\n",
    "            segments.append(time_series[1:,start:end])\n",
    "    return np.array(segments) # check why time stamps are kept \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1776 /1776\n"
     ]
    }
   ],
   "source": [
    "segments_counts=[]\n",
    "segments_errors=[]\n",
    "seg_ids=[]\n",
    "\n",
    "seg_len_s = 1024 # seconds \n",
    "stride_s = 64 # seconds\n",
    "cadence=4 # seconds\n",
    "\n",
    "seg_len = seg_len_s//cadence\n",
    "stride = stride_s//cadence\n",
    "\n",
    "\n",
    "\n",
    "for lc_index, lc in enumerate(lcs):\n",
    "    if len(lc[1]) >= seg_len: \n",
    "        segments = segmentation(lc, seg_len, stride, keep_time_stamps=False, experimental = False)\n",
    "    else:\n",
    "        continue\n",
    "    if len(segments) > 0:\n",
    "        segments_counts.append(segments[:,0,:])\n",
    "        segments_errors.append(segments[:,1,:])\n",
    "        seg_ids.append(ids[lc_index])\n",
    "        print(lc_index+1, \"/{}\".format(len(lcs)))\n",
    "        clear_output(wait=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_per_seg = []  # for each light curve, copy the observation id for every segment of the light curve\n",
    "for lc_index, lc in enumerate(segments_counts):\n",
    "    for i in range(len(lc)):\n",
    "        id_per_seg.append(seg_ids[lc_index]+\"_{}\".format(i))\n",
    "        \n",
    "segments_counts=np.vstack(segments_counts)\n",
    "segments_errors=np.vstack(segments_errors)\n",
    "segments_counts = np.expand_dims(segments_counts, axis=-1)\n",
    "segments_errors = np.expand_dims(segments_errors, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng_state = np.random.get_state()\n",
    "np.random.shuffle(segments_counts)\n",
    "np.random.set_state(rng_state)\n",
    "np.random.shuffle(segments_errors)\n",
    "np.random.set_state(rng_state)\n",
    "np.random.shuffle(id_per_seg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40454"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(segments_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('../../../data_GRS1915/40454_len256_s16_4cad_counts_errorfix.pkl', 'wb') as f:\n",
    "#     pickle.dump(segments_counts, f)\n",
    "    \n",
    "# with open('../../../data_GRS1915/40454_len256_s16_4cad_errors_errorfix.pkl', 'wb') as f:\n",
    "#     pickle.dump(segments_errors, f)\n",
    "    \n",
    "# with open('../../../data_GRS1915/40454_len256_s16_4cad_ids_errorfix.pkl', 'wb') as f:\n",
    "#     pickle.dump(id_per_seg, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
