{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from IPython.display import clear_output\n",
    "import os\n",
    "import fnmatch\n",
    "import numpy as np\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "cwd = os.getcwd()\n",
    "\n",
    "if cwd.split(\"/\")[1] == \"export\":\n",
    "    data_dir = \"../../../files_from_snuffy\"\n",
    "else:\n",
    "    data_dir = \"../../../data_GRS1915\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load light curves from txt files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lcs=[] # light curves (time stamps, count rate, uncertainty)\n",
    "lc_ids=[] # observation ids\n",
    "\n",
    "for root, dirnames, filenames in os.walk(\"{}/std1\".format(data_dir)): #Std1_PCU2\n",
    "    for filename in fnmatch.filter(filenames, \"*_std1_lc.txt\"):\n",
    "        lc = os.path.join(root, filename)\n",
    "        lc_ids.append(filename.split(\"_\")[0])\n",
    "        f=np.loadtxt(lc)\n",
    "        f=np.transpose(f)\n",
    "        lcs.append(f)\n",
    "        print(\"Loaded {} lightcurves\".format(len(lcs)))\n",
    "        clear_output(wait=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binning(times, counts, errors, output_cadence, input_cadence=0.125):\n",
    "    \"\"\"\n",
    "    Bin the input time series. Time series must contain an array of time stamps, \n",
    "    count values and uncertainty on the count.\n",
    "    Make sure that count rates are transformed to count values (uncertainty should be equal to the square root of the count).\n",
    "    \n",
    "    times = 1D array of bin time stamps\n",
    "    counts = 1D array of couts per bin \n",
    "    errors = 1D array of uncertainty of counts\n",
    "    input_cadence = input cadence in seconds\n",
    "    output_cadence = desired cadence in seconds\n",
    "    \"\"\"\n",
    "    binned_stamps = int(output_cadence/input_cadence) # how many data points to bin\n",
    "    \n",
    "    times = np.mean(times[:(len(times)//binned_stamps)*binned_stamps].reshape(-1, binned_stamps), axis=1)\n",
    "    counts = np.sum(counts[:(len(counts)//binned_stamps)*binned_stamps].reshape(-1, binned_stamps), axis=1)\n",
    "    errors = np.sqrt(counts)\n",
    "    rm_points = []\n",
    "    skip=False\n",
    "    for i in range(len(times)-1):\n",
    "        if skip==True:\n",
    "            skip=False\n",
    "            continue\n",
    "        delta = times[i+1]-times[i]\n",
    "        if delta > output_cadence:\n",
    "            rm_points.append(i+1)\n",
    "            skip=True\n",
    "\n",
    "    times=np.delete(times,rm_points)\n",
    "    counts=np.delete(counts,rm_points)\n",
    "    errors=np.delete(errors,rm_points)\n",
    "    return np.stack((times,counts, errors))\n",
    "\n",
    "def segmentation(time_series, seg_len, stride, keep_time_stamps=True, experimental = False, input_cadence=1):\n",
    "    \"\"\"\n",
    "    Create a list of 1D (when time_stamps=False) or 2D (when time_stamps=True) arrays, which are overlappig segments of ts. Incomplete fragments are rejected.\n",
    "\n",
    "    time_series = time series to be segmented\n",
    "    seg_len = length of a segment, \n",
    "    stride = step size; difference in the starting position of the consecutive segments\n",
    "    \"\"\"\n",
    "    segments=[]\n",
    "    for start in range(0, len(time_series[0])-seg_len, stride):\n",
    "        end=start+seg_len\n",
    "        if time_series[0][end]-time_series[0][start] != seg_len*input_cadence: #don't allow segments outside of good time intervals\n",
    "            continue\n",
    "        if keep_time_stamps==True:\n",
    "            segments.append(time_series[:,start:end])\n",
    "        else:\n",
    "            segments.append(time_series[1:,start:end])\n",
    "    return np.array(segments) # check why time stamps are kept "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Re-bin light curves to 4 second bins and segment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_cadence=4 # in seconds\n",
    "input_cadence=0.125\n",
    "\n",
    "binned_lcs = []\n",
    "for lc in lcs:\n",
    "    lc_rate = np.copy(lc)\n",
    "    # transform count rate values to counts per bin\n",
    "    lc_rate[1] *=input_cadence # photon count values\n",
    "    lc_rate[2] *=input_cadence # uncertainty values\n",
    "    # bin to 1 second cadence\n",
    "    binned_lc = binning(lc_rate[0],lc_rate[1],lc_rate[2], output_cadence=output_cadence, input_cadence=input_cadence)\n",
    "    # transform back to count rate\n",
    "    binned_lc[1] /=output_cadence # photon count values\n",
    "    binned_lc[2] /=output_cadence # uncertainty values\n",
    "    binned_lcs.append(binned_lc)\n",
    "    print(\"Binned {} light curves\".format(len(binned_lcs)))\n",
    "    clear_output(wait=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cadence=4\n",
    "seg_len_s=512\n",
    "stride_s=8\n",
    "\n",
    "segments_counts=[]\n",
    "segments_times = []\n",
    "segments_errors=[]\n",
    "seg_ids=[]\n",
    "\n",
    "\n",
    "seg_len = seg_len_s//cadence # segment length and stride size in data points\n",
    "stride = stride_s//cadence\n",
    "\n",
    "\n",
    "\n",
    "for lc_index, lc in enumerate(binned_lcs):\n",
    "    if len(lc[1]) >= seg_len: \n",
    "        segments = segmentation(lc, seg_len, stride, keep_time_stamps=True, experimental = False, input_cadence=cadence)\n",
    "    else:\n",
    "        continue\n",
    "    if len(segments) > 0:\n",
    "        segments_times.append(segments[:,0,:])\n",
    "        segments_counts.append(segments[:,1,:])\n",
    "        segments_errors.append(segments[:,2,:])\n",
    "        seg_ids.append(lc_ids[lc_index])\n",
    "        print(\"Segmented {}/{} light curves.\".format(lc_index+1, len(lcs)))\n",
    "        clear_output(wait=True)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Stacking the segments and creating segment IDs, shuffling.\")\n",
    "id_per_seg = []  # for each light curve, copy the observation id for every segment of the light curve\n",
    "for lc_index, lc in enumerate(segments_counts):\n",
    "    for i in range(len(lc)):\n",
    "        id_per_seg.append(seg_ids[lc_index]+\"_{}\".format(i))\n",
    "\n",
    "segments_counts=np.vstack(segments_counts)\n",
    "segments_errors=np.vstack(segments_errors)\n",
    "segments_counts = np.expand_dims(segments_counts, axis=-1)\n",
    "segments_errors = np.expand_dims(segments_errors, axis=-1)\n",
    "\n",
    "rng_state = np.random.get_state()\n",
    "np.random.shuffle(segments_counts)\n",
    "np.random.set_state(rng_state)\n",
    "np.random.shuffle(segments_errors)\n",
    "np.random.set_state(rng_state)\n",
    "np.random.shuffle(id_per_seg)\n",
    "\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('{}/468202_len128_stride8_4sec_cad_countrates_sum_bin.pkl'.format(data_dir), 'wb') as f:\n",
    "#     pickle.dump(segments_counts, f)\n",
    "    \n",
    "# with open('{}/468202_len128_stride8_4sec_cad_errors_sum_bin.pkl'.format(data_dir), 'wb') as f:\n",
    "#     pickle.dump(segments_errors, f)\n",
    "    \n",
    "# with open('{}/468202_len128_stride8_4sec_cad_ids_sum_bin.pkl'.format(data_dir), 'wb') as f:\n",
    "#     pickle.dump(id_per_seg, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Re-bin light curves to 1 second bins and segment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_cadence=1 # in seconds\n",
    "input_cadence=0.125\n",
    "\n",
    "binned_lcs = []\n",
    "for lc in lcs:\n",
    "    lc_rate = np.copy(lc)\n",
    "    # transform count rate values to counts per bin\n",
    "    lc_rate[1] *=input_cadence # photon count values\n",
    "    lc_rate[2] *=input_cadence # uncertainty values\n",
    "    # bin to 1 second cadence\n",
    "    binned_lc = binning(lc_rate[0],lc_rate[1],lc_rate[2], output_cadence=output_cadence, input_cadence=input_cadence)\n",
    "    # transform back to count rate\n",
    "    binned_lc[1] /=output_cadence # photon count values\n",
    "    binned_lc[2] /=output_cadence # uncertainty values\n",
    "    binned_lcs.append(binned_lc)\n",
    "    print(\"Binned {} light curves\".format(len(binned_lcs)))\n",
    "    clear_output(wait=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# observations missing from the 4 second dataset must be filtered out.\n",
    "# prepare a list of observation IDs\n",
    "with open('{}/468202_len128_stride8_4sec_cad_ids_sum_bin.pkl'.format(data_dir), 'rb') as f:\n",
    "    seg_ids_4s = pickle.load(f)\n",
    "ob_IDs_4s = np.unique([seg.split(\"_\")[0] for seg in seg_ids_4s]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cadence=1\n",
    "seg_len_s=128\n",
    "stride_s=10\n",
    "\n",
    "segments_counts=[]\n",
    "segments_times = []\n",
    "segments_errors=[]\n",
    "seg_ids=[]\n",
    "\n",
    "\n",
    "seg_len = seg_len_s//cadence # segment length and stride size in data points\n",
    "stride = stride_s//cadence\n",
    "\n",
    "\n",
    "\n",
    "for lc_index, lc in enumerate(binned_lcs):\n",
    "    if lc_ids[lc_index] in ob_IDs_4s: # filter out observations missing from the 4 second dataset\n",
    "        if len(lc[1]) >= seg_len: \n",
    "            segments = segmentation(lc, seg_len, stride, keep_time_stamps=True, experimental = False, input_cadence=cadence)\n",
    "        else:\n",
    "            continue\n",
    "        if len(segments) > 0:\n",
    "            segments_times.append(segments[:,0,:])\n",
    "            segments_counts.append(segments[:,1,:])\n",
    "            segments_errors.append(segments[:,2,:])\n",
    "            seg_ids.append(lc_ids[lc_index])\n",
    "            print(\"Segmented {}/{} light curves.\".format(lc_index+1, len(lcs)))\n",
    "            clear_output(wait=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Stacking the segments and creating segment IDs, shuffling.\")\n",
    "id_per_seg = []  # for each light curve, copy the observation id for every segment of the light curve\n",
    "for lc_index, lc in enumerate(segments_counts):\n",
    "    for i in range(len(lc)):\n",
    "        id_per_seg.append(seg_ids[lc_index]+\"_{}\".format(i))\n",
    "\n",
    "segments_counts=np.vstack(segments_counts)\n",
    "segments_errors=np.vstack(segments_errors)\n",
    "segments_counts = np.expand_dims(segments_counts, axis=-1)\n",
    "segments_errors = np.expand_dims(segments_errors, axis=-1)\n",
    "\n",
    "rng_state = np.random.get_state()\n",
    "np.random.shuffle(segments_counts)\n",
    "np.random.set_state(rng_state)\n",
    "np.random.shuffle(segments_errors)\n",
    "np.random.set_state(rng_state)\n",
    "np.random.shuffle(id_per_seg)\n",
    "\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "segments_counts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('{}/474471_len128_stride10_1sec_cad_countrates_sum_bin.pkl'.format(data_dir), 'wb') as f:\n",
    "#     pickle.dump(segments_counts, f)\n",
    "    \n",
    "# with open('{}/474471_len128_stride10_1sec_cad_errors_sum_bin.pkl'.format(data_dir), 'wb') as f:\n",
    "#     pickle.dump(segments_errors, f)\n",
    "    \n",
    "# with open('{}/474471_len128_stride10_1sec_cad_ids_sum_bin.pkl'.format(data_dir), 'wb') as f:\n",
    "#     pickle.dump(id_per_seg, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make the training/validation/testing split\n",
    "## Load classifications from Huppenkothen+2017"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_belloni = open('{}/1915Belloniclass_updated.dat'.format(data_dir))\n",
    "lines = clean_belloni.readlines()\n",
    "states = lines[0].split()\n",
    "belloni_clean = {}\n",
    "for h,l in zip(states, lines[1:]):\n",
    "    belloni_clean[h] = l.split()\n",
    "    #state: obsID1, obsID2...\n",
    "ob_state = {}\n",
    "for state, obs in belloni_clean.items():\n",
    "    if state == \"chi1\" or state == \"chi2\" or state == \"chi3\" or state == \"chi4\": state = \"chi\"\n",
    "    for ob in obs:\n",
    "        ob_state[ob] = state\n",
    "\n",
    "        \n",
    "# inverse the ob_state dictionary, so that inv_ob_state contains {\"state name\" : [list of observation IDs], ...}\n",
    "\n",
    "inv_ob_state = {}\n",
    "for k, v in ob_state.items():\n",
    "    inv_ob_state[v] = inv_ob_state.get(v, [])\n",
    "    inv_ob_state[v].append(k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "## Split observation IDs \n",
    "- according to the 7/1/2 ratio w.r.t. the number of data points in the light curves\n",
    "- stratify the split for the classified subset of data to ensure training/testing completeness\n",
    "- account for the fact that 4second data is missing some of the observations. Split must be based on this data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('{}/468202_len128_stride8_4sec_cad_ids_sum_bin.pkl'.format(data_dir), 'rb') as f:\n",
    "    seg_ids_4s = pickle.load(f)\n",
    "with open('{}/475765_len128_stride10_1sec_cad_ids_sum_bin.pkl'.format(data_dir), 'rb') as f:\n",
    "    seg_ids_1s = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get rid of the within-observation segment indices and create a degenerate list of observation IDs\n",
    "seg_ob_IDs = np.unique([seg.split(\"_\")[0] for seg in seg_ids_4s]) \n",
    "\n",
    "# create list of segment classifications\n",
    "classes = np.array([\"alpha\", \"beta\", \"gamma\", \"delta\", \"theta\", \"kappa\", \"lambda\", \"mu\", \"nu\", \"rho\", \"phi\", \"chi\", \"eta\", \"omega\"])\n",
    "seg_class = []\n",
    "for seg in seg_ob_IDs:\n",
    "    if seg in ob_state:\n",
    "        seg_class.append(ob_state[seg])\n",
    "    else:\n",
    "        seg_class.append(\"Unknown\")\n",
    "        \n",
    "print(np.unique(seg_class, return_counts=True)) # class eta doesn't have enough observations for stratification, \n",
    "# all of them will be included in the training set and the class won't be included in the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len([ob for ob in np.unique(seg_ob_IDs) if ob in ob_state.keys()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use only observations present in both datasets\n",
    "with open('{}/468202_len128_stride8_4sec_cad_ids_sum_bin.pkl'.format(data_dir), 'rb') as f:\n",
    "    seg_ids_4s = pickle.load(f)\n",
    "ob_IDs_4s = np.unique([seg.split(\"_\")[0] for seg in seg_ids_4s]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(seed=11)\n",
    "\n",
    "# calculate total number of data points in 1738 observations\n",
    "# obsrvation split will be common to all data sets\n",
    "total_data_volume = 0\n",
    "for lc_index, lc in enumerate(lcs):\n",
    "    if lc_ids[lc_index] in ob_IDs_4s:\n",
    "        total_data_volume += len(lc[0])\n",
    "needed_validation_data = total_data_volume*0.1\n",
    "needed_testing_data = total_data_volume*0.2\n",
    "\n",
    "test_set_size = 0\n",
    "val_set_size = 0\n",
    "train_set_size = 0\n",
    "\n",
    "test_set = []\n",
    "val_set = []\n",
    "train_set = []\n",
    "\n",
    "# 1738 observations found in the 4second data set. this is the subset that will be class-stratified\n",
    "obs_to_split = ob_IDs_4s\n",
    "\n",
    "# split is stratified, so done separately for each class\n",
    "# eta class has only 2 classified observations, so both will be in the training set\n",
    "for class_name in [\"alpha\", \"beta\", \"gamma\", \"delta\", \"theta\", \"kappa\", \"lambda\", \"mu\", \"nu\", \"rho\", \"phi\", \"chi\", \"omega\"]:\n",
    "\n",
    "    class_obs_all = inv_ob_state[class_name] # all labeled observation IDs of this class\n",
    "    class_obs = [] # exclude observations which did not produce any light curve segments\n",
    "    for ob in class_obs_all:\n",
    "        if ob in obs_to_split:\n",
    "            class_obs.append(ob)\n",
    "\n",
    "    # pick 20% of observations for the test set, at least 1 observation\n",
    "    test_obs = np.random.choice(class_obs, size=int(np.ceil(len(class_obs)*0.2)), replace=False) \n",
    "    \n",
    "    for ob in test_obs:\n",
    "        test_set_size += len(lcs[np.where(np.array(lc_ids) == ob)[0][0]][0]) # add the length of the light curve\n",
    "    \n",
    "    if len(test_obs) == 0:\n",
    "        print(class_name)\n",
    "    \n",
    "    # remove test observations from the class_obs list\n",
    "    class_obs = [ob for ob in class_obs if ob not in test_obs]\n",
    "    \n",
    "    # pick 10% of observations for the valdiation set, at least 1 observation\n",
    "    val_obs = np.random.choice(class_obs, size=int(np.ceil(len(class_obs)*0.1)), replace=False)\n",
    "    \n",
    "    for ob in val_obs:\n",
    "        val_set_size += len(lcs[np.where(np.array(lc_ids) == ob)[0][0]][0]) # add the length of the light curve\n",
    "\n",
    "    if len(val_obs) == 0:\n",
    "        print(class_name)\n",
    "        \n",
    "    # remove val observations from the class_obs list\n",
    "    class_obs = [ob for ob in class_obs if ob not in val_obs]\n",
    "\n",
    "    # use the remaining observations as training set\n",
    "    train_obs = []\n",
    "    for ob in class_obs:\n",
    "        train_obs.append(ob)\n",
    "        train_set_size += len(lcs[np.where(np.array(lc_ids) == ob)[0][0]][0]) # add the length of the light curve\n",
    "    \n",
    "    if len(train_obs) == 0:\n",
    "        print(class_name)\n",
    "    \n",
    "    test_set.append(test_obs)\n",
    "    val_set.append(val_obs)\n",
    "    train_set.append(train_obs)\n",
    "    \n",
    "class_obs_all = inv_ob_state[\"eta\"] # all labeled observation IDs of eta class\n",
    "# exclude observations which did not produce any light curve segments, and append the rest to training set\n",
    "class_obs=[]\n",
    "for ob in class_obs_all:\n",
    "    if ob in obs_to_split:\n",
    "        class_obs.append(ob)\n",
    "train_set.append(class_obs)\n",
    "\n",
    "test_set=list(np.hstack(test_set))\n",
    "val_set=list(np.hstack(val_set))\n",
    "train_set=list(np.hstack(train_set))\n",
    "\n",
    "\n",
    "# fill train/val/test sets with the remaining observations\n",
    "remaining_obs_to_split = [ob for ob in obs_to_split if (ob not in test_set) and (ob not in val_set) and (ob not in train_set)]\n",
    "np.random.shuffle(remaining_obs_to_split)\n",
    "\n",
    "for ob in remaining_obs_to_split:\n",
    "    test_set.append(ob)\n",
    "    test_set_size += len(lcs[np.where(np.array(lc_ids) == ob)[0][0]][0])\n",
    "    if test_set_size >= needed_testing_data:\n",
    "        break\n",
    "\n",
    "remaining_obs_to_split = [ob for ob in remaining_obs_to_split if ob not in test_set]\n",
    "np.random.shuffle(remaining_obs_to_split)\n",
    "\n",
    "for ob in remaining_obs_to_split:\n",
    "    val_set.append(ob)\n",
    "    val_set_size += len(lcs[np.where(np.array(lc_ids) == ob)[0][0]][0])\n",
    "    if val_set_size >= needed_validation_data:\n",
    "        break\n",
    "        \n",
    "remaining_obs_to_split = [ob for ob in remaining_obs_to_split if ob not in val_set]\n",
    "\n",
    "for ob in remaining_obs_to_split:\n",
    "    train_set.append(ob)\n",
    "    train_set_size += len(lcs[np.where(np.array(lc_ids) == ob)[0][0]][0])\n",
    "\n",
    "        \n",
    "test_set=np.hstack(test_set)\n",
    "val_set=np.hstack(val_set)\n",
    "train_set=np.hstack(train_set)\n",
    "\n",
    "split_obs = [train_set, val_set, test_set]\n",
    "\n",
    "print(\"Test set \", test_set_size/total_data_volume)\n",
    "print(\"Validation set percentage\", valid_set_size/total_data_volume)\n",
    "print(\"Training set percentage\", (total_data_volume-valid_set_size-test_set_size)/total_data_volume)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Observation ID intersection between: \\ntest-valid {} \\ntest-train {} \\nvalid-train sets {}\".format(\n",
    "      len([ob for ob in test_set if ob in val_set]),\n",
    "      len([ob for ob in test_set if ob in train_set]), \n",
    "      len([ob for ob in val_set if ob in train_set])))\n",
    "print()\n",
    "print(\"Sum of train/val/test sizes: {}\".format(np.sum([len(subset) for subset in split_obs])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('{}/lightcurve1738_train70_val10_test20.pkl'.format(data_dir), 'wb') as f:\n",
    "#     pickle.dump(split_obs, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique([x.split(\"_\")[0] for x in id_per_seg]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(seg_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(ids).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../../../data_GRS1915/468202_len128_stride8_4sec_cad_countrates_sum_bin.pkl', 'rb') as f:\n",
    "    segments = pickle.load(f)\n",
    "with open('../../../data_GRS1915/468202_len128_stride8_4sec_cad_errors_sum_bin.pkl', 'rb') as f:\n",
    "    errors = pickle.load(f)\n",
    "with open('../../../data_GRS1915/468202_len128_stride8_4sec_cad_ids_sum_bin.pkl', 'rb') as f:\n",
    "    ids = pickle.load(f)\n",
    "\n",
    "# errors = ((errors)/np.expand_dims(np.std(segments, axis=1), axis=1)).astype(np.float32)\n",
    "# segments = zscore(segments, axis=1).astype(np.float32)  # standardize per segment\n",
    "\n",
    "\n",
    "with open('../../../data_GRS1915/lightcurve1738_train70_val10_test20.pkl', 'rb') as f:\n",
    "    split_ob_ids = pickle.load(f)\n",
    "    \n",
    "ids_no_index = [obid.split(\"_\")[0] for obid in ids]\n",
    "training_segments_indices = np.array([seg_n for seg_n, seg in enumerate(ids_no_index) if seg in split_ob_ids[0]])\n",
    "validation_segments_indices = np.array([seg_n for seg_n, seg in enumerate(ids_no_index) if seg in split_ob_ids[1]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_segments_indices = np.array([seg_n for seg_n, seg in enumerate(ids_no_index) if seg in split_ob_ids[2]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[ob for ob in np.unique(np.take(ids_no_index, training_segments_indices)) if ob in np.unique(np.take(ids_no_index, validation_segments_indices))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[ob for ob in np.unique(np.take(ids_no_index, validation_segments_indices)) if ob in np.unique(np.take(ids_no_index, training_segments_indices))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[ob for ob in np.unique(np.take(ids_no_index, validation_segments_indices)) if ob in np.unique(np.take(ids_no_index, t_segments_indices))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[ob for ob in np.unique(np.take(ids_no_index, training_segments_indices)) if ob in np.unique(np.take(ids_no_index, t_segments_indices))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(training_segments_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(validation_segments_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(t_segments_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "327737+47198+93267"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Standard 1 light curves from txt, bin to 1 second cadence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def segmentation(time_series, seg_len, stride, keep_time_stamps=True, experimental = False, input_cadence=4):\n",
    "    \"\"\"\n",
    "    Create a list of 1D (when time_stamps=False) or 2D (when time_stamps=True) arrays, which are overlappig segments of ts. Incomplete fragments are rejected.\n",
    "\n",
    "    time_series = time series to be segmented\n",
    "    seg_len = length of a segment, \n",
    "    stride = step size; difference in the starting position of the consecutive segments\n",
    "    \"\"\"\n",
    "    segments=[]\n",
    "    for start in range(0, len(time_series[0])-seg_len, stride):\n",
    "        end=start+seg_len\n",
    "        if time_series[0][end]-time_series[0][start] != seg_len*input_cadence: #don't allow segments outside of good time intervals\n",
    "            continue\n",
    "        if keep_time_stamps==True:\n",
    "            segments.append(time_series[:,start:end])\n",
    "        else:\n",
    "            segments.append(time_series[1:,start:end])\n",
    "    return np.array(segments) # check why time stamps are kept \n",
    "\n",
    "def binning(time_series, output_cadence, input_cadence=0.125):\n",
    "    \"\"\"\n",
    "    Bin the input time series. First dimension of the time series must be equal to 3. Time series must contain an array of time stamps, \n",
    "    count values and uncertainty on the count.\n",
    "    Make sure that count rates are transformed to count values (uncertainty should be equal to the square root of the count).\n",
    "    \n",
    "    time_series = array of size [3, N], where N is the length of the series\n",
    "    input_cadence = input cadence in seconds\n",
    "    output_cadence = desired cadence in seconds\n",
    "    \"\"\"\n",
    "    binned_stamps = int(output_cadence/input_cadence) # how many data points to bin\n",
    "        \n",
    "    weights = f[2]**-2\n",
    "    weighted_counts = f[1]*weights # weigh counts by the inverse of squared error\n",
    "    binned_counts = np.sum(weighted_counts[:(len(weighted_counts)//binned_stamps)*binned_stamps].reshape(-1, binned_stamps), axis=1) # sum weighted counts within each bin\n",
    "    binned_weights = np.sum(weights[:(len(weights)//binned_stamps)*binned_stamps].reshape(-1, binned_stamps), axis=1) # sum weights within each bin\n",
    "    binned_counts/=binned_weights # normalise weighted values using sum of weights\n",
    "    binned_errors = np.sqrt(1.0/(binned_weights)) # calculate uncertainty of each bin\n",
    "    binned_time = np.mean(f[0][:(len(f[0])//binned_stamps)*binned_stamps].reshape(-1, binned_stamps), axis=1) # find the mean time of each bin\n",
    "    \n",
    "    # if bin crosses between two good time intervals, the difference between its binned time and the binned time of preceding bin will not\n",
    "    # be equal to the desired cadence. Remove those bins from the light curve\n",
    "    rm_points = []\n",
    "    skip=False\n",
    "    for i in range(len(binned_time)-1):\n",
    "        if skip==True:\n",
    "            skip=False\n",
    "            continue\n",
    "        delta = binned_time[i+1]-binned_time[i]\n",
    "        if delta > output_cadence:\n",
    "            rm_points.append(i+1)\n",
    "            skip=True\n",
    "    times=np.delete(binned_time,rm_points)\n",
    "    counts=np.delete(binned_counts,rm_points)\n",
    "    errors=np.delete(binned_errors,rm_points)\n",
    "    \n",
    "    return np.stack((times,counts, errors))\n",
    "\n",
    "\n",
    "def std1_to_segments(in_data_dir, cadence, seg_len_s, stride_s, random_seed):\n",
    "    \"\"\"\n",
    "    in_data_dir = directory that will be searched for \"*_std1_lc.txt\" files containing Standard1 light curve data\n",
    "    cadence = desired amount of time between data points of the final segments, unit of seconds, should be a multiple of 0.125 (std1 resolution)\n",
    "    seg_len_s = desired segment length in seconds\n",
    "    stride_s = time difference between consecutive segments; stride size of the moving window in seconds\n",
    "    random_seed = set the seed of the numpy random state\n",
    "    \n",
    "    returns segments_counts, segments_errors, id_per_seg\n",
    "    \"\"\"\n",
    "    np.random.seed(seed=random_seed)\n",
    "    \n",
    "    lcs = []\n",
    "    ids=[]\n",
    "\n",
    "    binned_stamps = int(cadence/0.125) # how many time stamps go into one bin\n",
    "\n",
    "    for root, dirnames, filenames in os.walk(in_data_dir): #Std1_PCU2\n",
    "        for filename in fnmatch.filter(filenames, \"*_std1_lc.txt\"):\n",
    "            lc = os.path.join(root, filename)\n",
    "            ids.append(filename.split(\"_\")[0])\n",
    "            f=np.loadtxt(lc)\n",
    "            f=np.transpose(f)#,axis=1)\n",
    "            \n",
    "            binned_lc = binning(f, bin_size=binned_stamps)\n",
    "            lcs.append(binned_lc)\n",
    "    \n",
    "    print(\"Binned {} light curves.\".format(len(lcs)))\n",
    "    clear_output(wait=True)\n",
    "            \n",
    "    segments_counts=[]\n",
    "    segments_errors=[]\n",
    "    seg_ids=[]\n",
    "\n",
    "\n",
    "    seg_len = seg_len_s//cadence # segment length and stride size in data points\n",
    "    stride = stride_s//cadence\n",
    "\n",
    "\n",
    "\n",
    "    for lc_index, lc in enumerate(lcs):\n",
    "        if len(lc[1]) >= seg_len: \n",
    "            segments = segmentation(lc, seg_len, stride, keep_time_stamps=False, experimental = False)\n",
    "        else:\n",
    "            continue\n",
    "        if len(segments) > 0:\n",
    "            segments_counts.append(segments[:,0,:])\n",
    "            segments_errors.append(segments[:,1,:])\n",
    "            seg_ids.append(ids[lc_index])\n",
    "            print(\"Segmented {}/{} light curves.\".format(lc_index+1, len(lcs)))\n",
    "            clear_output(wait=True)\n",
    "    \n",
    "    print(\"Stacking the segments and creating segment IDs, shuffling.\")\n",
    "    id_per_seg = []  # for each light curve, copy the observation id for every segment of the light curve\n",
    "    for lc_index, lc in enumerate(segments_counts):\n",
    "        for i in range(len(lc)):\n",
    "            id_per_seg.append(seg_ids[lc_index]+\"_{}\".format(i))\n",
    "\n",
    "    segments_counts=np.vstack(segments_counts)\n",
    "    segments_errors=np.vstack(segments_errors)\n",
    "    segments_counts = np.expand_dims(segments_counts, axis=-1)\n",
    "    segments_errors = np.expand_dims(segments_errors, axis=-1)\n",
    "    \n",
    "    rng_state = np.random.get_state()\n",
    "    np.random.shuffle(segments_counts)\n",
    "    np.random.set_state(rng_state)\n",
    "    np.random.shuffle(segments_errors)\n",
    "    np.random.set_state(rng_state)\n",
    "    np.random.shuffle(id_per_seg)\n",
    "\n",
    "    print(\"Done\")\n",
    "    \n",
    "    return segments_counts, segments_errors, id_per_seg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "segments_counts, segments_errors, id_per_seg = std1_to_segments(in_data_dir=\"/export/data/jakubok/GRS1915+105/Std1_PCU2\", cadence=4, seg_len_s=512, stride_s=8, random_seed=11)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = (50.0, 5.0)\n",
    "plt.rcParams.update({'font.size': 12})\n",
    "plt.errorbar(lcs[4][0],lcs[4][1], yerr=lcs[4][2], ecolor=\"orange\",barsabove=True)\n",
    "# plt.errorbar(binned_lc[0],binned_lc[1], yerr=binned_lc[2], ecolor=\"orange\",barsabove=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lcs[4][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(2112/8) #+-16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sqrt(264)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "129.98461447/8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sqrt(lcs[4][1]/8)*8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lcs[4][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lcs[4][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lcs[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sqrt(lcs[4][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lcs[4][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lc_notrate=lcs[4]\n",
    "lc_notrate[1]/=8\n",
    "lc_notrate[2]/=8\n",
    "\n",
    "\n",
    "binned_lc = binning(lc_notrate, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lc_notrate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "binned_lc[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sqrt(binned_lc[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "binned_lc[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lcs[4][2].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "binned_lc[2].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "11544/4565"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts=lcs[0][1]\n",
    "errors=lcs[0][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalised_variance(counts, errors):\n",
    "    return (np.var(counts)-np.mean(errors**2))/np.mean(counts**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalised_variance(counts, errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalised_variance(binned_lc[1], binned_lc[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sqrt(lcs[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lcs[0][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lcs[0][2][:8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sqrt(lcs[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lcs[0][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sqrt(    1/ sum(lcs[0][2][:8]**-2)    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sqrt(    sum(lcs[0][2][:8]**2)    )/8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "binned_lc[2][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lcs=[]\n",
    "ids=[]\n",
    "\n",
    "for root, dirnames, filenames in os.walk(\"/data/jkok1g14/data_GRS1915/std1\"): #Std1_PCU2\n",
    "    for filename in fnmatch.filter(filenames, \"*_std1_lc.txt\"):\n",
    "        lc = os.path.join(root, filename)\n",
    "        ids.append(filename.split(\"_\")[0])\n",
    "        f=np.loadtxt(lc)\n",
    "        f=np.transpose(f)#,axis=1)\n",
    "        #f=f[0:2]\n",
    "        ###1s average and time check to eliminate points outside of GTIs\n",
    "        f8t = np.mean(f[0][:(len(f[0])//8)*8].reshape(-1, 8), axis=1)\n",
    "        f8c = np.mean(f[1][:(len(f[1])//8)*8].reshape(-1, 8), axis=1)\n",
    "        f8e = np.sqrt(np.sum(f[2][:(len(f[2])//8)*8].reshape(-1, 8)**2, axis=1))/8\n",
    "        rm_points = []\n",
    "        skip=False\n",
    "        for i in range(len(f8t)-1):\n",
    "            if skip==True:\n",
    "                skip=False\n",
    "                continue\n",
    "            delta = f8t[i+1]-f8t[i]\n",
    "            if delta > 1.0:\n",
    "                rm_points.append(i+1)\n",
    "                skip=True\n",
    "\n",
    "        times=np.delete(f8t,rm_points)\n",
    "        counts=np.delete(f8c,rm_points)\n",
    "        errors=np.delete(f8e,rm_points)\n",
    "        lcs.append(np.stack((times,counts, errors)))\n",
    "#         break\n",
    "#     if len(lcs)>0:\n",
    "#         break\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('../../../data_GRS1915/1776_light_curves_1s_bin_errorfix.pkl', 'wb') as f:\n",
    "#     pickle.dump(lcs, f)\n",
    "# with open('../../../data_GRS1915/1776_light_curves_1s_bin_ids_errorfix.pkl', 'wb') as f:\n",
    "#     pickle.dump(ids, f)\n",
    "\n",
    "with open('../../../data_GRS1915/1776_light_curves_1s_bin_errorfix.pkl', 'rb') as f:\n",
    "    lcs = pickle.load(f)\n",
    "with open('../../../data_GRS1915/1776_light_curves_1s_bin_ids_errorfix.pkl', 'rb') as f:\n",
    "    ids = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_belloni = open('../../../data_GRS1915/1915Belloniclass_updated.dat')\n",
    "lines = clean_belloni.readlines()\n",
    "states = lines[0].split()\n",
    "belloni_clean = {}\n",
    "for h,l in zip(states, lines[1:]):\n",
    "    belloni_clean[h] = l.split()\n",
    "    #state: obsID1, obsID2...\n",
    "ob_state = {}\n",
    "for state, obs in belloni_clean.items():\n",
    "    if state == \"chi1\" or state == \"chi2\" or state == \"chi3\" or state == \"chi4\": state = \"chi\"\n",
    "    for ob in obs:\n",
    "        ob_state[ob] = state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def segmentation(time_series, seg_len, stride, keep_time_stamps=True, experimental = False):\n",
    "    \"\"\"\n",
    "    Create a list of 1D (when time_stamps=False) or 2D (when time_stamps=True) arrays, which are overlappig segments of ts. Incomplete fragments are rejected.\n",
    "    \n",
    "    time_series = time series to be segmented\n",
    "    seg_len = length of a segment, \n",
    "    stride = step size; difference in the starting position of the consecutive segments\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    segments=[]\n",
    "    for start in range(0, len(time_series[0])-seg_len, stride):\n",
    "        end=start+seg_len\n",
    "        ############################################# *4 because of the 4 second cadance \n",
    "        if time_series[0][end]-time_series[0][start] != seg_len: #don't allow temporally discontinous segments\n",
    "            continue\n",
    "        if keep_time_stamps==True:\n",
    "            segments.append(time_series[:,start:end])\n",
    "        else:\n",
    "            segments.append(time_series[1:,start:end])\n",
    "    return np.array(segments) # check why time stamps are kept \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "segments_counts=[]\n",
    "segments_errors=[]\n",
    "seg_ids=[]\n",
    "for lc_index, lc in enumerate(lcs):\n",
    "    if len(lc[1]) >= 512: \n",
    "        segments = segmentation(lc, 512, 40, keep_time_stamps=False, experimental = False)\n",
    "    else:\n",
    "        continue\n",
    "    if len(segments) > 0:\n",
    "        segments_counts.append(segments[:,0,:])\n",
    "        segments_errors.append(segments[:,1,:])\n",
    "        seg_ids.append(ids[lc_index])\n",
    "        print(lc_index+1, \"/{}\".format(len(lcs)))\n",
    "        clear_output(wait=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_per_seg = []  # for each light curve, copy the observation id for every segment of the light curve\n",
    "for lc_index, lc in enumerate(segments_counts):\n",
    "    for i in range(len(lc)):\n",
    "        id_per_seg.append(seg_ids[lc_index]+\"_{}\".format(i))\n",
    "        \n",
    "segments_counts=np.vstack(segments_counts)\n",
    "segments_errors=np.vstack(segments_errors)\n",
    "segments_counts = np.expand_dims(segments_counts, axis=-1)\n",
    "segments_errors = np.expand_dims(segments_errors, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng_state = np.random.get_state()\n",
    "np.random.shuffle(segments_counts)\n",
    "np.random.set_state(rng_state)\n",
    "np.random.shuffle(segments_errors)\n",
    "np.random.set_state(rng_state)\n",
    "np.random.shuffle(id_per_seg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('../../../data_GRS1915/94465_len512_s40_counts_errorfix.pkl', 'wb') as f:\n",
    "#     pickle.dump(segments_counts, f)\n",
    "    \n",
    "# with open('../../../data_GRS1915/94465_len512_s40_errors_errorfix.pkl', 'wb') as f:\n",
    "#     pickle.dump(segments_errors, f)\n",
    "    \n",
    "# with open('../../../data_GRS1915/94465_len512_s40_ids_errorfix.pkl', 'wb') as f:\n",
    "#     pickle.dump(id_per_seg, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def std1_to_segments(in_data_dir, cadence, seg_len_s, stride_s, random_seed):\n",
    "    \"\"\"\n",
    "    in_data_dir = directory that will be searched for \"*_std1_lc.txt\" files containing Standard1 light curve data\n",
    "    cadence = desired amount of time between data points of the final segments, unit of seconds, should be a multiple of 0.125 (std1 resolution)\n",
    "    seg_len_s = desired segment length in seconds\n",
    "    stride_s = time difference between consecutive segments; stride size of the moving window in seconds\n",
    "    random_seed = set the seed of the numpy random state\n",
    "    \n",
    "    returns segments_counts, segments_errors, id_per_seg\n",
    "    \"\"\"\n",
    "    np.random.seed(seed=random_seed)\n",
    "    \n",
    "    def segmentation(time_series, seg_len, stride, keep_time_stamps=True, experimental = False, cadence=4):\n",
    "        \"\"\"\n",
    "        Create a list of 1D (when time_stamps=False) or 2D (when time_stamps=True) arrays, which are overlappig segments of ts. Incomplete fragments are rejected.\n",
    "\n",
    "        time_series = time series to be segmented\n",
    "        seg_len = length of a segment, \n",
    "        stride = step size; difference in the starting position of the consecutive segments\n",
    "        \"\"\"\n",
    "\n",
    "\n",
    "        segments=[]\n",
    "        for start in range(0, len(time_series[0])-seg_len, stride):\n",
    "            end=start+seg_len\n",
    "            ############################################# *4 because of the 4 second cadance \n",
    "            if time_series[0][end]-time_series[0][start] != seg_len*cadence: #don't allow temporally discontinous segments\n",
    "                continue\n",
    "            if keep_time_stamps==True:\n",
    "                segments.append(time_series[:,start:end])\n",
    "            else:\n",
    "                segments.append(time_series[1:,start:end])\n",
    "        return np.array(segments) # check why time stamps are kept \n",
    "\n",
    "\n",
    "    \n",
    "    lcs=[]\n",
    "    ids=[]\n",
    "\n",
    "    binned_stamps = int(cadence/0.125)\n",
    "\n",
    "    for root, dirnames, filenames in os.walk(in_data_dir): #Std1_PCU2\n",
    "        for filename in fnmatch.filter(filenames, \"*_std1_lc.txt\"):\n",
    "            lc = os.path.join(root, filename)\n",
    "            ids.append(filename.split(\"_\")[0])\n",
    "            f=np.loadtxt(lc)\n",
    "            f=np.transpose(f)#,axis=1)\n",
    "            #f=f[0:2]\n",
    "            ###1s average and time check to eliminate points outside of GTIs\n",
    "            fbinned_t = np.mean(f[0][:(len(f[0])//binned_stamps)*binned_stamps].reshape(-1, binned_stamps), axis=1)\n",
    "            fbinned_c = np.mean(f[1][:(len(f[1])//binned_stamps)*binned_stamps].reshape(-1, binned_stamps), axis=1)\n",
    "            fbinned_e = np.sqrt(np.sum(f[2][:(len(f[2])//binned_stamps)*binned_stamps].reshape(-1, binned_stamps)**2, axis=1))/binned_stamps\n",
    "            rm_points = []\n",
    "            skip=False\n",
    "            for i in range(len(fbinned_t)-1):\n",
    "                if skip==True:\n",
    "                    skip=False\n",
    "                    continue\n",
    "                delta = fbinned_t[i+1]-fbinned_t[i]\n",
    "                if delta > cadence:\n",
    "                    rm_points.append(i+1)\n",
    "                    skip=True\n",
    "\n",
    "            times=np.delete(fbinned_t,rm_points)\n",
    "            counts=np.delete(fbinned_c,rm_points)\n",
    "            errors=np.delete(fbinned_e,rm_points)\n",
    "            lcs.append(np.stack((times,counts, errors)))\n",
    "            \n",
    "            print(\"Binned {} light curves.\".format(len(lcs)))\n",
    "            clear_output(wait=True)\n",
    "            \n",
    "            \n",
    "    segments_counts=[]\n",
    "    segments_errors=[]\n",
    "    seg_ids=[]\n",
    "\n",
    "\n",
    "    seg_len = seg_len_s//cadence # segment length and stride size in data points\n",
    "    stride = stride_s//cadence\n",
    "\n",
    "\n",
    "\n",
    "    for lc_index, lc in enumerate(lcs):\n",
    "        if len(lc[1]) >= seg_len: \n",
    "            segments = segmentation(lc, seg_len, stride, keep_time_stamps=False, experimental = False)\n",
    "        else:\n",
    "            continue\n",
    "        if len(segments) > 0:\n",
    "            segments_counts.append(segments[:,0,:])\n",
    "            segments_errors.append(segments[:,1,:])\n",
    "            seg_ids.append(ids[lc_index])\n",
    "            print(\"Segmented {}/{} light curves.\".format(lc_index+1, len(lcs)))\n",
    "            clear_output(wait=True)\n",
    "    \n",
    "    print(\"Stacking the segments and creating segment IDs, shuffling.\")\n",
    "    id_per_seg = []  # for each light curve, copy the observation id for every segment of the light curve\n",
    "    for lc_index, lc in enumerate(segments_counts):\n",
    "        for i in range(len(lc)):\n",
    "            id_per_seg.append(seg_ids[lc_index]+\"_{}\".format(i))\n",
    "\n",
    "    segments_counts=np.vstack(segments_counts)\n",
    "    segments_errors=np.vstack(segments_errors)\n",
    "    segments_counts = np.expand_dims(segments_counts, axis=-1)\n",
    "    segments_errors = np.expand_dims(segments_errors, axis=-1)\n",
    "    \n",
    "    rng_state = np.random.get_state()\n",
    "    np.random.shuffle(segments_counts)\n",
    "    np.random.set_state(rng_state)\n",
    "    np.random.shuffle(segments_errors)\n",
    "    np.random.set_state(rng_state)\n",
    "    np.random.shuffle(id_per_seg)\n",
    "\n",
    "    print(\"Done\")\n",
    "    \n",
    "    return segments_counts, segments_errors, id_per_seg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "segments_counts, segments_errors, id_per_seg = std1_to_segments(in_data_dir=\"/export/data/jakubok/GRS1915+105/Std1_PCU2\", cadence=4, seg_len_s=512, stride_s=8, random_seed=11)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"../../../files_from_snuffy\"\n",
    "with open('{}/1776_light_curves_4s_bin_errorfix.pkl'.format(data_dir), 'rb') as f:\n",
    "    lcs = pickle.load(f)\n",
    "with open('{}/1776_light_curves_4s_bin_ids_errorfix.pkl'.format(data_dir), 'rb') as f:\n",
    "    ids = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def segmentation(time_series, seg_len, stride, keep_time_stamps=True, experimental = False, cadence=4):\n",
    "    \"\"\"\n",
    "    Create a list of 1D (when time_stamps=False) or 2D (when time_stamps=True) arrays, which are overlappig segments of ts. Incomplete fragments are rejected.\n",
    "\n",
    "    time_series = time series to be segmented\n",
    "    seg_len = length of a segment, \n",
    "    stride = step size; difference in the starting position of the consecutive segments\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    segments=[]\n",
    "    for start in range(0, len(time_series[0])-seg_len, stride):\n",
    "        end=start+seg_len\n",
    "        ############################################# *4 because of the 4 second cadance \n",
    "        if time_series[0][end]-time_series[0][start] != seg_len*cadence: #don't allow temporally discontinous segments\n",
    "            continue\n",
    "        if keep_time_stamps==True:\n",
    "            segments.append(time_series[:,start:end])\n",
    "        else:\n",
    "            segments.append(time_series[1:,start:end])\n",
    "    return np.array(segments) # check why time stamps are kept \n",
    "\n",
    "\n",
    "cadence=4\n",
    "seg_len_s=512\n",
    "stride_s=8\n",
    "\n",
    "segments_counts=[]\n",
    "segments_times = []\n",
    "segments_errors=[]\n",
    "seg_ids=[]\n",
    "\n",
    "\n",
    "seg_len = seg_len_s//cadence # segment length and stride size in data points\n",
    "stride = stride_s//cadence\n",
    "\n",
    "\n",
    "\n",
    "for lc_index, lc in enumerate(lcs):\n",
    "    if len(lc[1]) >= seg_len: \n",
    "        segments = segmentation(lc, seg_len, stride, keep_time_stamps=True, experimental = False)\n",
    "    else:\n",
    "        continue\n",
    "    if len(segments) > 0:\n",
    "        segments_times.append(segments[:,0,:])\n",
    "        segments_counts.append(segments[:,1,:])\n",
    "        segments_errors.append(segments[:,2,:])\n",
    "        seg_ids.append(ids[lc_index])\n",
    "        print(\"Segmented {}/{} light curves.\".format(lc_index+1, len(lcs)))\n",
    "        clear_output(wait=True)\n",
    "        \n",
    "id_per_seg = []  # for each light curve, copy the observation id for every segment of the light curve\n",
    "for lc_index, lc in enumerate(segments_counts):\n",
    "    for i in range(len(lc)):\n",
    "        id_per_seg.append(seg_ids[lc_index]+\"_{}\".format(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "segments_times[0][:,0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seg_start_times = np.concatenate([time[:,0] for time in segments_times])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seg_id_start_time_df = pd.DataFrame(seg_start_times, columns=[\"Start_time\"], index=id_per_seg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seg_id_start_time_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "seg_id_start_time_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../../../files_from_snuffy/468202_len128_s2_4cad_start_times_errorfix.pkl', 'wb') as f:\n",
    "    pickle.dump(seg_id_start_time_df, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('../../../data_GRS1915/468202_len128_s2_4cad_counts_errorfix.pkl', 'wb') as f:\n",
    "#     pickle.dump(segments_counts, f)\n",
    "    \n",
    "# with open('../../../data_GRS1915/468202_len128_s2_4cad_errors_errorfix.pkl', 'wb') as f:\n",
    "#     pickle.dump(segments_errors, f)\n",
    "    \n",
    "# with open('../../../data_GRS1915/468202_len128_s2_4cad_ids_errorfix.pkl', 'wb') as f:\n",
    "#     pickle.dump(id_per_seg, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lcs=[]\n",
    "ids=[]\n",
    "\n",
    "cadence=4 # seconds\n",
    "binned_stamps = int(cadence/0.125)\n",
    "\n",
    "for root, dirnames, filenames in os.walk(\"/data/jkok1g14/data_GRS1915/std1\"): #Std1_PCU2\n",
    "    for filename in fnmatch.filter(filenames, \"*_std1_lc.txt\"):\n",
    "        lc = os.path.join(root, filename)\n",
    "        ids.append(filename.split(\"_\")[0])\n",
    "        f=np.loadtxt(lc)\n",
    "        f=np.transpose(f)#,axis=1)\n",
    "        #f=f[0:2]\n",
    "        ###1s average and time check to eliminate points outside of GTIs\n",
    "        fbinned_t = np.mean(f[0][:(len(f[0])//binned_stamps)*binned_stamps].reshape(-1, binned_stamps), axis=1)\n",
    "        fbinned_c = np.mean(f[1][:(len(f[1])//binned_stamps)*binned_stamps].reshape(-1, binned_stamps), axis=1)\n",
    "        fbinned_e = np.sqrt(np.sum(f[2][:(len(f[2])//binned_stamps)*binned_stamps].reshape(-1, binned_stamps)**2, axis=1))/binned_stamps\n",
    "        rm_points = []\n",
    "        skip=False\n",
    "        for i in range(len(fbinned_t)-1):\n",
    "            if skip==True:\n",
    "                skip=False\n",
    "                continue\n",
    "            delta = fbinned_t[i+1]-fbinned_t[i]\n",
    "            if delta > cadence:\n",
    "                rm_points.append(i+1)\n",
    "                skip=True\n",
    "\n",
    "        times=np.delete(fbinned_t,rm_points)\n",
    "        counts=np.delete(fbinned_c,rm_points)\n",
    "        errors=np.delete(fbinned_e,rm_points)\n",
    "        lcs.append(np.stack((times,counts, errors)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('../../../data_GRS1915/1776_light_curves_4s_bin_errorfix.pkl', 'wb') as f:\n",
    "#     pickle.dump(lcs, f)\n",
    "# with open('../../../data_GRS1915/1776_light_curves_4s_bin_ids_errorfix.pkl', 'wb') as f:\n",
    "#     pickle.dump(ids, f)\n",
    "\n",
    "# with open('../../../data_GRS1915/1776_light_curves_4s_bin_errorfix.pkl', 'rb') as f:\n",
    "#     lcs = pickle.load(f)\n",
    "# with open('../../../data_GRS1915/1776_light_curves_4s_bin_ids_errorfix.pkl', 'rb') as f:\n",
    "#     ids = pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def segmentation(time_series, seg_len, stride, keep_time_stamps=True, experimental = False, cadence=4):\n",
    "    \"\"\"\n",
    "    Create a list of 1D (when time_stamps=False) or 2D (when time_stamps=True) arrays, which are overlappig segments of ts. Incomplete fragments are rejected.\n",
    "    \n",
    "    time_series = time series to be segmented\n",
    "    seg_len = length of a segment, \n",
    "    stride = step size; difference in the starting position of the consecutive segments\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    segments=[]\n",
    "    for start in range(0, len(time_series[0])-seg_len, stride):\n",
    "        end=start+seg_len\n",
    "        ############################################# *4 because of the 4 second cadance \n",
    "        if time_series[0][end]-time_series[0][start] != seg_len*cadence: #don't allow temporally discontinous segments\n",
    "            continue\n",
    "        if keep_time_stamps==True:\n",
    "            segments.append(time_series[:,start:end])\n",
    "        else:\n",
    "            segments.append(time_series[1:,start:end])\n",
    "    return np.array(segments) # check why time stamps are kept \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "segments_counts=[]\n",
    "segments_errors=[]\n",
    "seg_ids=[]\n",
    "\n",
    "seg_len_s = 1024 # seconds \n",
    "stride_s = 16 # seconds\n",
    "cadence=4 # seconds\n",
    "\n",
    "seg_len = seg_len_s//cadence\n",
    "stride = stride_s//cadence\n",
    "\n",
    "\n",
    "\n",
    "for lc_index, lc in enumerate(lcs):\n",
    "    if len(lc[1]) >= seg_len: \n",
    "        segments = segmentation(lc, seg_len, stride, keep_time_stamps=False, experimental = False)\n",
    "    else:\n",
    "        continue../../../files_from_snuffy\n",
    "    if len(segments) > 0:\n",
    "        segments_counts.append(segments[:,0,:])\n",
    "        segments_errors.append(segments[:,1,:])\n",
    "        seg_ids.append(ids[lc_index])\n",
    "        print(lc_index+1, \"/{}\".format(len(lcs)))\n",
    "        clear_output(wait=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_per_seg = []  # for each light curve, copy the observation id for every segment of the light curve\n",
    "for lc_index, lc in enumerate(segments_counts):\n",
    "    for i in range(len(lc)):\n",
    "        id_per_seg.append(seg_ids[lc_index]+\"_{}\".format(i))\n",
    "        \n",
    "segments_counts=np.vstack(segments_counts)\n",
    "segments_errors=np.vstack(segments_errors)\n",
    "segments_counts = np.expand_dims(segments_counts, axis=-1)\n",
    "segments_errors = np.expand_dims(segments_errors, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng_state = np.random.get_state()\n",
    "np.random.shuffle(segments_counts)\n",
    "np.random.set_state(rng_state)\n",
    "np.random.shuffle(segments_errors)\n",
    "np.random.set_state(rng_state)\n",
    "np.random.shuffle(id_per_seg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "segments.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('../../../data_GRS1915/159927_len256_s4_4cad_counts_errorfix.pkl', 'wb') as f:\n",
    "#     pickle.dump(segments_counts, f)\n",
    "    \n",
    "# with open('../../../data_GRS1915/159927_len256_s4_4cad_errors_errorfix.pkl', 'wb') as f:\n",
    "#     pickle.dump(segments_errors, f)\n",
    "    \n",
    "# with open('../../../data_GRS1915/159927_len256_s4_4cad_ids_errorfix.pkl', 'wb') as f:\n",
    "#     pickle.dump(id_per_seg, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 96 data point segments, 4 second cadence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('{}/1776_light_curves_4s_bin_errorfix.pkl'.format(data_dir), 'rb') as f:\n",
    "    lcs = pickle.load(f)\n",
    "with open('{}/1776_light_curves_4s_bin_ids_errorfix.pkl'.format(data_dir), 'rb') as f:\n",
    "    ids = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def segmentation(time_series, seg_len, stride, keep_time_stamps=True, experimental = False, cadence=4):\n",
    "    \"\"\"\n",
    "    Create a list of 1D (when time_stamps=False) or 2D (when time_stamps=True) arrays, which are overlappig segments of ts. Incomplete fragments are rejected.\n",
    "\n",
    "    time_series = time series to be segmented\n",
    "    seg_len = length of a segment, \n",
    "    stride = step size; difference in the starting position of the consecutive segments\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    segments=[]\n",
    "    for start in range(0, len(time_series[0])-seg_len, stride):\n",
    "        end=start+seg_len\n",
    "        ############################################# *4 because of the 4 second cadance \n",
    "        if time_series[0][end]-time_series[0][start] != seg_len*cadence: #don't allow temporally discontinous segments\n",
    "            continue\n",
    "        if keep_time_stamps==True:\n",
    "            segments.append(time_series[:,start:end])\n",
    "        else:\n",
    "            segments.append(time_series[1:,start:end])\n",
    "    return np.array(segments) # check why time stamps are kept \n",
    "\n",
    "\n",
    "cadence=4\n",
    "seg_len_s=384\n",
    "stride_s=8\n",
    "\n",
    "segments_counts=[]\n",
    "segments_times = []\n",
    "segments_errors=[]\n",
    "seg_ids=[]\n",
    "\n",
    "\n",
    "seg_len = seg_len_s//cadence # segment length and stride size in data points\n",
    "stride = stride_s//cadence\n",
    "\n",
    "\n",
    "\n",
    "for lc_index, lc in enumerate(lcs):\n",
    "    if len(lc[1]) >= seg_len: \n",
    "        segments = segmentation(lc, seg_len, stride, keep_time_stamps=True, experimental = False)\n",
    "    else:\n",
    "        continue\n",
    "    if len(segments) > 0:\n",
    "        segments_times.append(segments[:,0,:])\n",
    "        segments_counts.append(segments[:,1,:])\n",
    "        segments_errors.append(segments[:,2,:])\n",
    "        seg_ids.append(ids[lc_index])\n",
    "        print(\"Segmented {}/{} light curves.\".format(lc_index+1, len(lcs)))\n",
    "        clear_output(wait=True)\n",
    "        \n",
    "id_per_seg = []  # for each light curve, copy the observation id for every segment of the light curve\n",
    "for lc_index, lc in enumerate(segments_counts):\n",
    "    for i in range(len(lc)):\n",
    "        id_per_seg.append(seg_ids[lc_index]+\"_{}\".format(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "segments_counts=np.vstack(segments_counts)\n",
    "segments_errors=np.vstack(segments_errors)\n",
    "segments_counts = np.expand_dims(segments_counts, axis=-1)\n",
    "segments_errors = np.expand_dims(segments_errors, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng_state = np.random.get_state()\n",
    "np.random.shuffle(segments_counts)\n",
    "np.random.set_state(rng_state)\n",
    "np.random.shuffle(segments_errors)\n",
    "np.random.set_state(rng_state)\n",
    "np.random.shuffle(id_per_seg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('{}/509201_len96_stride8_4sec_cad_counts_errorfix.pkl'.format(data_dir), 'wb') as f:\n",
    "#     pickle.dump(segments_counts, f)\n",
    "    \n",
    "# with open('{}/509201_len96_stride8_4sec_cad_errors_errorfix.pkl'.format(data_dir), 'wb') as f:\n",
    "#     pickle.dump(segments_errors, f)\n",
    "    \n",
    "# with open('{}/509201_len96_stride8_4sec_cad_ids_errorfix.pkl'.format(data_dir), 'wb') as f:\n",
    "#     pickle.dump(id_per_seg, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('{}/509201_len96_stride8_4sec_cad_ids_errorfix.pkl'.format(data_dir), 'rb') as f:\n",
    "    seg_ids = pickle.load(f)\n",
    "    \n",
    "ObID_per_sample = np.array([seg_id.split(\"_\")[0] for seg_id in seg_ids])\n",
    "\n",
    "\n",
    "needed_validation_segments = 509201*0.25\n",
    "unique_ObIDs = np.unique(ObID_per_sample, return_counts=True)\n",
    "ObIDs_no = len(unique_ObIDs[0])\n",
    "shuffle_indices = np.array(range(ObIDs_no))\n",
    "np.random.seed(seed=11)\n",
    "np.random.shuffle(shuffle_indices)\n",
    "\n",
    "\n",
    "valid_set_obs = []\n",
    "valid_set_size = 0\n",
    "\n",
    "for ob_index in shuffle_indices:\n",
    "    valid_set_obs.append(unique_ObIDs[0][ob_index])\n",
    "    valid_set_size += unique_ObIDs[1][ob_index]\n",
    "    if valid_set_size > needed_validation_segments:\n",
    "        break\n",
    "        \n",
    "valid_set_sample_indices = []\n",
    "for valid_set_ob in np.array(valid_set_obs):\n",
    "    valid_set_sample_indices.append(np.where(ObID_per_sample == valid_set_ob)[0])\n",
    "\n",
    "valid_set_sample_indices = [item for sublist in valid_set_sample_indices for item in sublist]\n",
    "\n",
    "train_set_sample_indices = []\n",
    "for train_set_ob in shuffle_indices[len(valid_set_obs):]:\n",
    "    train_set_sample_indices.append(np.where(ObID_per_sample == unique_ObIDs[0][train_set_ob])[0])\n",
    "    \n",
    "train_set_sample_indices = [item for sublist in train_set_sample_indices for item in sublist]\n",
    "\n",
    "\n",
    "split_indices = [train_set_sample_indices, valid_set_sample_indices]\n",
    "\n",
    "# with open('{}/509201_len96_stride8_4sec_cad_observation75-25split.pkl'.format(data_dir), 'wb') as f:\n",
    "#     pickle.dump(split_indices, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(ObID_per_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(split_indices[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "381701/3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 128 data point segments, 1 second cadence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('{}/1776_light_curves_1s_bin_errorfix.pkl'.format(data_dir), 'rb') as f:\n",
    "    lcs = pickle.load(f)\n",
    "with open('{}/1776_light_curves_1s_bin_ids_errorfix.pkl'.format(data_dir), 'rb') as f:\n",
    "    ids = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def segmentation(time_series, seg_len, stride, keep_time_stamps=True, experimental = False, cadence=1):\n",
    "    \"\"\"\n",
    "    Create a list of 1D (when time_stamps=False) or 2D (when time_stamps=True) arrays, which are overlappig segments of ts. Incomplete fragments are rejected.\n",
    "\n",
    "    time_series = time series to be segmented\n",
    "    seg_len = length of a segment, \n",
    "    stride = step size; difference in the starting position of the consecutive segments\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    segments=[]\n",
    "    for start in range(0, len(time_series[0])-seg_len, stride):\n",
    "        end=start+seg_len\n",
    "        ############################################# *4 because of the 4 second cadance \n",
    "        if time_series[0][end]-time_series[0][start] != seg_len*cadence: #don't allow temporally discontinous segments\n",
    "            continue\n",
    "        if keep_time_stamps==True:\n",
    "            segments.append(time_series[:,start:end])\n",
    "        else:\n",
    "            segments.append(time_series[1:,start:end])\n",
    "    return np.array(segments) # check why time stamps are kept \n",
    "\n",
    "\n",
    "cadence=1\n",
    "seg_len_s=128\n",
    "stride_s=8\n",
    "\n",
    "segments_counts=[]\n",
    "segments_times = []\n",
    "segments_errors=[]\n",
    "seg_ids=[]\n",
    "\n",
    "\n",
    "seg_len = seg_len_s//cadence # segment length and stride size in data points\n",
    "stride = stride_s//cadence\n",
    "\n",
    "\n",
    "\n",
    "for lc_index, lc in enumerate(lcs):\n",
    "    if len(lc[1]) >= seg_len: \n",
    "        segments = segmentation(lc, seg_len, stride, keep_time_stamps=True, experimental = False, cadence=cadence)\n",
    "    else:\n",
    "        continue\n",
    "    if len(segments) > 0:\n",
    "        segments_times.append(segments[:,0,:])\n",
    "        segments_counts.append(segments[:,1,:])\n",
    "        segments_errors.append(segments[:,2,:])\n",
    "        seg_ids.append(ids[lc_index])\n",
    "        print(\"Segmented {}/{} light curves.\".format(lc_index+1, len(lcs)))\n",
    "        clear_output(wait=True)\n",
    "        \n",
    "id_per_seg = []  # for each light curve, copy the observation id for every segment of the light curve\n",
    "for lc_index, lc in enumerate(segments_counts):\n",
    "    for i in range(len(lc)):\n",
    "        id_per_seg.append(seg_ids[lc_index]+\"_{}\".format(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "segments_counts=np.vstack(segments_counts)\n",
    "segments_errors=np.vstack(segments_errors)\n",
    "segments_counts = np.expand_dims(segments_counts, axis=-1)\n",
    "segments_errors = np.expand_dims(segments_errors, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng_state = np.random.get_state()\n",
    "np.random.shuffle(segments_counts)\n",
    "np.random.set_state(rng_state)\n",
    "np.random.shuffle(segments_errors)\n",
    "np.random.set_state(rng_state)\n",
    "np.random.shuffle(id_per_seg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(id_per_seg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('{}/594483_len128_stride8_1sec_cad_counts_errorfix.pkl'.format(data_dir), 'wb') as f:\n",
    "    pickle.dump(segments_counts, f)\n",
    "    \n",
    "with open('{}/594483_len128_stride8_1sec_cad_errors_errorfix.pkl'.format(data_dir), 'wb') as f:\n",
    "    pickle.dump(segments_errors, f)\n",
    "    \n",
    "with open('{}/594483_len128_stride8_1sec_cad_ids_errorfix.pkl'.format(data_dir), 'wb') as f:\n",
    "    pickle.dump(id_per_seg, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('{}/594483_len128_stride8_1sec_cad_ids_errorfix.pkl'.format(data_dir), 'rb') as f:\n",
    "    seg_ids = pickle.load(f)\n",
    "    \n",
    "ObID_per_sample = np.array([seg_id.split(\"_\")[0] for seg_id in seg_ids])\n",
    "\n",
    "\n",
    "needed_validation_segments = len(seg_ids)*0.25\n",
    "unique_ObIDs = np.unique(ObID_per_sample, return_counts=True)\n",
    "ObIDs_no = len(unique_ObIDs[0])\n",
    "shuffle_indices = np.array(range(ObIDs_no))\n",
    "np.random.seed(seed=11)\n",
    "np.random.shuffle(shuffle_indices)\n",
    "\n",
    "\n",
    "valid_set_obs = []\n",
    "valid_set_size = 0\n",
    "\n",
    "for ob_index in shuffle_indices:\n",
    "    valid_set_obs.append(unique_ObIDs[0][ob_index])\n",
    "    valid_set_size += unique_ObIDs[1][ob_index]\n",
    "    if valid_set_size > needed_validation_segments:\n",
    "        break\n",
    "        \n",
    "valid_set_sample_indices = []\n",
    "for valid_set_ob in np.array(valid_set_obs):\n",
    "    valid_set_sample_indices.append(np.where(ObID_per_sample == valid_set_ob)[0])\n",
    "\n",
    "valid_set_sample_indices = [item for sublist in valid_set_sample_indices for item in sublist]\n",
    "\n",
    "train_set_sample_indices = []\n",
    "for train_set_ob in shuffle_indices[len(valid_set_obs):]:\n",
    "    train_set_sample_indices.append(np.where(ObID_per_sample == unique_ObIDs[0][train_set_ob])[0])\n",
    "    \n",
    "train_set_sample_indices = [item for sublist in train_set_sample_indices for item in sublist]\n",
    "\n",
    "\n",
    "split_indices = [train_set_sample_indices, valid_set_sample_indices]\n",
    "\n",
    "# with open('{}/594483_len128_stride8_1sec_cad_observation75-25split.pkl'.format(data_dir), 'wb') as f:\n",
    "#     pickle.dump(split_indices, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(split_indices[0]), len(split_indices[1])/len(split_indices[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Histogram generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../../../data_GRS1915/468202_len128_s2_4cad_counts_errorfix.pkl', 'rb') as f:\n",
    "    segments_counts = pickle.load(f)\n",
    "with open('../../../data_GRS1915/468202_len128_s2_4cad_errors_errorfix.pkl', 'rb') as f:\n",
    "    segments_errors = pickle.load(f)\n",
    "with open('../../../data_GRS1915/468202_len128_s2_4cad_ids_errorfix.pkl', 'rb') as f:\n",
    "    id_per_seg = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "histograms = np.zeros((segments_counts.shape[0], 32))\n",
    "for seg_ind, seg in enumerate(segments_counts):\n",
    "    histograms[seg_ind] = np.histogram(seg.squeeze(), bins=32, range=[0,13000])[0]\n",
    "    print(seg_ind)\n",
    "    clear_output(wait=True)\n",
    "histograms = np.expand_dims(histograms, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../../../data_GRS1915/468202_len128_s2_4cad_histograms_32bin_0-13k_errorfix.pkl', 'wb') as f:\n",
    "    pickle.dump(histograms, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stat/feature calculation for segments as an alternative for histograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../../../data_GRS1915/468202_len128_s2_4cad_counts_errorfix.pkl', 'rb') as f:\n",
    "    segments_counts = pickle.load(f)\n",
    "with open('../../../data_GRS1915/468202_len128_s2_4cad_errors_errorfix.pkl', 'rb') as f:\n",
    "    segments_errors = pickle.load(f)\n",
    "with open('../../../data_GRS1915/468202_len128_s2_4cad_ids_errorfix.pkl', 'rb') as f:\n",
    "    id_per_seg = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "import umap\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from scipy.stats import zscore\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "desc_stats = np.zeros((len(segments_counts), 5)) # median, mean, std, skew, kurt, GM1_bic, GM2_bic, GM3_bic\n",
    "# search for descriptive statistics\n",
    "#https://towardsdatascience.com/modality-tests-and-kernel-density-estimations-3f349bb9e595"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "desc_stats[:,0] = np.median(segments_counts, axis=1).flatten()\n",
    "desc_stats[:,1] = np.mean(segments_counts, axis=1).flatten()\n",
    "desc_stats[:,2] = np.std(segments_counts, axis=1).flatten()\n",
    "desc_stats[:,3] = stats.skew(segments_counts, axis=1).flatten()\n",
    "desc_stats[:,4] = stats.kurtosis(segments_counts, axis=1).flatten()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_dir = \"../../../model_weights/model_2020-04-29_09-12-23.h5\"\n",
    "segments_dir = '../../../data_GRS1915/468202_len128_s2_4cad_counts_errorfix.pkl'\n",
    "segment_encoding_dir = '../../../data_GRS1915/segment_encoding_{}_segments_{}.pkl'.format(weights_dir.split(\"/\")[-1].split(\".\")[0], segments_dir.split(\"/\")[-1].split(\".\")[0])\n",
    "\n",
    "with open(segment_encoding_dir, 'rb') as f:\n",
    "    segment_encoding = pickle.load(f)\n",
    "    \n",
    "segment_encoding_scaled_means = zscore(segment_encoding[:,0,:], axis=None).astype(np.float32)  # standardize\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zscore_desc_stats = zscore(desc_stats, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GMM_bics = np.zeros((len(segments_counts), 3))\n",
    "\n",
    "for n_seg ,segment in enumerate(segments_counts):\n",
    "    clf = GaussianMixture(n_components=1, covariance_type='full', verbose=0)\n",
    "    clf.fit(segment)\n",
    "    GMM_bics[n_seg, 0] = clf.bic(segment)\n",
    "    clf = GaussianMixture(n_components=2, covariance_type='full', verbose=0)\n",
    "    clf.fit(segment)\n",
    "    GMM_bics[n_seg, 1] = clf.bic(segment)\n",
    "    clf = GaussianMixture(n_components=3, covariance_type='full', verbose=0)\n",
    "    clf.fit(segment)\n",
    "    GMM_bics[n_seg, 2] = clf.bic(segment)\n",
    "    GMM_bics[n_seg, :] = zscore(GMM_bics[n_seg, :])\n",
    "    print(n_seg)\n",
    "    clear_output(wait=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('../../../data_GRS1915/468202_segment_GMM_bic_1-3components_zscored.pkl', 'wb') as f:\n",
    "#     pickle.dump(GMM_bics, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "desc_GM = np.hstack((zscore(desc_stats, axis=0), GMM_bics))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shape_desc_GM = np.hstack((segment_encoding_scaled_means, desc_GM))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "UMAP_mapper = umap.UMAP(verbose=True)#n_neighbors=50, min_dist=0.0\n",
    "UMAP_mapper.fit(shape_desc_GM[:50000,:])\n",
    "umaped_data = UMAP_mapper.transform(shape_desc_GM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = (abs((np.min(umaped_data[:,0])-0.5) -(np.max(umaped_data[:,0])+0.5)), abs((np.min(umaped_data[:,1])-0.5)- (np.max(umaped_data[:,1])+0.5)))\n",
    "plt.scatter(umaped_data[:,0], umaped_data[:,1], s=0.05)\n",
    "plt.xlim([np.min(umaped_data[:,0])-0.5, np.max(umaped_data[:,0])+0.5])\n",
    "plt.ylim([np.min(umaped_data[:,1])-0.5, np.max(umaped_data[:,1])+0.5])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('{}/468202_len128_s2_4cad_ids_errorfix.pkl'.format(data_dir), 'rb') as f:\n",
    "    seg_ids = pickle.load(f)\n",
    "    \n",
    "ObID_per_sample = np.array([seg_id.split(\"_\")[0] for seg_id in seg_ids])\n",
    "\n",
    "\n",
    "needed_validation_segments = 468202/10\n",
    "unique_ObIDs = np.unique(ObID_per_sample, return_counts=True)\n",
    "ObIDs_no = len(unique_ObIDs[0])\n",
    "shuffle_indices = np.array(range(ObIDs_no))\n",
    "np.random.seed(seed=11)\n",
    "np.random.shuffle(shuffle_indices)\n",
    "\n",
    "\n",
    "valid_set_obs = []\n",
    "valid_set_size = 0\n",
    "\n",
    "for ob_index in shuffle_indices:\n",
    "    valid_set_obs.append(unique_ObIDs[0][ob_index])\n",
    "    valid_set_size += unique_ObIDs[1][ob_index]\n",
    "    if valid_set_size > needed_validation_segments:\n",
    "        break\n",
    "        \n",
    "valid_set_sample_indices = []\n",
    "for valid_set_ob in np.array(valid_set_obs):\n",
    "    valid_set_sample_indices.append(np.where(ObID_per_sample == valid_set_ob)[0])\n",
    "\n",
    "valid_set_sample_indices = [item for sublist in valid_set_sample_indices for item in sublist]\n",
    "\n",
    "train_set_sample_indices = []\n",
    "for train_set_ob in shuffle_indices[len(valid_set_obs):]:\n",
    "    train_set_sample_indices.append(np.where(ObID_per_sample == unique_ObIDs[0][train_set_ob])[0])\n",
    "    \n",
    "train_set_sample_indices = [item for sublist in train_set_sample_indices for item in sublist]\n",
    "\n",
    "\n",
    "split_indices = [train_set_sample_indices, valid_set_sample_indices]\n",
    "\n",
    "# with open('{}/468202_len128_s2_4cad_observation90-10split.pkl'.format(data_dir), 'wb') as f:\n",
    "#     pickle.dump(split_indices, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(split_indices[0]), len(split_indices[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(3224-565)**2/555**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(3224-565/555)**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../../../data_GRS1915/1776_light_curves_1s_bin_errorfix.pkl', 'rb') as f:\n",
    "    lcs = pickle.load(f)\n",
    "with open('../../../data_GRS1915/1776_light_curves_1s_bin_ids_errorfix.pkl', 'rb') as f:\n",
    "    ids = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "binned_stamps = int(4/1) # final cadence over original cadence\n",
    "f = lcs[1]\n",
    "bin_lc = np.sqrt(np.sum(f[2][:(len(f[2])//binned_stamps)*binned_stamps].reshape(-1, binned_stamps)**2, axis=1))/binned_stamps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sqrt(np.sum((lcs[0][2][:32])**2))/32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sqrt(np.sum((lcs[0][2][:32])**2)/32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "binned_stamps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bin_lc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_lc = 1\n",
    "perc_diff = []\n",
    "for n_lc in range(1000):\n",
    "\n",
    "    binned_stamps = int(4/1) # final cadence over original cadence\n",
    "    f = lcs[n_lc]\n",
    "    bin_lc = np.sqrt(np.sum(f[2][:(len(f[2])//binned_stamps)*binned_stamps].reshape(-1, binned_stamps)**2, axis=1))/binned_stamps\n",
    "\n",
    "    new_binned_errors=[]\n",
    "    for i in range(len(bin_lc)):\n",
    "        firstBinErrs = lcs[n_lc][2][i:i+4]\n",
    "        weights =  np.sum(firstBinErrs**-2.0) # sum inverse of variance\n",
    "        binErr=np.sqrt(1.0/(weights)) # root of sum of variances\n",
    "        new_binned_errors.append(binErr)\n",
    "    # print(new_binned_errors)\n",
    "\n",
    "    perc_diff.append(100*np.mean(np.array(new_binned_errors)-bin_lc)/np.mean(bin_lc)) # I've been slightly overestimating the errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(perc_diff, bins=20)\n",
    "plt.xlabel(\"Mean percentage difference between uncertainty values calculated with the two methods\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(perc_diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sqrt(    1/sum(er_array**-2)    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(np.array(new_binned_errors)-bin_lc)/np.mean(bin_lc) # I've been slightly overestimating the errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# binn\n",
    "bins=30# chosen number of bins across the period\n",
    "width=1.0/float(nbins)# calculate the width of the bins\n",
    "\n",
    "# create arrays for bin values and weights\n",
    "bins=np.zeros(nbins)\n",
    "weights=np.zeros(nbins)\n",
    "\n",
    "# bin!\n",
    "for i in range(len(flux)):\n",
    "    n=int(foldTimes[i]/width)# calculate bin number for this value\n",
    "    weight=err[i]**-2.0# calculate weight == error^-2\n",
    "    bins[n]+=flux[i]*weight# add weighted value to bin (value times weight)\n",
    "    weights[n]+=weight# add weight to bin weights\n",
    "    \n",
    "bins/=weights# normalise weighted values using sum of weights\n",
    "binErr=np.sqrt(1.0/(weights))# calculate bin errors from squared weights\n",
    "binEdges=np.arange(nbins)*width# create array of bin edge values for plotting\n",
    "\n",
    "plt.errorbar(binEdges,bins,yerr=binErr,linestyle='none',marker='o')# plotbinned lightcurve\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "X = np.array(list(range(8))+[\"a\", \"b\"])\n",
    "y = np.array([0, 0, 0,0,0,0,0,0,1,1])\n",
    "skf = StratifiedKFold(n_splits=2)\n",
    "skf.get_n_splits(X, y)\n",
    "\n",
    "print(skf)\n",
    "\n",
    "for train_index, test_index in skf.split(X, y):\n",
    "    \n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    print(\"TRAIN:\", X_train, \"TEST:\", X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1.47e7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0.125 cadence segments (16 s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Segment re-binned light curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def segmentation(time_series, seg_len, stride, keep_time_stamps=True, experimental = False, input_cadence=1):\n",
    "    \"\"\"\n",
    "    Create a list of 1D (when time_stamps=False) or 2D (when time_stamps=True) arrays, which are overlappig segments of ts. Incomplete fragments are rejected.\n",
    "\n",
    "    time_series = time series to be segmented\n",
    "    seg_len = length of a segment, \n",
    "    stride = step size; difference in the starting position of the consecutive segments\n",
    "    \"\"\"\n",
    "    segments=[]\n",
    "    for start in range(0, len(time_series[0])-seg_len, stride):\n",
    "        end=start+seg_len\n",
    "        if time_series[0][end]-time_series[0][start] != seg_len*input_cadence: #don't allow segments outside of good time intervals\n",
    "            continue\n",
    "        if keep_time_stamps==True:\n",
    "            segments.append(time_series[:,start:end])\n",
    "        else:\n",
    "            segments.append(time_series[1:,start:end])\n",
    "    return np.array(segments) # check why time stamps are kept "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cadence=0.125\n",
    "seg_len_s=16\n",
    "stride_s=1\n",
    "\n",
    "segments_counts=[]\n",
    "segments_times = []\n",
    "segments_errors=[]\n",
    "seg_ids=[]\n",
    "\n",
    "\n",
    "seg_len = seg_len_s//cadence # segment length and stride size in data points\n",
    "stride = stride_s//cadence\n",
    "\n",
    "\n",
    "\n",
    "for lc_index, lc in enumerate(lcs):\n",
    "    if len(lc[1]) >= seg_len: \n",
    "        segments = segmentation(lc, seg_len, stride, keep_time_stamps=True, experimental = False, input_cadence=cadence)\n",
    "    else:\n",
    "        continue\n",
    "    if len(segments) > 0:\n",
    "        segments_times.append(segments[:,0,:])\n",
    "        segments_counts.append(segments[:,1,:])\n",
    "        segments_errors.append(segments[:,2,:])\n",
    "        seg_ids.append(ids[lc_index])\n",
    "        print(\"Segmented {}/{} light curves.\".format(lc_index+1, len(lcs)))\n",
    "        clear_output(wait=True)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Shuffle light curve segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Stacking the segments and creating segment IDs, shuffling.\")\n",
    "id_per_seg = []  # for each light curve, copy the observation id for every segment of the light curve\n",
    "for lc_index, lc in enumerate(segments_counts):\n",
    "    for i in range(len(lc)):\n",
    "        id_per_seg.append(seg_ids[lc_index]+\"_{}\".format(i))\n",
    "\n",
    "segments_counts=np.vstack(segments_counts)\n",
    "segments_errors=np.vstack(segments_errors)\n",
    "segments_counts = np.expand_dims(segments_counts, axis=-1)\n",
    "segments_errors = np.expand_dims(segments_errors, axis=-1)\n",
    "\n",
    "rng_state = np.random.get_state()\n",
    "np.random.shuffle(segments_counts)\n",
    "np.random.set_state(rng_state)\n",
    "np.random.shuffle(segments_errors)\n",
    "np.random.set_state(rng_state)\n",
    "np.random.shuffle(id_per_seg)\n",
    "\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../../../data_GRS1915/468202_len128_stride8_4sec_cad_countrates_sum_bin.pkl', 'rb') as f:\n",
    "    segments = pickle.load(f)\n",
    "with open('../../../data_GRS1915/468202_len128_stride8_4sec_cad_errors_sum_bin.pkl', 'rb') as f:\n",
    "    errors = pickle.load(f)\n",
    "with open('../../../data_GRS1915/468202_len128_stride8_4sec_cad_ids_sum_bin.pkl', 'rb') as f:\n",
    "    ids = pickle.load(f)\n",
    "\n",
    "# errors = ((errors)/np.expand_dims(np.std(segments, axis=1), axis=1)).astype(np.float32)\n",
    "# segments = zscore(segments, axis=1).astype(np.float32)  # standardize per segment\n",
    "\n",
    "\n",
    "# with open('../../../data_GRS1915/lightcurve1776_train70_val10_test20.pkl', 'rb') as f:\n",
    "#     split_ob_ids = pickle.load(f)\n",
    "    \n",
    "# ids_no_index = [obid.split(\"_\")[0] for obid in ids]\n",
    "# training_segments_indices = np.array([seg_n for seg_n, seg in enumerate(ids_no_index) if seg in split_ob_ids[0]])\n",
    "# validation_segments_indices = np.array([seg_n for seg_n, seg in enumerate(ids_no_index) if seg in split_ob_ids[1]])\n",
    "# test_segments_indices = np.array([seg_n for seg_n, seg in enumerate(ids_no_index) if seg in split_ob_ids[2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../../../data_GRS1915/474471_len128_stride10_1sec_cad_countrates_sum_bin.pkl', 'rb') as f:\n",
    "    segments = pickle.load(f)\n",
    "with open('../../../data_GRS1915/474471_len128_stride10_1sec_cad_errors_sum_bin.pkl', 'rb') as f:\n",
    "    errors = pickle.load(f)\n",
    "with open('../../../data_GRS1915/474471_len128_stride10_1sec_cad_ids_sum_bin.pkl', 'rb') as f:\n",
    "    ids = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sqrt((segments[990][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "errors[990][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique([x.split(\"_\")[0] for x in ids]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_obs = np.unique([x.split(\"_\")[0] for x in np.take(ids, training_segments_indices)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_obs = np.unique([x.split(\"_\")[0] for x in np.take(ids, validation_segments_indices)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique([x.split(\"_\")[0] for x in np.take(ids, test_segments_indices)]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "[x for x in val_obs if x in val_obs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_segments_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(ids_no_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_segments_indices.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_segments_indices.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_segments_indices.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "326762+46445+94995"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jakub-tf",
   "language": "python",
   "name": "jakub-tf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
