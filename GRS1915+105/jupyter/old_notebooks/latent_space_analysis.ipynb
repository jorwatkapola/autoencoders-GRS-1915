{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from IPython.display import clear_output\n",
    "import os\n",
    "import fnmatch\n",
    "import numpy as np\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import umap\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from scipy import stats\n",
    "# from sklearn.cluster import OPTICS\n",
    "\n",
    "# import tensorflow as tf\n",
    "# from tensorflow.keras import layers\n",
    "# from tensorflow.keras.backend import mean\n",
    "# from tensorflow.keras.backend import square\n",
    "# from tensorflow.keras.models import Sequential\n",
    "# from tensorflow.keras.layers import CuDNNLSTM\n",
    "# from tensorflow.keras.layers import Dense\n",
    "# from tensorflow.keras.layers import RepeatVector\n",
    "# from tensorflow.keras.layers import TimeDistributed\n",
    "# from tensorflow.keras.callbacks import EarlyStopping\n",
    "# from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "# from tensorflow.keras.layers import Flatten\n",
    "\n",
    "# from tensorflow.keras.utils import Sequence\n",
    "# from tensorflow.keras import Input\n",
    "# from tensorflow.keras import Model\n",
    "# from tensorflow.keras.layers import BatchNormalization\n",
    "# from tensorflow.keras.layers import Conv1D\n",
    "from scipy.stats import zscore\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (5.0, 5.0)\n",
    "plt.rcParams.update({'font.size': 12})\n",
    "plt.rcParams.update(plt.rcParamsDefault)\n",
    "\n",
    "np.random.seed(seed=11)\n",
    "\n",
    "\n",
    "cwd = os.getcwd()\n",
    "\n",
    "if cwd.split(\"/\")[1] == \"export\":\n",
    "    data_dir = \"../../../files_from_snuffy\"\n",
    "else:\n",
    "    data_dir = \"../../../data_GRS1915\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !for pid in `pgrep -f jupyter`; do { renice 5 $pid; }; done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !for pid in `pgrep -f jupyter`; do { ps -u -p $pid; }; done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !stat -c '%y' OPTICS_shape16_moments4_max_eps4_min_samp500_euclidean_alldata.pkl\n",
    "# 2020-06-22 01:46:51.500058368 +0100\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the model (shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_dir = \"../../../model_weights/model_2020-04-29_09-12-23.h5\"\n",
    "\n",
    "class Sampling(layers.Layer):\n",
    "    \"\"\"Uses (z_mean, z_log_var) to sample z, the vector encoding a digit.\n",
    "    https://www.tensorflow.org/guide/keras/custom_layers_and_models#putting_it_all_together_an_end-to-end_example\"\"\"\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        z_mean, z_log_var = inputs\n",
    "        batch = tf.shape(z_mean)[0]\n",
    "        dim = tf.shape(z_mean)[1]\n",
    "        epsilon = tf.keras.backend.random_normal(shape=(batch, dim))\n",
    "        return z_mean + tf.exp(0.5 * z_log_var) * epsilon\n",
    "\n",
    "\n",
    "original_dim = 128\n",
    "intermediate_dim = 512\n",
    "latent_dim = 16\n",
    "\n",
    "# Define encoder model.\n",
    "original_inputs = tf.keras.Input(shape=(original_dim,1), name='encoder_input')\n",
    "input_err = Input(shape=(original_dim,1))\n",
    "x = layers.CuDNNLSTM(intermediate_dim, return_sequences=False)(original_inputs)\n",
    "z_mean = layers.Dense(latent_dim, name='z_mean')(x)\n",
    "z_log_var = layers.Dense(latent_dim, name='z_log_var')(x)\n",
    "z = Sampling()((z_mean, z_log_var))\n",
    "encoder = tf.keras.Model(inputs=original_inputs, outputs=z, name='encoder')\n",
    "\n",
    "# Define decoder model.\n",
    "latent_inputs = tf.keras.Input(shape=(latent_dim,), name='z_sampling')\n",
    "x = layers.RepeatVector(original_dim)(latent_inputs)\n",
    "x = layers.CuDNNLSTM(intermediate_dim, return_sequences=True)(x)\n",
    "outputs = layers.TimeDistributed(layers.Dense(1))(x)\n",
    "decoder = tf.keras.Model(inputs=latent_inputs, outputs=outputs, name='decoder')\n",
    "\n",
    "# Define VAE model.\n",
    "outputs = decoder(z)\n",
    "vae = tf.keras.Model(inputs=[original_inputs, input_err], outputs=outputs, name='vae')\n",
    "\n",
    "vae.load_weights(weights_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data (segments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "segments_dir = '../../../data_GRS1915/468202_len128_s2_4cad_counts_errorfix.pkl'\n",
    "errors_dir = '../../../data_GRS1915/468202_len128_s2_4cad_errors_errorfix.pkl'\n",
    "\n",
    "with open(segments_dir, 'rb') as f:\n",
    "    segments = pickle.load(f)\n",
    "with open(errors_dir, 'rb') as f:\n",
    "    errors = pickle.load(f)\n",
    "\n",
    "\n",
    "errors = ((errors)/np.expand_dims(np.std(segments, axis=1), axis=1)).astype(np.float32)\n",
    "segments = zscore(segments, axis=1).astype(np.float32)  # standardize per segment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Try reconstruction (shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try_first_x = 20\n",
    "reconstructions = np.zeros((try_first_x, segments.shape[1]))\n",
    "for segment_index, segment in enumerate(segments[:try_first_x]):\n",
    "    reconstructions[segment_index] = vae.predict([np.expand_dims(segment, axis=0), np.expand_dims(errors[segment_index], axis=0)]).flatten()\n",
    "    \n",
    "plot_index = np.copy(try_first_x)-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = (10.0, 5.0)\n",
    "plt.rcParams.update({'font.size': 8})\n",
    "\n",
    "plt.errorbar(np.linspace(0,512, 128), segments[plot_index], yerr=errors[plot_index], ecolor=\"magenta\")\n",
    "plt.plot(np.linspace(0,512, 128), reconstructions[plot_index], color=\"orange\")\n",
    "plt.ylabel(\"Normalized count rate\", fontsize=12)\n",
    "plt.xlabel(\"Time (seconds)\", fontsize=12)\n",
    "plt.show()\n",
    "if plot_index>0:\n",
    "    plot_index-=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encode the data set (shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_encoder = tf.keras.Model(inputs=vae.input, outputs=[vae.get_layer(\"z_mean\").output, vae.get_layer(\"z_log_var\").output])\n",
    "segment_encoding = np.zeros((segments.shape[0], 2, 16))\n",
    "for seg_ind, seg in enumerate(segments):\n",
    "    prediction = trained_encoder.predict([np.expand_dims(seg, axis=0), np.expand_dims(errors[seg_ind], axis=0)])\n",
    "    segment_encoding[seg_ind][0] = prediction[0].flatten()\n",
    "    segment_encoding[seg_ind][1] = prediction[1].flatten()\n",
    "    print(seg_ind)\n",
    "    clear_output(wait=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "segment_encoding_dir = '../../../data_GRS1915/segment_encoding_{}_segments_{}.pkl'.format(weights_dir.split(\"/\")[-1].split(\".\")[0], segments_dir.split(\"/\")[-1].split(\".\")[0])\n",
    "# with open(segment_encoding_dir, 'wb') as f:\n",
    "#     pickle.dump(segment_encoding, f)\n",
    "print(\"Encodings saved to: \", segment_encoding_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PCA the encoded data (shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_dir = \"../../../model_weights/model_2020-04-29_09-12-23.h5\"\n",
    "segments_dir = '../../../data_GRS1915/468202_len128_s2_4cad_counts_errorfix.pkl'\n",
    "segment_encoding_dir = '../../../data_GRS1915/segment_encoding_{}_segments_{}.pkl'.format(weights_dir.split(\"/\")[-1].split(\".\")[0], segments_dir.split(\"/\")[-1].split(\".\")[0])\n",
    "\n",
    "with open(segment_encoding_dir, 'rb') as f:\n",
    "    segment_encoding = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import IncrementalPCA, PCA\n",
    "\n",
    "n_components = None# segment_encoding.shape[-1]\n",
    "# ipca = IncrementalPCA(n_components=n_components, batch_size=468202)\n",
    "pca = PCA(n_components=n_components)\n",
    "\n",
    "# X_ipca = ipca.fit(segment_encoding[:,0,:])\n",
    "X_pca = pca.fit(segment_encoding[:,0,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_transformed = X_ipca.transform(segment_encoding[:,0,:])\n",
    "X_transformed = X_pca.transform(segment_encoding[:,0,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probe_points = np.zeros((5,10,16))#5 PCs, 10 points, 16 coordinates\n",
    "\n",
    "for PC, eigen_vec in enumerate(pca.components_[:5]):#first 5 components give 78% of explained variance, next one is 4.3%\n",
    "    bins = np.linspace(np.min(X_transformed[:,PC]),np.max(X_transformed[:,PC]), 11)\n",
    "    binned_indices = np.digitize(X_transformed[:,PC], bins)\n",
    "    for bin_index in range(10):\n",
    "        bin_segment_indices = np.where(binned_indices == bin_index+1)[0]\n",
    "        probe_points[PC,bin_index,:] = np.mean(X_transformed[bin_segment_indices], axis=0)\n",
    "#     tenth_range = (np.max(X_transformed[:,PC]) - np.min(X_transformed[:,PC]))/10\n",
    "#     for point in range(10):\n",
    "#         probe_points[PC,point,:] = eigen_vec\n",
    "#     np.max(X_transformed[:,PC])/tenth_range\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "umaped_data = X_transformed\n",
    "plt.rcParams['figure.figsize'] = (abs((np.min(umaped_data[:,0])-0.5) -(np.max(umaped_data[:,0])+0.5)), abs((np.min(umaped_data[:,1])-0.5)- (np.max(umaped_data[:,1])+0.5)))\n",
    "plt.scatter(X_transformed[:,0], X_transformed[:,1], s=0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# UMAP the encoded data (shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(segment_encoding_dir, 'rb') as f:\n",
    "    segment_encoding = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "UMAP_mapper = umap.UMAP(verbose=True)#n_neighbors=50, min_dist=0.0, local_connectivity, repulsion_strength, negative_sample_rate\n",
    "UMAP_mapper.fit(segment_encoding[:100000,0,:])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('../../../data_GRS1915/fast_UMAPmapper_means_{}.pkl'.format(segment_encoding_dir.split(\"/\")[-1]), 'wb') as f:\n",
    "#     pickle.dump(UMAP_mapper, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../../../data_GRS1915/fast_UMAPmapper_means_{}.pkl'.format(segment_encoding_dir.split(\"/\")[-1]), 'rb') as f:\n",
    "    UMAP_mapper = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "umaped_data = UMAP_mapper.transform(segment_encoding[:,0,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "encoded_probes = np.zeros((5,10,2))\n",
    "\n",
    "for n, PC in enumerate(probe_points):\n",
    "    UMAPed_PC_centroids = UMAP_mapper.transform(PC)\n",
    "    encoded_probes[n] = UMAPed_PC_centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = (abs((np.min(umaped_data[:,0])-0.5) -(np.max(umaped_data[:,0])+0.5)), abs((np.min(umaped_data[:,1])-0.5)- (np.max(umaped_data[:,1])+0.5)))\n",
    "plt.scatter(umaped_data[:,0], umaped_data[:,1], s=0.05)\n",
    "plt.xlim([np.min(umaped_data[:,0])-0.5, np.max(umaped_data[:,0])+0.5])\n",
    "plt.ylim([np.min(umaped_data[:,1])-0.5, np.max(umaped_data[:,1])+0.5])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = (abs((np.min(umaped_data[:,0])-0.5) -(np.max(umaped_data[:,0])+0.5)), abs((np.min(umaped_data[:,1])-0.5)- (np.max(umaped_data[:,1])+0.5)))\n",
    "plt.scatter(umaped_data[:,0], umaped_data[:,1], s=0.05, c=\"magenta\")\n",
    "plt.xlim([np.min(umaped_data[:,0])-0.5, np.max(umaped_data[:,0])+0.5])\n",
    "plt.ylim([np.min(umaped_data[:,1])-0.5, np.max(umaped_data[:,1])+0.5])\n",
    "\n",
    "for pc in encoded_probes[:4]:\n",
    "    plt.plot(pc[:,0], pc[:,1])\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate synthetic samples along principal components(shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_dir = \"../../../model_weights/model_2020-04-29_09-12-23.h5\"\n",
    "\n",
    "class Sampling(layers.Layer):\n",
    "    \"\"\"Uses (z_mean, z_log_var) to sample z, the vector encoding a digit.\n",
    "    https://www.tensorflow.org/guide/keras/custom_layers_and_models#putting_it_all_together_an_end-to-end_example\"\"\"\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        z_mean, z_log_var = inputs\n",
    "        batch = tf.shape(z_mean)[0]\n",
    "        dim = tf.shape(z_mean)[1]\n",
    "        epsilon = tf.keras.backend.random_normal(shape=(batch, dim))\n",
    "        return z_mean + tf.exp(0.5 * z_log_var) * epsilon\n",
    "\n",
    "\n",
    "original_dim = 128\n",
    "intermediate_dim = 512\n",
    "latent_dim = 16\n",
    "\n",
    "# Define encoder model.\n",
    "original_inputs = tf.keras.Input(shape=(original_dim,1), name='encoder_input')\n",
    "input_err = Input(shape=(original_dim,1))\n",
    "x = layers.CuDNNLSTM(intermediate_dim, return_sequences=False)(original_inputs)\n",
    "z_mean = layers.Dense(latent_dim, name='z_mean')(x)\n",
    "z_log_var = layers.Dense(latent_dim, name='z_log_var')(x)\n",
    "z = Sampling()((z_mean, z_log_var))\n",
    "encoder = tf.keras.Model(inputs=original_inputs, outputs=z, name='encoder')\n",
    "\n",
    "# Define decoder model.\n",
    "latent_inputs = tf.keras.Input(shape=(latent_dim,), name='z_sampling')\n",
    "x = layers.RepeatVector(original_dim)(latent_inputs)\n",
    "x = layers.CuDNNLSTM(intermediate_dim, return_sequences=True)(x)\n",
    "outputs = layers.TimeDistributed(layers.Dense(1))(x)\n",
    "decoder = tf.keras.Model(inputs=latent_inputs, outputs=outputs, name='decoder')\n",
    "\n",
    "# Define VAE model.\n",
    "outputs = decoder(z)\n",
    "vae = tf.keras.Model(inputs=[original_inputs, input_err], outputs=outputs, name='vae')\n",
    "\n",
    "vae.load_weights(weights_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "generated_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_decoder = tf.keras.Model(inputs=vae.get_layer(\"decoder\").input, outputs=vae.get_layer(\"decoder\").output)\n",
    "generated_samples = np.zeros((5,10, 128))\n",
    "for n_PC, PC in enumerate(probe_points):\n",
    "    for n_point, point in enumerate(PC):\n",
    "        prediction = trained_decoder.predict(np.expand_dims(point, axis=0))\n",
    "        generated_samples[n_PC,n_point, :] = prediction.flatten()\n",
    "\n",
    "        print(n_point)\n",
    "        clear_output(wait=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = (10.0, 5.0)\n",
    "plt.rcParams.update({'font.size': 8})\n",
    "plot_index = 0\n",
    "PC_ind = 4\n",
    "# plt.errorbar(np.linspace(0,512, 128), segments[plot_index], yerr=errors[plot_index], ecolor=\"magenta\")\n",
    "offset = 0\n",
    "for n_lc, lc in enumerate(generated_samples[PC_ind]):\n",
    "    if n_lc>0:\n",
    "        offset += np.max(generated_samples[PC_ind, n_lc])-np.min(generated_samples[PC_ind, n_lc])\n",
    "    plt.plot(np.linspace(0,512, 128), generated_samples[PC_ind, n_lc]+offset)\n",
    "# plt.ylabel(\"Normalized count rate\", fontsize=12)\n",
    "plt.xlabel(\"Time (seconds)\", fontsize=12)\n",
    "plt.title(\"Series generated along Principal Component {}\".format(PC_ind+1), fontsize=15)\n",
    "plt.show()\n",
    "if plot_index>0:\n",
    "    plot_index-=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cluster the latent space(shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.mixture import GaussianMixture\n",
    "\n",
    "clf = GaussianMixture(n_components=20, covariance_type='full', verbose=1)\n",
    "clf.fit(segment_encoding[:100000,0,:])\n",
    "# aics[n_comps] = clf.aic(segment_encoding[:,0,:])\n",
    "# print(n_comps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "custer_labels = clf.predict(segment_encoding[:,0,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_means = UMAP_mapper.transform(clf.means_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = (abs((np.min(umaped_data[:,0])-0.5) -(np.max(umaped_data[:,0])+0.5)), abs((np.min(umaped_data[:,1])-0.5)- (np.max(umaped_data[:,1])+0.5)))\n",
    "plt.scatter(umaped_data[:,0], umaped_data[:,1], s=0.05, c=custer_labels)\n",
    "plt.xlim([np.min(umaped_data[:,0])-0.5, np.max(umaped_data[:,0])+0.5])\n",
    "plt.ylim([np.min(umaped_data[:,1])-0.5, np.max(umaped_data[:,1])+0.5])\n",
    "for mean_n, mean in enumerate(transformed_means[15:]):\n",
    "    plt.scatter(mean[0], mean[1], s=300, label=mean_n, edgecolors=\"white\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate synthetic samples from Gaussian centroids (shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_dir = \"../../../model_weights/model_2020-04-29_09-12-23.h5\"\n",
    "\n",
    "class Sampling(layers.Layer):\n",
    "    \"\"\"Uses (z_mean, z_log_var) to sample z, the vector encoding a digit.\n",
    "    https://www.tensorflow.org/guide/keras/custom_layers_and_models#putting_it_all_together_an_end-to-end_example\"\"\"\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        z_mean, z_log_var = inputs\n",
    "        batch = tf.shape(z_mean)[0]\n",
    "        dim = tf.shape(z_mean)[1]\n",
    "        epsilon = tf.keras.backend.random_normal(shape=(batch, dim))\n",
    "        return z_mean + tf.exp(0.5 * z_log_var) * epsilon\n",
    "\n",
    "\n",
    "original_dim = 128\n",
    "intermediate_dim = 512\n",
    "latent_dim = 16\n",
    "\n",
    "# Define encoder model.\n",
    "original_inputs = tf.keras.Input(shape=(original_dim,1), name='encoder_input')\n",
    "input_err = Input(shape=(original_dim,1))\n",
    "x = layers.CuDNNLSTM(intermediate_dim, return_sequences=False)(original_inputs)\n",
    "z_mean = layers.Dense(latent_dim, name='z_mean')(x)\n",
    "z_log_var = layers.Dense(latent_dim, name='z_log_var')(x)\n",
    "z = Sampling()((z_mean, z_log_var))\n",
    "encoder = tf.keras.Model(inputs=original_inputs, outputs=z, name='encoder')\n",
    "\n",
    "# Define decoder model.\n",
    "latent_inputs = tf.keras.Input(shape=(latent_dim,), name='z_sampling')\n",
    "x = layers.RepeatVector(original_dim)(latent_inputs)\n",
    "x = layers.CuDNNLSTM(intermediate_dim, return_sequences=True)(x)\n",
    "outputs = layers.TimeDistributed(layers.Dense(1))(x)\n",
    "decoder = tf.keras.Model(inputs=latent_inputs, outputs=outputs, name='decoder')\n",
    "\n",
    "# Define VAE model.\n",
    "outputs = decoder(z)\n",
    "vae = tf.keras.Model(inputs=[original_inputs, input_err], outputs=outputs, name='vae')\n",
    "\n",
    "vae.load_weights(weights_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "trained_decoder = tf.keras.Model(inputs=vae.get_layer(\"decoder\").input, outputs=vae.get_layer(\"decoder\").output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_samples = 5\n",
    "generated_samples = np.zeros((no_samples, 128))\n",
    "for n_point, point in enumerate(clf.means_[15:]):\n",
    "    prediction = trained_decoder.predict(np.expand_dims(point, axis=0))\n",
    "    generated_samples[n_point, :] = prediction.flatten()\n",
    "\n",
    "    print(n_point)\n",
    "    clear_output(wait=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = (10.0, 5.0)\n",
    "plt.rcParams.update({'font.size': 8})\n",
    "plot_index = 0\n",
    "PC_ind = 4\n",
    "# plt.errorbar(np.linspace(0,512, 128), segments[plot_index], yerr=errors[plot_index], ecolor=\"magenta\")\n",
    "offset = 0\n",
    "for n_lc, lc in enumerate(generated_samples):\n",
    "    if n_lc>0:\n",
    "        offset += np.max(lc)-np.min(lc)+ (np.max(generated_samples[n_lc])-np.min(generated_samples[n_lc]))\n",
    "    plt.plot(np.linspace(0,512, 128), lc+offset)\n",
    "# plt.ylabel(\"Normalized count rate\", fontsize=12)\n",
    "plt.xlabel(\"Time (seconds)\", fontsize=12)\n",
    "plt.title(\"Series generated from Gaussian mixture means\", fontsize=15)\n",
    "plt.show()\n",
    "if plot_index>0:\n",
    "    plot_index-=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overplot UMAP with the classified data (shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load observation classifications from Huppenkothen 2017\n",
    "# %matplotlib inline\n",
    "\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "clean_belloni = open('../../../data_GRS1915/1915Belloniclass_updated.dat')\n",
    "lines = clean_belloni.readlines()\n",
    "states = lines[0].split()\n",
    "belloni_clean = {}\n",
    "for h,l in zip(states, lines[1:]):\n",
    "    belloni_clean[h] = l.split()\n",
    "    #state: obsID1, obsID2...\n",
    "ob_state = {}\n",
    "for state, obs in belloni_clean.items():\n",
    "    if state == \"chi1\" or state == \"chi2\" or state == \"chi3\" or state == \"chi4\": state = \"chi\"\n",
    "    for ob in obs:\n",
    "        ob_state[ob] = state\n",
    "        \n",
    "# load segmented light curves\n",
    "\n",
    "import pickle\n",
    "with open('../../../data_GRS1915/468202_len128_s2_4cad_counts_errorfix.pkl', 'rb') as f:\n",
    "    segments = pickle.load(f)\n",
    "with open('../../../data_GRS1915/468202_len128_s2_4cad_ids_errorfix.pkl', 'rb') as f:\n",
    "    seg_ids = pickle.load(f)\n",
    "\n",
    "# # HF QPO observation ids\n",
    "# paper_obIDs = np.loadtxt(\"../../../data_GRS1915/Belloni_Altamirano_obsIDs.txt\", dtype=str)\n",
    "\n",
    "# qpo_colours = []\n",
    "\n",
    "# for seg_id in seg_ids:\n",
    "#     if seg_id.split(\"_\")[0] in paper_obIDs:\n",
    "#         qpo_colours.append(\"red\")\n",
    "#     else:\n",
    "#         qpo_colours.append(\"grey\")\n",
    "        \n",
    "# qpo_labels = []\n",
    "\n",
    "# for seg_id in seg_ids:\n",
    "#     if seg_id.split(\"_\")[0] in paper_obIDs:\n",
    "#         qpo_labels.append(\"QPO\")\n",
    "#     else:\n",
    "#         qpo_labels.append(\"other\")\n",
    "        \n",
    "        \n",
    "# qpo_scales = []\n",
    "\n",
    "# for seg_id in seg_ids:\n",
    "#     if seg_id.split(\"_\")[0] in paper_obIDs:\n",
    "#         qpo_scales.append(\"QPO\")\n",
    "#     else:\n",
    "#         qpo_scales.append(\"other\")\n",
    "        \n",
    "        \n",
    "xxx = [seg.split(\"_\")[0] for seg in seg_ids]\n",
    "\n",
    "classes = np.array([\"alpha\", \"beta\", \"gamma\", \"delta\", \"theta\", \"kappa\", \"lambda\", \"mu\", \"nu\", \"rho\", \"phi\", \"chi\", \"eta\", \"omega\"])\n",
    "class_colour = []\n",
    "for ob in xxx:\n",
    "    if ob in ob_state:\n",
    "        class_colour.append(np.where(classes == ob_state[ob])[0][0])\n",
    "    else:\n",
    "        class_colour.append(15)\n",
    "        \n",
    "classes = np.array([\"alpha\", \"beta\", \"gamma\", \"delta\", \"theta\", \"kappa\", \"lambda\", \"mu\", \"nu\", \"rho\", \"phi\", \"chi\", \"eta\", \"omega\"])\n",
    "scales = []\n",
    "segment_class = []\n",
    "for ob in xxx:\n",
    "    if ob in ob_state:\n",
    "        segment_class.append(ob_state[ob])\n",
    "        scales.append(5)\n",
    "    else:\n",
    "        segment_class.append(\"Unknown\")\n",
    "        scales.append(0.1)\n",
    "        \n",
    "        \n",
    "from matplotlib import cm\n",
    "cm.get_cmap(plt.get_cmap(\"Set1\"))\n",
    "\n",
    "\n",
    "colours = ['#ffd8b1', '#000075', '#808080', '#e6194b', '#3cb44b', '#ffe119', '#4363d8', '#f58231', '#911eb4', '#46f0f0', '#f032e6', '#bcf60c', '#fabebe', '#008080', '#e6beff', '#9a6324', '#fffac8', '#800000', '#aaffc3', '#808000', '#000000']\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "# sns.set_style(\"white\")\n",
    "plt.rcParams['figure.figsize'] = (30.0, 30.0)\n",
    "plt.rcParams.update({'font.size': 0})\n",
    "\n",
    "embeddings_lap = umaped_data\n",
    "\n",
    "# fig, ax = plt.subplots()\n",
    "\n",
    "fig, axs = plt.subplots(4, 4)\n",
    "axs = axs.flatten()\n",
    "\n",
    "for plot_class_ind, plot_class in enumerate(classes):\n",
    "    class_indices = np.where(np.array(segment_class) == \"Unknown\")[0]\n",
    "    class_data = embeddings_lap[class_indices]\n",
    "    axs[plot_class_ind].scatter(class_data[:,0], class_data[:,1], s = 0.2, c=\"grey\", label=\"Unknown\")\n",
    "\n",
    "    class_indices = np.where(np.array(segment_class) == plot_class)[0]\n",
    "    class_data = embeddings_lap[class_indices]\n",
    "    \n",
    "    axs[plot_class_ind].scatter(class_data[:,0], class_data[:,1], s = 25, c='red', label=plot_class)\n",
    "    \n",
    "# plt.legend()\n",
    "    axs[plot_class_ind].set_title(\"{}\".format(plot_class), fontsize=42)\n",
    "axs.reshape((4,4))\n",
    "# plt.savefig(\"classes_separate.png\")\n",
    "\n",
    "# plt.savefig(\"UMAP_embedding_separate_classes_model_2020-02-09_10-36-06.png\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# redint = np.where(np.array(qpo_colours) == \"red\")\n",
    "# greyint= np.where(np.array(qpo_colours) != \"red\")\n",
    "# plt.scatter(embeddings_lap[:,0][greyint], embeddings_lap[:,1][greyint], s=1, c=\"grey\", label= \"other\")\n",
    "# plt.scatter(embeddings_lap[:,0][redint], embeddings_lap[:,1][redint], s=1, c=\"red\", label= \"HF QPO\")\n",
    "# plt.title(\"UMAP embedding of the encoded GRS1915 segments, neighbors=50, min_dist=0.0, components=2\", fontsize=12)\n",
    "# plt.legend()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Histogram reconstruction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_dir = \"../../../model_weights/model_2020-04-29_13-04-35.h5\" #24>16>24\n",
    "segments_dir = '../../../data_GRS1915/468202_len128_s2_4cad_histograms_24bin_0-13k_errorfix.pkl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(segments_dir, 'rb') as f:\n",
    "    segments = pickle.load(f)\n",
    "# with open('../../data_GRS1915/94465_len512_s40_errors_errorfix.pkl', 'rb') as f:\n",
    "#     errors = pickle.load(f)\n",
    "    \n",
    "# errors = ((errors)/np.std(segments)).astype(np.float32)\n",
    "segments = zscore(segments, axis=None).astype(np.float32)  # standardize\n",
    "\n",
    "\n",
    "class Sampling(layers.Layer):\n",
    "    \"\"\"Uses (z_mean, z_log_var) to sample z, the vector encoding a digit.\n",
    "    https://www.tensorflow.org/guide/keras/custom_layers_and_models#putting_it_all_together_an_end-to-end_example\"\"\"\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        z_mean, z_log_var = inputs\n",
    "        batch = tf.shape(z_mean)[0]\n",
    "        dim = tf.shape(z_mean)[1]\n",
    "        epsilon = tf.keras.backend.random_normal(shape=(batch, dim))\n",
    "        return z_mean + tf.exp(0.5 * z_log_var) * epsilon\n",
    "\n",
    "original_dim = 24\n",
    "intermediate_dim = 64\n",
    "latent_dim = 16\n",
    "\n",
    "# Define encoder model.\n",
    "original_inputs = tf.keras.Input(shape=(original_dim,1), name='encoder_input')\n",
    "input_err = Input(shape=(original_dim,1))\n",
    "x = layers.CuDNNLSTM(intermediate_dim, return_sequences=False)(original_inputs)\n",
    "z_mean = layers.Dense(latent_dim, name='z_mean')(x)\n",
    "z_log_var = layers.Dense(latent_dim, name='z_log_var')(x)\n",
    "z = Sampling()((z_mean, z_log_var))\n",
    "encoder = tf.keras.Model(inputs=original_inputs, outputs=z, name='encoder')\n",
    "\n",
    "# Define decoder model.\n",
    "latent_inputs = tf.keras.Input(shape=(latent_dim,), name='z_sampling')\n",
    "x = layers.RepeatVector(original_dim)(latent_inputs)\n",
    "x = layers.CuDNNLSTM(intermediate_dim, return_sequences=True)(x)\n",
    "outputs = layers.TimeDistributed(layers.Dense(1))(x)\n",
    "decoder = tf.keras.Model(inputs=latent_inputs, outputs=outputs, name='decoder')\n",
    "\n",
    "# Define VAE model.\n",
    "outputs = decoder(z)\n",
    "vae = tf.keras.Model(inputs=original_inputs, outputs=outputs, name='vae')\n",
    "\n",
    "\n",
    "vae.load_weights(weights_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try_first_x = 200\n",
    "hist_reconstructions = np.zeros((try_first_x, segments.shape[1]))\n",
    "for segment_index, segment in enumerate(segments[:try_first_x]):\n",
    "    hist_reconstructions[segment_index] = vae.predict(np.expand_dims(segment, axis=0)).flatten()\n",
    "    \n",
    "plot_index = np.copy(try_first_x)-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(segments[plot_index])\n",
    "plt.plot(hist_reconstructions[plot_index])\n",
    "plt.show()\n",
    "if plot_index>0:\n",
    "    plot_index-=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encode the data set (histograms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_encoder = tf.keras.Model(inputs=vae.input, outputs=[vae.get_layer(\"z_mean\").output, vae.get_layer(\"z_log_var\").output])\n",
    "segment_encoding = np.zeros((segments.shape[0], 2, 16))\n",
    "for seg_ind, seg in enumerate(segments):\n",
    "    prediction = trained_encoder.predict(np.expand_dims(seg, axis=0))\n",
    "    segment_encoding[seg_ind][0] = prediction[0].flatten()\n",
    "    segment_encoding[seg_ind][1] = prediction[1].flatten()\n",
    "    print(seg_ind)\n",
    "    clear_output(wait=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_dir = \"../../../model_weights/model_2020-04-29_13-04-35.h5\" #24>16>24\n",
    "\n",
    "segments_dir = '../../../data_GRS1915/468202_len128_s2_4cad_histograms_24bin_0-13k_errorfix.pkl'\n",
    "segment_encoding_dir = '../../../data_GRS1915/histogram_encoding_{}_segments_{}.pkl'.format(weights_dir.split(\"/\")[-1].split(\".\")[0], segments_dir.split(\"/\")[-1].split(\".\")[0])\n",
    "# with open(segment_encoding_dir, 'wb') as f:\n",
    "#     pickle.dump(segment_encoding, f)\n",
    "print(\"Encodings saved to: \", segment_encoding_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# UMAP the encoded data (histograms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(segment_encoding_dir, 'rb') as f:\n",
    "    segment_encoding = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(segments_dir, 'rb') as f:\n",
    "    segments = pickle.load(f)\n",
    "# with open('../../data_GRS1915/94465_len512_s40_errors_errorfix.pkl', 'rb') as f:\n",
    "#     errors = pickle.load(f)\n",
    "    \n",
    "# errors = ((errors)/np.std(segments)).astype(np.float32)\n",
    "segments = zscore(segments, axis=None).astype(np.float32)  # standardize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "UMAP_mapper = umap.UMAP(verbose=True)#n_neighbors=50, min_dist=0.0\n",
    "UMAP_mapper.fit(segment_encoding[:100000,:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "umaped_data = UMAP_mapper.transform(segment_encoding[:,:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = (abs((np.min(umaped_data[:,0])-0.5) -(np.max(umaped_data[:,0])+0.5)), abs((np.min(umaped_data[:,1])-0.5)- (np.max(umaped_data[:,1])+0.5)))\n",
    "plt.scatter(umaped_data[:,0], umaped_data[:,1], s=0.05)\n",
    "plt.xlim([np.min(umaped_data[:,0])-0.5, np.max(umaped_data[:,0])+0.5])\n",
    "plt.ylim([np.min(umaped_data[:,1])-0.5, np.max(umaped_data[:,1])+0.5])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cluster the latent space (histograms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.mixture import GaussianMixture\n",
    "\n",
    "clf = GaussianMixture(n_components=100, covariance_type='full', verbose=1)\n",
    "clf.fit(segment_encoding[:100000,0,:])\n",
    "# aics[n_comps] = clf.aic(segment_encoding[:,0,:])\n",
    "# print(n_comps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "custer_labels = clf.predict(segment_encoding[:,0,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_means = UMAP_mapper.transform(clf.means_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prop_cycle = plt.rcParams['axes.prop_cycle']\n",
    "colors = prop_cycle.by_key()['color']\n",
    "color_matched_9 = [\"#a8a495\" if x>9 else colors[x] for x in custer_labels]\n",
    "# color_matched_19= [\"#a8a495\" if x<=9 else colors[x-10] for x in custer_labels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prop_cycle = plt.rcParams['axes.prop_cycle']\n",
    "colors = prop_cycle.by_key()['color']\n",
    "\n",
    "color_matched_9 = [\"#a8a495\" if x>9 else colors[x] for x in custer_labels]\n",
    "# color_matched_19= [\"#a8a495\" if x<=9 else colors[x-10] for x in custer_labels]\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (abs((np.min(umaped_data[:,0])-0.5) -(np.max(umaped_data[:,0])+0.5)), abs((np.min(umaped_data[:,1])-0.5)- (np.max(umaped_data[:,1])+0.5)))\n",
    "plt.scatter(umaped_data[:,0], umaped_data[:,1], s=0.05, c=color_matched_9)\n",
    "plt.xlim([np.min(umaped_data[:,0])-0.5, np.max(umaped_data[:,0])+0.5])\n",
    "plt.ylim([np.min(umaped_data[:,1])-0.5, np.max(umaped_data[:,1])+0.5])\n",
    "for mean_n, mean in enumerate(transformed_means[:10]):\n",
    "    plt.scatter(mean[0], mean[1], s=300, label=mean_n, edgecolors=\"white\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate synthetic samples (histograms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_dir = \"../../../model_weights/model_2020-04-29_13-04-35.h5\" #24>16>24\n",
    "\n",
    "class Sampling(layers.Layer):\n",
    "    \"\"\"Uses (z_mean, z_log_var) to sample z, the vector encoding a digit.\n",
    "    https://www.tensorflow.org/guide/keras/custom_layers_and_models#putting_it_all_together_an_end-to-end_example\"\"\"\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        z_mean, z_log_var = inputs\n",
    "        batch = tf.shape(z_mean)[0]\n",
    "        dim = tf.shape(z_mean)[1]\n",
    "        epsilon = tf.keras.backend.random_normal(shape=(batch, dim))\n",
    "        return z_mean + tf.exp(0.5 * z_log_var) * epsilon\n",
    "\n",
    "\n",
    "original_dim = 24\n",
    "intermediate_dim = 64\n",
    "latent_dim = 16\n",
    "\n",
    "# Define encoder model.\n",
    "original_inputs = tf.keras.Input(shape=(original_dim,1), name='encoder_input')\n",
    "input_err = Input(shape=(original_dim,1))\n",
    "x = layers.CuDNNLSTM(intermediate_dim, return_sequences=False)(original_inputs)\n",
    "z_mean = layers.Dense(latent_dim, name='z_mean')(x)\n",
    "z_log_var = layers.Dense(latent_dim, name='z_log_var')(x)\n",
    "z = Sampling()((z_mean, z_log_var))\n",
    "encoder = tf.keras.Model(inputs=original_inputs, outputs=z, name='encoder')\n",
    "\n",
    "# Define decoder model.\n",
    "latent_inputs = tf.keras.Input(shape=(latent_dim,), name='z_sampling')\n",
    "x = layers.RepeatVector(original_dim)(latent_inputs)\n",
    "x = layers.CuDNNLSTM(intermediate_dim, return_sequences=True)(x)\n",
    "outputs = layers.TimeDistributed(layers.Dense(1))(x)\n",
    "decoder = tf.keras.Model(inputs=latent_inputs, outputs=outputs, name='decoder')\n",
    "\n",
    "# Define VAE model.\n",
    "outputs = decoder(z)\n",
    "vae = tf.keras.Model(inputs=[original_inputs, input_err], outputs=outputs, name='vae')\n",
    "\n",
    "vae.load_weights(weights_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_decoder = tf.keras.Model(inputs=vae.get_layer(\"decoder\").input, outputs=vae.get_layer(\"decoder\").output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_samples = 10\n",
    "generated_samples = np.zeros((no_samples, 24))\n",
    "for n_point, point in enumerate(clf.means_[:10]):\n",
    "    prediction = trained_decoder.predict(np.expand_dims(point, axis=0))\n",
    "    generated_samples[n_point, :] = prediction.flatten()\n",
    "\n",
    "    print(n_point)\n",
    "    clear_output(wait=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = (10.0, 5.0)\n",
    "plt.rcParams.update({'font.size': 8})\n",
    "plot_index = 0\n",
    "PC_ind = 4\n",
    "# plt.errorbar(np.linspace(0,512, 128), segments[plot_index], yerr=errors[plot_index], ecolor=\"magenta\")\n",
    "offset = 0\n",
    "for n_lc, lc in enumerate(generated_samples):\n",
    "    if n_lc>0:\n",
    "        offset += np.max(generated_samples[n_lc-1])-np.min(generated_samples[n_lc-1]) +1\n",
    "    plt.plot(np.linspace(0,13100, num=24), lc+offset)\n",
    "# plt.ylabel(\"Normalized count rate\", fontsize=12)\n",
    "plt.xlabel(\"Count rate bins\", fontsize=12)\n",
    "plt.title(\"Series generated from Gaussian mixture means\", fontsize=15)\n",
    "plt.show()\n",
    "if plot_index>0:\n",
    "    plot_index-=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# UMAP the raw histogram data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "segments_dir = '../../../data_GRS1915/468202_len128_s2_4cad_histograms_24bin_0-13k_errorfix.pkl'\n",
    "with open(segments_dir, 'rb') as f:\n",
    "    segments = pickle.load(f)\n",
    "segments = zscore(segments, axis=None).astype(np.float32)  # standardize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "UMAP_mapper = umap.UMAP(verbose=True)#n_neighbors=50, min_dist=0.0\n",
    "UMAP_mapper.fit(segments[:,:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('../../../data_GRS1915/UMAPmapper_raw_histograms_trainedonall.pkl', 'wb') as f:\n",
    "#     pickle.dump(UMAP_mapper, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "umaped_data = UMAP_mapper.transform(segments[:,:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = (abs((np.min(umaped_data[:,0])-0.5) -(np.max(umaped_data[:,0])+0.5)), abs((np.min(umaped_data[:,1])-0.5)- (np.max(umaped_data[:,1])+0.5)))\n",
    "plt.scatter(umaped_data[:,0], umaped_data[:,1], s=0.05)\n",
    "plt.xlim([np.min(umaped_data[:,0])-0.5, np.max(umaped_data[:,0])+0.5])\n",
    "plt.ylim([np.min(umaped_data[:,1])-0.5, np.max(umaped_data[:,1])+0.5])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cluster the raw histograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.mixture import GaussianMixture\n",
    "\n",
    "clf = GaussianMixture(n_components=200, covariance_type='full', verbose=1)\n",
    "clf.fit(segments[:10000,:,0])\n",
    "# aics[n_comps] = clf.aic(segment_encoding[:,0,:])\n",
    "# print(n_comps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "segments.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "custer_labels = clf.predict(segments[:,:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "custer_labels = clf.predict(segments[:,:,0])\n",
    "transformed_means = UMAP_mapper.transform(clf.means_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prop_cycle = plt.rcParams['axes.prop_cycle']\n",
    "colors = prop_cycle.by_key()['color']\n",
    "color_matched_9 = [\"#a8a495\" if x>9 else colors[x] for x in custer_labels]\n",
    "# color_matched_19= [\"#a8a495\" if x<=9 else colors[x-10] for x in custer_labels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prop_cycle = plt.rcParams['axes.prop_cycle']\n",
    "colors = prop_cycle.by_key()['color']\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (abs((np.min(umaped_data[:,0])-0.5) -(np.max(umaped_data[:,0])+0.5)), abs((np.min(umaped_data[:,1])-0.5)- (np.max(umaped_data[:,1])+0.5)))\n",
    "plt.scatter(umaped_data[:,0], umaped_data[:,1], s=0.05, c=color_matched_9)\n",
    "plt.xlim([np.min(umaped_data[:,0])-0.5, np.max(umaped_data[:,0])+0.5])\n",
    "plt.ylim([np.min(umaped_data[:,1])-0.5, np.max(umaped_data[:,1])+0.5])\n",
    "for mean_n, mean in enumerate(transformed_means[:10]):\n",
    "    plt.scatter(mean[0], mean[1], s=300, label=mean_n, edgecolors=\"white\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = (10.0, 5.0)\n",
    "plt.rcParams.update({'font.size': 8})\n",
    "plot_index = 0\n",
    "PC_ind = 4\n",
    "# plt.errorbar(np.linspace(0,512, 128), segments[plot_index], yerr=errors[plot_index], ecolor=\"magenta\")\n",
    "offset = 0\n",
    "for n_lc, lc in enumerate(clf.means_[:10]):\n",
    "    if n_lc>0:\n",
    "        offset += np.max(clf.means_[n_lc-1])-np.min(clf.means_[n_lc-1]) +1\n",
    "    plt.plot(np.linspace(0,13100, num=24), lc+offset)\n",
    "# plt.ylabel(\"Normalized count rate\", fontsize=12)\n",
    "plt.xlabel(\"Count rate bins\", fontsize=12)\n",
    "plt.title(\"Series generated from Gaussian mixture means\", fontsize=15)\n",
    "plt.show()\n",
    "if plot_index>0:\n",
    "    plot_index-=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# UMAP Shape and intensity combination (16 shape latent variables and 24 histogram bin populations) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_dir = '../../../data_GRS1915/468202_len128_s2_4cad_histograms_24bin_0-13k_errorfix.pkl'\n",
    "with open(hist_dir, 'rb') as f:\n",
    "    hists = pickle.load(f)\n",
    "# with open('../../data_GRS1915/94465_len512_s40_errors_errorfix.pkl', 'rb') as f:\n",
    "#     errors = pickle.load(f)\n",
    "    \n",
    "# errors = ((errors)/np.std(segments)).astype(np.float32)\n",
    "hists = zscore(hists[:,:,0], axis=None).astype(np.float32)  # standardize\n",
    "\n",
    "\n",
    "weights_dir = \"../../../model_weights/model_2020-04-29_09-12-23.h5\"\n",
    "segments_dir = '../../../data_GRS1915/468202_len128_s2_4cad_counts_errorfix.pkl'\n",
    "segment_encoding_dir = '../../../data_GRS1915/segment_encoding_{}_segments_{}.pkl'.format(weights_dir.split(\"/\")[-1].split(\".\")[0], segments_dir.split(\"/\")[-1].split(\".\")[0])\n",
    "\n",
    "with open(segment_encoding_dir, 'rb') as f:\n",
    "    segment_encoding = pickle.load(f)\n",
    "    \n",
    "segment_encoding_scaled_means = zscore(segment_encoding[:,0,:], axis=None).astype(np.float32)  # standardize\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAI_data = np.hstack((segment_encoding_scaled_means, hists))\n",
    "SAI_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "UMAP_mapper = umap.UMAP(verbose=True)#n_neighbors=50, min_dist=0.0\n",
    "UMAP_mapper.fit(SAI_data[:50000,:])\n",
    "umaped_data = UMAP_mapper.transform(SAI_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = (abs((np.min(umaped_data[:,0])-0.5) -(np.max(umaped_data[:,0])+0.5)), abs((np.min(umaped_data[:,1])-0.5)- (np.max(umaped_data[:,1])+0.5)))\n",
    "plt.scatter(umaped_data[:,0], umaped_data[:,1], s=0.05)\n",
    "plt.xlim([np.min(umaped_data[:,0])-0.5, np.max(umaped_data[:,0])+0.5])\n",
    "plt.ylim([np.min(umaped_data[:,1])-0.5, np.max(umaped_data[:,1])+0.5])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cluster the shape and intensity features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.mixture import GaussianMixture\n",
    "criteria111_114 = np.zeros((2,4))\n",
    "\n",
    "\n",
    "for ind, n_components in enumerate([111, 112, 113, 114]):\n",
    "\n",
    "    clf = GaussianMixture(n_components=n_components, covariance_type='full', verbose=1)\n",
    "    clf.fit(SAI_data[:50000,:])\n",
    "    \n",
    "#     with open('../../../data_GRS1915/SAI_GM_components{}_fitto50k_v2.pkl'.format(n_components), 'wb') as f:\n",
    "#         pickle.dump(clf, f)\n",
    "    \n",
    "    criteria111_114[0,ind] = clf.aic(SAI_data)\n",
    "    criteria111_114[1,ind] = clf.bic(SAI_data)\n",
    "    print(n_components, criteria111_114[:,ind])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] =[20,10]\n",
    "\n",
    "plt.plot([5,10,20,30,50,75,100,110,120,130,140,150,160,170,180,190,200,225,250,300,500], criteria4[0,:], label=\"AIC\") #125\n",
    "plt.plot([5,10,20,30,50,75,100,110,120,130,140,150,160,170,180,190,200,225,250,300,500], criteria4[1,:], label=\"BIC\") #125\n",
    "plt.title(\"Information criteria for GMM trained on 50k samples of shape and intensity data\", pad=20, fontsize=24)\n",
    "plt.ylabel(\"Information criterion value\", fontsize=24)\n",
    "plt.xlabel(\"Number of Gaussian components\", fontsize=24)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] =[20,10]\n",
    "\n",
    "plt.plot([111, 112, 113, 114], criteria111_114[0,:], label=\"AIC\") #125\n",
    "plt.plot([111, 112, 113, 114], criteria111_114[1,:], label=\"BIC\") #125\n",
    "plt.title(\"Information criteria for GMM trained on 50k samples of shape and intensity data\", pad=20, fontsize=24)\n",
    "plt.ylabel(\"Information criterion value\", fontsize=24)\n",
    "plt.xlabel(\"Number of Gaussian components\", fontsize=24)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CLassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_belloni = open('../../../data_GRS1915/1915Belloniclass_updated.dat')\n",
    "lines = clean_belloni.readlines()\n",
    "states = lines[0].split()\n",
    "belloni_clean = {}\n",
    "for h,l in zip(states, lines[1:]):\n",
    "    belloni_clean[h] = l.split()\n",
    "    #state: obsID1, obsID2...\n",
    "ob_state = {}\n",
    "for state, obs in belloni_clean.items():\n",
    "    if state == \"chi1\" or state == \"chi2\" or state == \"chi3\" or state == \"chi4\": state = \"chi\"\n",
    "    for ob in obs:\n",
    "        ob_state[ob] = state\n",
    "\n",
    "\n",
    "\n",
    "# inv_ob_state = {v: k for k, v in ob_state.items()}\n",
    "\n",
    "inv_ob_state = {}\n",
    "for k, v in ob_state.items():\n",
    "    inv_ob_state[v] = inv_ob_state.get(v, [])\n",
    "    inv_ob_state[v].append(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../../../data_GRS1915/468202_len128_s2_4cad_counts_errorfix.pkl', 'rb') as f:\n",
    "    segments = pickle.load(f)\n",
    "with open('../../../data_GRS1915/468202_len128_s2_4cad_ids_errorfix.pkl', 'rb') as f:\n",
    "    seg_ids = pickle.load(f)\n",
    "    \n",
    "xxx = [seg.split(\"_\")[0] for seg in seg_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = np.array([\"alpha\", \"beta\", \"gamma\", \"delta\", \"theta\", \"kappa\", \"lambda\", \"mu\", \"nu\", \"rho\", \"phi\", \"chi\", \"eta\", \"omega\"])\n",
    "scales = []\n",
    "segment_class = []\n",
    "for ob in xxx:\n",
    "    if ob in ob_state:\n",
    "        segment_class.append(ob_state[ob])\n",
    "        scales.append(5)\n",
    "    else:\n",
    "        segment_class.append(\"Unknown\")\n",
    "        scales.append(0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set = []\n",
    "train_set = []\n",
    "\n",
    "for class_name in [\"alpha\", \"beta\", \"gamma\", \"delta\", \"theta\", \"kappa\", \"lambda\", \"mu\", \"nu\", \"rho\", \"phi\", \"chi\", \"eta\", \"omega\"]:\n",
    "    \n",
    "    class_obs_all = inv_ob_state[class_name]\n",
    "    class_obs = []\n",
    "    for ob in class_obs_all:\n",
    "        if ob in xxx:\n",
    "            class_obs.append(ob)\n",
    "    \n",
    "    test_obs = np.random.choice(class_obs, size=int(np.ceil(len(class_obs)/3)))\n",
    "    \n",
    "    if len(test_obs) == 0:\n",
    "        print(class_name)\n",
    "    \n",
    "    train_obs = []\n",
    "    for ob in class_obs:\n",
    "        if ob not in test_obs:\n",
    "            train_obs.append(ob)\n",
    "    test_set.append(test_obs)\n",
    "    train_set.append(train_obs)\n",
    "#     X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n",
    "    \n",
    "#     test_set = np.random.choice(class_obs, size=)\n",
    "    \n",
    "#     np.where(class_name == \"class_name\")\n",
    "test_set=np.hstack(test_set)\n",
    "train_set=np.hstack(train_set)\n",
    "\n",
    "test_seg_ids=[]\n",
    "train_seg_ids=[]\n",
    "\n",
    "for ind_ob, ob in enumerate(xxx):\n",
    "    if ob in test_set:\n",
    "        test_seg_ids.append(ind_ob)\n",
    "    elif ob in train_set:\n",
    "        train_seg_ids.append(ind_ob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "SVC_clf = SVC().fit(SAI_data[train_seg_ids], np.array(segment_class)[train_seg_ids])\n",
    "SVC_clf_score = SVC_clf.score(SAI_data[test_seg_ids], np.array(segment_class)[test_seg_ids])\n",
    "print(SVC_clf_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "RF_clf = RandomForestClassifier().fit(SAI_data[train_seg_ids], np.array(segment_class)[train_seg_ids])\n",
    "RF_clf_score = RF_clf.score(SAI_data[test_seg_ids], np.array(segment_class)[test_seg_ids])\n",
    "print(RF_clf_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "KNN_clf = KNeighborsClassifier().fit(SAI_data[train_seg_ids], np.array(segment_class)[train_seg_ids])\n",
    "KNN_clf_score = KNN_clf.score(SAI_data[test_seg_ids], np.array(segment_class)[test_seg_ids])\n",
    "print(KNN_clf_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = SVC_clf.predict(SAI_data[test_seg_ids])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for class_name in [\"alpha\", \"beta\", \"gamma\", \"delta\", \"theta\", \"kappa\", \"lambda\", \"mu\", \"nu\", \"rho\", \"phi\", \"chi\", \"eta\", \"omega\"]:\n",
    "\n",
    "for "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(np.array(segment_class)[test_seg_ids], preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Shape data and 8 descriptive stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "# import umap\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from scipy.stats import zscore\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../../../data_GRS1915/468202_len128_s2_4cad_counts_errorfix.pkl', 'rb') as f:\n",
    "    segments_counts = pickle.load(f)\n",
    "with open('../../../data_GRS1915/468202_len128_s2_4cad_errors_errorfix.pkl', 'rb') as f:\n",
    "    segments_errors = pickle.load(f)\n",
    "with open('../../../data_GRS1915/468202_len128_s2_4cad_ids_errorfix.pkl', 'rb') as f:\n",
    "    id_per_seg = pickle.load(f)\n",
    "\n",
    "weights_dir = \"../../../model_weights/model_2020-04-29_09-12-23.h5\"\n",
    "segments_dir = '../../../data_GRS1915/468202_len128_s2_4cad_counts_errorfix.pkl'\n",
    "segment_encoding_dir = '../../../data_GRS1915/segment_encoding_{}_segments_{}.pkl'.format(weights_dir.split(\"/\")[-1].split(\".\")[0], segments_dir.split(\"/\")[-1].split(\".\")[0])\n",
    "\n",
    "with open(segment_encoding_dir, 'rb') as f:\n",
    "    segment_encoding = pickle.load(f)\n",
    "    \n",
    "segment_encoding_scaled_means = zscore(segment_encoding[:,0,:], axis=None).astype(np.float32)  # standardize\n",
    "\n",
    "with open('../../../data_GRS1915/468202_segment_GMM_bic_1-3components_zscored.pkl'.format(segment_encoding_dir.split(\"/\")[-1]), 'rb') as f:\n",
    "    GMM_bics = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "desc_stats = np.zeros((len(segments_counts), 5)) # median, mean, std, skew, kurt, GM1_bic, GM2_bic, GM3_bic\n",
    "desc_stats[:,0] = np.median(segments_counts, axis=1).flatten()\n",
    "desc_stats[:,1] = np.mean(segments_counts, axis=1).flatten()\n",
    "desc_stats[:,2] = np.std(segments_counts, axis=1).flatten()\n",
    "desc_stats[:,3] = stats.skew(segments_counts, axis=1).flatten()\n",
    "desc_stats[:,4] = stats.kurtosis(segments_counts, axis=1).flatten()\n",
    "zscore_desc_stats = zscore(desc_stats, axis=0)\n",
    "\n",
    "# desc_GM = np.hstack((zscore(desc_stats, axis=0), GMM_bics))\n",
    "desc_GM = np.hstack((desc_stats, GMM_bics))\n",
    "\n",
    "\n",
    "shape_desc_GM = np.hstack((segment_encoding_scaled_means, desc_GM))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SVC_clf2 = SVC().fit(shape_desc_GM[train_seg_ids], np.array(segment_class)[train_seg_ids])\n",
    "SVC_clf2_score = SVC_clf2.score(shape_desc_GM[test_seg_ids], np.array(segment_class)[test_seg_ids])\n",
    "print(SVC_clf2_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(np.array(segment_class)[test_seg_ids], preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shape_desc_GM.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "UMAP_mapper = umap.UMAP(verbose=True)#n_neighbors=50, min_dist=0.0, local_connectivity, repulsion_strength, negative_sample_rate\n",
    "UMAP_mapper.fit(shape_desc_GM)\n",
    "    \n",
    "# with open('../../../data_GRS1915/UMAPmapper_shape16latent_8desc_stats_trainedonall.pkl', 'wb') as f:\n",
    "#     pickle.dump(UMAP_mapper, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../../../data_GRS1915/UMAPmapper_shape16latent_8desc_stats_trainedonall.pkl', 'rb') as f:\n",
    "    UMAP_mapper = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "umaped_data = UMAP_mapper.transform(shape_desc_GM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams.update({'font.size': 12})\n",
    "plt.rcParams['figure.figsize'] = (abs((np.min(umaped_data[:,0])-0.5) -(np.max(umaped_data[:,0])+0.5)), abs((np.min(umaped_data[:,1])-0.5)- (np.max(umaped_data[:,1])+0.5)))\n",
    "plt.scatter(umaped_data[:,0], umaped_data[:,1], s=0.05)\n",
    "plt.xlim([np.min(umaped_data[:,0])-0.5, np.max(umaped_data[:,0])+0.5])\n",
    "plt.ylim([np.min(umaped_data[:,1])-0.5, np.max(umaped_data[:,1])+0.5])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_dir = \"../../../model_weights/model_2020-04-29_09-12-23.h5\"\n",
    "segments_dir = '../../../data_GRS1915/468202_len128_s2_4cad_counts_errorfix.pkl'\n",
    "segment_encoding_dir = '../../../data_GRS1915/segment_encoding_{}_segments_{}.pkl'.format(weights_dir.split(\"/\")[-1].split(\".\")[0], segments_dir.split(\"/\")[-1].split(\".\")[0])\n",
    "\n",
    "with open(segment_encoding_dir, 'rb') as f:\n",
    "    segment_encoding = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comaparison of histogram vs descriptive statistics in classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "from sklearn.svm import SVC\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compar_results = np.zeros((2,1))\n",
    "\n",
    "\n",
    "for test_iter in range(1):\n",
    "    test_set = []\n",
    "    train_set = []\n",
    "\n",
    "    for class_name in [\"alpha\", \"beta\", \"gamma\", \"delta\", \"theta\", \"kappa\", \"lambda\", \"mu\", \"nu\", \"rho\", \"phi\", \"chi\", \"eta\", \"omega\"]:\n",
    "\n",
    "        class_obs_all = inv_ob_state[class_name]\n",
    "        class_obs = []\n",
    "        for ob in class_obs_all:\n",
    "            if ob in xxx:\n",
    "                class_obs.append(ob)\n",
    "\n",
    "        test_obs = np.random.choice(class_obs, size=int(np.ceil(len(class_obs)/3)))\n",
    "\n",
    "        if len(test_obs) == 0:\n",
    "            print(class_name)\n",
    "\n",
    "        train_obs = []\n",
    "        for ob in class_obs:\n",
    "            if ob not in test_obs:\n",
    "                train_obs.append(ob)\n",
    "        test_set.append(test_obs)\n",
    "        train_set.append(train_obs)\n",
    "    #     X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n",
    "\n",
    "    #     test_set = np.random.choice(class_obs, size=)\n",
    "\n",
    "    #     np.where(class_name == \"class_name\")\n",
    "    test_set=np.hstack(test_set)\n",
    "    train_set=np.hstack(train_set)\n",
    "\n",
    "    test_seg_ids=[]\n",
    "    train_seg_ids=[]\n",
    "\n",
    "    for ind_ob, ob in enumerate(xxx):\n",
    "        if ob in test_set:\n",
    "            test_seg_ids.append(ind_ob)\n",
    "        elif ob in train_set:\n",
    "            train_seg_ids.append(ind_ob)\n",
    "\n",
    "\n",
    "#     SVC_clf = SVC(gamma=\"auto\").fit(SAI_data[train_seg_ids], np.array(segment_class)[train_seg_ids])\n",
    "#     SVC_clf_score = SVC_clf.score(SAI_data[test_seg_ids], np.array(segment_class)[test_seg_ids])\n",
    "#     compar_results[0, test_iter] = SVC_clf_score\n",
    "\n",
    "\n",
    "#     SVC_clf2 = SVC(gamma=\"auto\").fit(shape_desc_GM[train_seg_ids], np.array(segment_class)[train_seg_ids])\n",
    "#     SVC_clf2_score = SVC_clf2.score(shape_desc_GM[test_seg_ids], np.array(segment_class)[test_seg_ids])\n",
    "#     compar_results[1, test_iter] = SVC_clf2_score\n",
    "#     print(test_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "np.where(np.array(segment_class))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = (15.0, 15.0)\n",
    "plt.rcParams.update({'font.size': 12})\n",
    "# plt.rcParams.update(plt.rcParamsDefault)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using Pearson Correlation\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "data_df  = pd.DataFrame(shape_6moments[:,16:], columns=[\"mean\", \"std\", \"skew\", \"kurt\", \"5th\", \"6th\"])\n",
    "\n",
    "plt.figure(figsize=(12,10))\n",
    "cor = data_df.corr()\n",
    "sns.heatmap(cor, annot=True, cmap=plt.cm.Reds)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sqrt(60)/60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(shape_desc_GM[:,-3], shape_desc_GM[:,-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.scatter(shape_desc_GM[:,-3], shape_desc_GM[:,-2], shape_desc_GM[:,-1])\n",
    "ax.set_xlabel('1comp', fontsize=20, rotation=150)\n",
    "ax.set_ylabel('2comp', fontsize=20)\n",
    "ax.set_zlabel(\"3comp+\", fontsize=20, rotation=60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "UMAP_mapper = umap.UMAP(verbose=True)#n_neighbors=50, min_dist=0.0, local_connectivity, repulsion_strength, negative_sample_rate\n",
    "UMAP_mapper.fit(zscore(shape_desc4, axis=0))\n",
    "    \n",
    "with open('../../../data_GRS1915/UMAPmapper_shape16_4moments_trainedonall.pkl', 'wb') as f:\n",
    "    pickle.dump(UMAP_mapper, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('../../../data_GRS1915/UMAPmapper_shape16latent_8desc_stats_trainedonall.pkl', 'rb') as f:\n",
    "#     UMAP_mapper = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "umaped_data = UMAP_mapper.transform(shape_desc_GM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams.update({'font.size': 12})\n",
    "plt.rcParams['figure.figsize'] = (abs((np.min(umaped_data[:,0])-0.5) -(np.max(umaped_data[:,0])+0.5)), abs((np.min(umaped_data[:,1])-0.5)- (np.max(umaped_data[:,1])+0.5)))\n",
    "plt.scatter(umaped_data[:,0], umaped_data[:,1], s=0.05)\n",
    "plt.xlim([np.min(umaped_data[:,0])-0.5, np.max(umaped_data[:,0])+0.5])\n",
    "plt.ylim([np.min(umaped_data[:,1])-0.5, np.max(umaped_data[:,1])+0.5])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=2)\n",
    "pca.fit(shape_desc_GM[:,-3:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Gaus_PCA = pca.transform(shape_desc_GM[:,-3:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(zscore(Gaus_PCA[:,0]), zscore(Gaus_PCA[:,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shape_desc4_GMPCA2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shape_desc_normGM = shape_desc_GM\n",
    "# shape_desc_normGM[:,-3:] = zscore(shape_desc_GM[:,-3:])\n",
    "\n",
    "# shape_desc4 = np.hstack((shape_desc_GM[:,:16], shape_desc_GM[:,17:-3]))\n",
    "\n",
    "# shape_desc4_GMPCA2 = np.hstack((zscore(shape_desc4), zscore(Gaus_PCA)))\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (10,10)\n",
    "\n",
    "experiment_list = [\n",
    "#     [SAI_data, \"40d: 16 shape and 24 histogram features\"],\n",
    "#     [shape_desc_GM[:, :-2], \"24d: 16 shape features and 8 descriptive statistics\"], ### 22 with the indexing\n",
    "#     [shape_desc_normGM, \"24d with zscored GM BICs\"],\n",
    "#     [shape_desc_GM[:,:-3], \"21d: 16 shape features and 5 descriptive statistics\"],\n",
    "#     [shape_desc4, \"20d: 16 shape features and 4 descriptive statistics\"],\n",
    "#     [zscore(shape_desc4, axis=0), \"20d: 16 shape features and 4 descriptive statistics, standardized\"],\n",
    "    [shape_moments, \"20d: 16 shape features and 4 moments, standardized\"],\n",
    "    [shape_moments, \"20d: 16 shape features and 4 moments, standardized, balanced\"],\n",
    "    [shape_6moments,\"22d: 16 shape features and 6 moments, standardized\" ]\n",
    "#     [shape_desc4_GMPCA2, \"22d: 16 shape, 4 desc. stats and 2 PCs of BIC\"]\n",
    "#     [shape_desc_GM[:,16:], \"8 descriptive statistics\"],\n",
    "#     [SAI_data[:,:16],\"16 shape features\"],\n",
    "#     [SAI_data[:,16:], \"24 histogram features\"]\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "for dataset, title in experiment_list:\n",
    "    SVC_clf = SVC(gamma=\"auto\").fit(dataset[train_seg_ids], np.array(segment_class)[train_seg_ids])\n",
    "    if title[-8:] == \"balanced\":\n",
    "        SVC_clf = SVC(gamma=\"auto\", class_weight=\"balanced\").fit(dataset[train_seg_ids], np.array(segment_class)[train_seg_ids])\n",
    "\n",
    "    preds = SVC_clf.predict(dataset[test_seg_ids])\n",
    "    print(\"{}\".format(title))\n",
    "    print(classification_report(np.array(segment_class)[test_seg_ids], preds))\n",
    "    \n",
    "    \n",
    "    \n",
    "    disp = plot_confusion_matrix(SVC_clf, dataset[test_seg_ids], np.array(segment_class)[test_seg_ids],\n",
    "                             cmap=plt.cm.Blues,\n",
    "                             normalize='true')\n",
    "    disp.ax_.set_title(\"Normalized confusion matrix for {}\".format(title))\n",
    "    plt.show()\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"22d: 16 shape features and 6 moments, standardized\")\n",
    "print(classification_report(np.array(segment_class)[test_seg_ids], preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res1 = np.array([[0.77866676, 0.73311277, 0.78083213, 0.69198966, 0.72730377,\n",
    "        0.78456467, 0.78693837, 0.74564189, 0.71476161, 0.80871077],\n",
    "       [0.79372295, 0.83236835, 0.8409853 , 0.71007752, 0.78622103,\n",
    "        0.83191778, 0.82478704, 0.81088997, 0.78396835, 0.83451901]])\n",
    "\n",
    "res2 = np.array([[0.74569551, 0.72505262, 0.83017905, 0.79743465, 0.79211854,\n",
    "        0.84074226, 0.82077626, 0.80905131, 0.77454224, 0.72485089],\n",
    "       [0.80997834, 0.73009652, 0.85212816, 0.86321069, 0.82912774,\n",
    "        0.85724932, 0.84044608, 0.85375467, 0.82216909, 0.79736129]])\n",
    "\n",
    "res3 = np.array([[0.74975602, 0.76254434, 0.755848  , 0.77953619, 0.7278387 ,\n",
    "        0.7519164 , 0.75126418, 0.77643904, 0.792922  , 0.75279727,\n",
    "        0.72323293, 0.76381347, 0.72353266, 0.77342608, 0.80767374,\n",
    "        0.71281754, 0.76253514, 0.76736953, 0.85906312, 0.79865008,\n",
    "        0.75978855, 0.67200531, 0.83241201, 0.80688174, 0.76097234,\n",
    "        0.71648287, 0.79924554, 0.76408826, 0.73696223, 0.75044426],\n",
    "       [0.78048124, 0.82040156, 0.81124087, 0.83567979, 0.77840704,\n",
    "        0.81282178, 0.80083615, 0.82939717, 0.83040996, 0.81371566,\n",
    "        0.77192067, 0.80505023, 0.77363956, 0.8242435 , 0.83905954,\n",
    "        0.78059637, 0.77894025, 0.79149336, 0.86587611, 0.84104353,\n",
    "        0.77852716, 0.71145472, 0.85347952, 0.83835766, 0.81526955,\n",
    "        0.7709217 , 0.85668724, 0.83868636, 0.79672106, 0.79592084]])\n",
    "\n",
    "res4 = np.array([[0.79191534, 0.76087685, 0.67046855, 0.72114209, 0.76189944,\n",
    "        0.77667217, 0.7330542 , 0.73618209, 0.81619439, 0.75384945,\n",
    "        0.71386682, 0.80245486, 0.79031112, 0.72723923, 0.74515297,\n",
    "        0.72560147, 0.72812745, 0.77691827, 0.67197249, 0.81058563,\n",
    "        0.75088462, 0.75157571, 0.81092084, 0.76765467, 0.70972944,\n",
    "        0.80203729, 0.79307808, 0.758981  , 0.76432133, 0.70835706,\n",
    "        0.78316553, 0.65833264, 0.79104252, 0.80694916, 0.79571977,\n",
    "        0.76556444, 0.76959578, 0.76056712, 0.72495242, 0.79219349,\n",
    "        0.79374297, 0.73663961, 0.71930101, 0.78722207, 0.78305448,\n",
    "        0.75806628, 0.7187707 , 0.71505897, 0.72505291, 0.84645117],\n",
    "       [0.83263955, 0.81029424, 0.74589675, 0.79941799, 0.82089385,\n",
    "        0.77814498, 0.79755573, 0.76267984, 0.85024375, 0.81226108,\n",
    "        0.78982217, 0.85099239, 0.83496526, 0.75015659, 0.80638563,\n",
    "        0.80097514, 0.77870636, 0.83239569, 0.73674135, 0.84918495,\n",
    "        0.79053734, 0.8257413 , 0.84715791, 0.78475336, 0.78390969,\n",
    "        0.85366135, 0.84370507, 0.80469095, 0.82173239, 0.72859475,\n",
    "        0.76969203, 0.74902724, 0.82775535, 0.84975974, 0.83397338,\n",
    "        0.81577569, 0.80737807, 0.81309559, 0.76012779, 0.86020742,\n",
    "        0.82233046, 0.84990381, 0.83276921, 0.84491979, 0.85557161,\n",
    "        0.81210947, 0.76720978, 0.78806509, 0.8159287 , 0.88692258]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res1234 = np.hstack((res1,res2,res3,res4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import iqr\n",
    "\n",
    "fig, axs = plt.subplots(1, 1)\n",
    "\n",
    "plt.hist(res1234[0,:], label=\"hist. data\")\n",
    "plt.hist(res1234[1,:], alpha=0.5, label=\"desc. stats\")\n",
    "fig.text(0.5, 0.94, \"hist.data: mean={:.3f}, std={:.3f}, min={:.3f}, max={:.3f}, IQR={:.3f}\\ndesc.stats: mean={:.3f}, std={:.3f}, min={:.3f}, max={:.3f}, IQR={:.3f}\".format(\n",
    "    np.mean(res1234[0,:]),np.std(res1234[0,:]),np.min(res1234[0,:]),np.max(res1234[0,:]), iqr(res1234[0,:]),\n",
    "np.mean(res1234[1,:]),np.std(res1234[1,:]),np.min(res1234[1,:]),np.max(res1234[1,:]), iqr(res1234[1,:])), ha='center', va=\"top\")\n",
    "plt.ylabel(\"Population\")\n",
    "plt.xlabel(\"Test set accuracy\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cluster the shape and descriptive stats features\n",
    "https://jakevdp.github.io/PythonDataScienceHandbook/05.12-gaussian-mixtures.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distances47.shape# = np.copy(distances)\n",
    "distances100.shape # = np.copy(distances)\n",
    "distances200.shape # = np.copy(distances)\n",
    "distances70.shape # = np.copy(distances)\n",
    "distances300# = np.copy(distances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distances47"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('all_dat_kneigh_dists.pkl', 'wb') as f:\n",
    "#     pickle.dump(distances47, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distances = np.zeros((468202, 7))\n",
    "\n",
    "for n_k, k in enumerate([30, 47, 70, 100, 200, 500, 1000]):\n",
    "#     with open('k-NN_distances_all_data_24D_468202samples_k{}.pkl'.format(k), 'wb') as f:\n",
    "#         pickle.dump(distances, f)\n",
    "        \n",
    "    with open('k-NN_distances_all_data_24D_468202samples_k{}.pkl'.format(k), 'rb') as f:\n",
    "        distances_k = pickle.load(f)\n",
    "        \n",
    "    distances[:,n_k] = distances_k\n",
    "\n",
    "# plt.plot(distances47, label=\"k=47\")\n",
    "# plt.plot(distances70, label=\"k=70\")\n",
    "# plt.plot(distances100, label=\"k=100\")\n",
    "# plt.plot(distances200, label=\"k=200\")\n",
    "# plt.plot(distances, label=\"k=300\")\n",
    "# plt.ylim((0,5))\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distances0_30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = (20.0, 20.0)\n",
    "plt.rcParams.update({'font.size': 12})\n",
    "\n",
    "plt.plot(distances0_30[:,5], label=\"k=5\")\n",
    "plt.plot(distances0_30[:,10], label=\"k=10\")\n",
    "plt.plot(distances0_30[:,20], label=\"k=20\")\n",
    "\n",
    "plt.plot(distances[:, 0], label=\"k=30\")\n",
    "plt.plot(distances[:, 1], label=\"k=47\")\n",
    "plt.plot(distances[:, 2], label=\"k=70\")\n",
    "plt.plot(distances[:, 3], label=\"k=100\")\n",
    "plt.plot(distances[:, 4], label=\"k=200\")\n",
    "plt.plot(distances[:, 5], label=\"k=500\")\n",
    "plt.plot(distances[:, 6], label=\"k=1000\")\n",
    "\n",
    "plt.title(\"Sorted k-distance for 468202 samples of 24d data\")\n",
    "plt.xlabel(\"k-order\")\n",
    "plt.ylabel(\"distance (Euclidean)\")\n",
    "plt.ylim((0,3))\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import NearestNeighbors\n",
    "# X = shape_desc_GM[:50000,:]\n",
    "\n",
    "for k in [30]:\n",
    "    neigh = NearestNeighbors(n_neighbors=k, n_jobs=7, metric='euclidean')\n",
    "    nbrs = neigh.fit(shape_desc_GM)\n",
    "    distances, indices = nbrs.kneighbors(shape_desc_GM)\n",
    "    distances = np.sort(distances, axis=0)\n",
    "    #distances = distances[:,-1]\n",
    "    with open('k-NN_distances_all_data_24D_468202samples_k0to{}.pkl'.format(k), 'wb') as f:\n",
    "        pickle.dump(distances, f)\n",
    "    print(k)\n",
    "#     clear_output(wait=True)\n",
    "# plt.plot(distances)\n",
    "# plt.ylim((0,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('k-NN_distances_all_data_24D_468202samples_k0to30.pkl', 'rb') as f:\n",
    "    distances0_30 = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shape_desc_GM.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('shape_desc_stat_24D_GMM_aic_bic_5-500comps_100k.pkl', 'wb') as f:\n",
    "    pickle.dump(criteria5_500, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('shape_desc_stat_24D_GMM_aic_bic_127_130_135comps_alldata.pkl'.format(segment_encoding_dir.split(\"/\")[-1]), 'rb') as f:\n",
    "    criteria127_130 = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.mixture import GaussianMixture\n",
    "\n",
    "comp_no_list = [151, 152, 153]\n",
    "\n",
    "criteria = np.zeros((2,len(comp_no_list)))\n",
    "\n",
    "\n",
    "for ind, n_components in enumerate(comp_no_list):\n",
    "\n",
    "    clf = GaussianMixture(n_components=n_components, covariance_type='full', verbose=1)\n",
    "    clf.fit(shape_desc_GM)\n",
    "    \n",
    "    criteria[0,ind] = clf.aic(shape_desc_GM)\n",
    "    criteria[1,ind] = clf.bic(shape_desc_GM)\n",
    "    print(n_components, criteria[:,ind])\n",
    "    \n",
    "    with open('../../../data_GRS1915/shape16_stats8_GM_components{}_alldata.pkl'.format(n_components), 'wb') as f:\n",
    "        pickle.dump(clf, f)\n",
    "    \n",
    "#     with open('shape_desc_stat_24D_GMM_aic_bic_range145-160-2comps_50K.pkl', 'wb') as f:\n",
    "#         pickle.dump(criteria, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(range(145, 160, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] =[20,10]\n",
    "\n",
    "plt.plot([135,140,145,150,155,160,165]+list(range(170, 240, 5)), np.hstack((criteria135_165[:,0],criteria[0,:])), label=\"AIC\") #125\n",
    "plt.plot([135,140,145,150,155,160,165]+list(range(170, 240, 5)), np.hstack((criteria135_165[:,1],criteria[1,:])), label=\"BIC\") #125\n",
    "plt.title(\"Information criteria for GMM trained on 50k samples of shape data and descriptive statistics\", pad=20, fontsize=24)\n",
    "plt.ylabel(\"Information criterion value\", fontsize=24)\n",
    "plt.xlabel(\"Number of Gaussian components\", fontsize=24)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criteria135_165[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criteria135_165 = np.array([[-7446053.41718781, -6960953.73107995],\n",
    "[-7426157.10488937, -6923090.35423473],\n",
    "[-7426586.17351449, -6905552.35831308],\n",
    "[-7510014.68999931, -6971013.81025113],\n",
    "[-7512876.35162247, -6955908.40732751],\n",
    "[-7400180.39300913, -6825245.38416739],\n",
    "[-7496718.9980214,  -6903816.92463288]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all data\n",
    "115 [-8073146.27283041 -7659914.84490966]\n",
    "120 [-8140824.45935922 -7709625.96689169]\n",
    "125 [-8192828.92915021 -7743663.3721359 ]\n",
    "127 [-8249915.1942408  -7793562.81140779]\n",
    "130 [-8263993.33624558, -7796860.7146845 ]\n",
    "135 [-8328600.2442305 , -7843500.55812264]\n",
    "140 [-8412351.18572903 -7909284.43507439]\n",
    "151 [-8527567.38468881 -7984973.09203127] ###\n",
    "152 [-8528492.50677821 -7982304.80121131]\n",
    "\n",
    "# 50k samples\n",
    "135 [-7446053.41718781 -6960953.73107995]\n",
    "140 [-7426157.10488937 -6923090.35423473]\n",
    "145 [-7426586.17351449 -6905552.35831308]\n",
    "150 [-7510014.68999931 -6971013.81025113]###\n",
    "155 [-7512876.35162247 -6955908.40732751]\n",
    "160 [-7400180.39300913 -6825245.38416739]\n",
    "165 [-7496718.9980214  -6903816.92463288]\n",
    "\n",
    "\n",
    "145 [-7506767.23430007 -6985733.41909866]\n",
    "147 [-7505868.92420308 -6977648.28318296]\n",
    "149 [-7487498.22538117 -6952090.75854234]\n",
    "151 [-7509885.08825046 -6967290.79559292]\n",
    "153 [-7551318.88243436 -7001537.7639581 ] ###\n",
    "155 [-7490368.82190454 -6933400.87760957]\n",
    "157 [-7487697.41874471 -6923542.64863104]\n",
    "159 [-7508364.59994809 -6937023.00401571]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.where(np.array(xxx) == \"10408-01-22-00\") #64, 74,4,11,113,44,38,85"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist_mat = np.zeros((128,128))\n",
    "for n1, seg1_ind in enumerate(np.where(np.array(xxx) == \"10408-01-22-00\")[0]):\n",
    "    for n2, seg2_ind in enumerate(np.random.randint(len(xxx), size=128)):\n",
    "        dist_mat[n1,n2] = np.linalg.norm(shape_desc_GM[seg1_ind]-shape_desc_GM[seg2_ind])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(np.where(np.array(xxx) == \"10408-01-22-00\")[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.min(dist_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(dist_mat.flatten())\n",
    "# plt.title(\"Distribution of euclidean distances between 128 segments of light curve 10408-01-22-00 (chi) in 24D latent space (16shape 8stats). Both triags and diagonal of distance matrix\")\n",
    "plt.title(\"Distribution of euclidean distances between 128 segments of light cur ve 10408-01-22-00 (chi) in 24D latent space (16shape 8stats) and random segments. Both triags and diagonal of distance matrix\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "dbscan_grid = []\n",
    "\n",
    "for n_eps, eps in enumerate(np.arange(1.5, 2.2, 0.05)):\n",
    "    for n_min, min_samples in enumerate(range(40, 120, 5)):\n",
    "\n",
    "        clf = DBSCAN(eps=eps,min_samples=min_samples, n_jobs=7)\n",
    "        clf.fit(shape_desc_GM[:50000,:])\n",
    "        \n",
    "        dbscan_grid.append((np.unique(clf.labels_, return_counts=1), eps, min_samples))\n",
    "        \n",
    "        print(n_eps, n_min)\n",
    "        clear_output(wait=True)\n",
    "        \n",
    "# with open('dbscan_grid_search_50k.pkl', 'wb') as f:\n",
    "#     pickle.dump(dbscan_grid, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('dbscan_grid_search_50k_eps15-22_min40-120.pkl', 'wb') as f:\n",
    "    pickle.dump(dbscan_grid, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import OPTICS\n",
    "dbscan_grid_op = []\n",
    "\n",
    "for n_min, min_samples in enumerate(range(47, 120, 5)):\n",
    "\n",
    "    clf = OPTICS(max_eps=3,min_samples=min_samples, metric=\"euclidean\", n_jobs=7)\n",
    "    clf.fit(shape_desc_GM[:50000,:])\n",
    "\n",
    "    dbscan_grid_op.append((np.unique(clf.labels_, return_counts=1), eps, min_samples))\n",
    "\n",
    "    print(n_eps, n_min)\n",
    "    clear_output(wait=True)\n",
    "\n",
    "# with open('dbscan_grid_search_50k.pkl', 'wb') as f:\n",
    "#     pickle.dump(dbscan_grid, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = OPTICS(max_eps=2,min_samples=500, metric=\"euclidean\", n_jobs=7)\n",
    "clf.fit(shape_desc_GM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(clf.labels_, return_counts=1)# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(clf.labels_, return_counts=1)# 2,300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(array([-1,  0,  1,  2]), array([11890, 32158,  5569,   383])) 1.8, 90\n",
    "(array([-1,  0,  1,  2,  3]), array([11351, 32433,  5619,   436,   161])) 1.8, 80\n",
    "(array([-1,  0,  1,  2,  3]), array([13242, 29944,  5302,  1215,   297])) 1.7 80"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = DBSCAN(eps=1.7,min_samples=3, n_jobs=7)\n",
    "clf.fit(shape_desc_GM[:50000,:])\n",
    "\n",
    "np.unique(clf.labels_, return_counts=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../../../data_GRS1915/UMAPmapper_shape16latent_8desc_stats_trainedonall.pkl', 'rb') as f:\n",
    "    UMAP_mapper = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "umaped_data = UMAP_mapper.transform(shape_desc_GM[:50000,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prop_cycle = plt.rcParams['axes.prop_cycle']\n",
    "colors = prop_cycle.by_key()['color']\n",
    "color_matched = [\"#a8a495\" if x==-1 else colors[x] for x in clf.labels_]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams.update({'font.size': 12})\n",
    "plt.rcParams['figure.figsize'] = (abs((np.min(umaped_data[:,0])-0.5) -(np.max(umaped_data[:,0])+0.5)), abs((np.min(umaped_data[:,1])-0.5)- (np.max(umaped_data[:,1])+0.5)))\n",
    "plt.scatter(umaped_data[:,0], umaped_data[:,1], s=1, c=color_matched)\n",
    "plt.xlim([np.min(umaped_data[:,0])-0.5, np.max(umaped_data[:,0])+0.5])\n",
    "plt.ylim([np.min(umaped_data[:,1])-0.5, np.max(umaped_data[:,1])+0.5])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(clf.labels_, return_counts=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "for record in dbscan_grid:\n",
    "#     print(record[0][0][1][0])\n",
    "    if record[0][1][0] < 1000:# and record[0][1][1] < 40000:\n",
    "        count +=1\n",
    "#         print(record)\n",
    "#         break\n",
    "print(count)\n",
    "        #print(record)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "dbscan_grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(clf.labels_, return_counts=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(np.unique(xxx, return_counts=1)[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[5,10,20,30,50,75,100,120,140,160,180,200,225,250,300,500][np.argmin(criteria5_500[1,:])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] =[20,10]\n",
    "\n",
    "plt.plot([130,140,150,160,170,180,190,200], criteria130_200[0,:], label=\"AIC\") #125\n",
    "plt.plot([130,140,150,160,170,180,190,200], criteria130_200[1,:], label=\"BIC\") #125\n",
    "plt.title(\"Information criteria for GMM trained on 50k samples of shape and descriptive statistics data\", pad=20, fontsize=24)\n",
    "plt.ylabel(\"Information criterion value\", fontsize=24)\n",
    "plt.xlabel(\"Number of Gaussian components\", fontsize=24)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis of 20d space; 16 shape features and 4 moments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_dir = '../../../data_GRS1915/468202_len128_s2_4cad_histograms_24bin_0-13k_errorfix.pkl'\n",
    "with open(hist_dir, 'rb') as f:\n",
    "    hists = pickle.load(f)\n",
    "# with open('../../data_GRS1915/94465_len512_s40_errors_errorfix.pkl', 'rb') as f:\n",
    "#     errors = pickle.load(f)\n",
    "    \n",
    "# errors = ((errors)/np.std(segments)).astype(np.float32)\n",
    "hists = zscore(hists[:,:,0], axis=None).astype(np.float32)  # standardize\n",
    "\n",
    "\n",
    "weights_dir = \"../../../model_weights/model_2020-04-29_09-12-23.h5\"\n",
    "segments_dir = '../../../data_GRS1915/468202_len128_s2_4cad_counts_errorfix.pkl'\n",
    "segment_encoding_dir = '../../../data_GRS1915/segment_encoding_{}_segments_{}.pkl'.format(weights_dir.split(\"/\")[-1].split(\".\")[0], segments_dir.split(\"/\")[-1].split(\".\")[0])\n",
    "\n",
    "with open(segment_encoding_dir, 'rb') as f:\n",
    "    segment_encoding = pickle.load(f)\n",
    "    \n",
    "segment_encoding_scaled_means = zscore(segment_encoding[:,0,:], axis=None).astype(np.float32)  # standardize\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAI_data = np.hstack((segment_encoding_scaled_means, hists))\n",
    "SAI_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('{}/468202_len128_s2_4cad_counts_errorfix.pkl'.format(data_dir), 'rb') as f:\n",
    "    segments_counts = pickle.load(f)\n",
    "# with open('../../../data_GRS1915/468202_len128_s2_4cad_errors_errorfix.pkl', 'rb') as f:\n",
    "#     segments_errors = pickle.load(f)\n",
    "# with open('../../../data_GRS1915/468202_len128_s2_4cad_ids_errorfix.pkl', 'rb') as f:\n",
    "#     id_per_seg = pickle.load(f)\n",
    "\n",
    "weights_dir = \"../../../model_weights/model_2020-04-29_09-12-23.h5\"\n",
    "segments_dir = '../../../data_GRS1915/468202_len128_s2_4cad_counts_errorfix.pkl'\n",
    "segment_encoding_dir = '{}/segment_encoding_{}_segments_{}.pkl'.format(data_dir, weights_dir.split(\"/\")[-1].split(\".\")[0], segments_dir.split(\"/\")[-1].split(\".\")[0])\n",
    "\n",
    "with open(segment_encoding_dir, 'rb') as f:\n",
    "    segment_encoding = pickle.load(f)\n",
    "    \n",
    "segment_encoding_scaled_means = zscore(segment_encoding[:,0,:], axis=0).astype(np.float32)  # standardize per feature\n",
    "\n",
    "# with open('../../../data_GRS1915/468202_segment_GMM_bic_1-3components_zscored.pkl'.format(segment_encoding_dir.split(\"/\")[-1]), 'rb') as f:\n",
    "#     GMM_bics = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "desc_stats = np.zeros((len(segments_counts), 4)) #mean, std, skew, kurt\n",
    "# desc_stats[:,0] = np.median(segments_counts, axis=1).flatten()\n",
    "desc_stats[:,0] = np.mean(segments_counts, axis=1).flatten()\n",
    "desc_stats[:,1] = np.std(segments_counts, axis=1).flatten()\n",
    "desc_stats[:,2] = stats.skew(segments_counts, axis=1).flatten()\n",
    "desc_stats[:,3] = stats.kurtosis(segments_counts, axis=1).flatten()\n",
    "zscore_desc_stats = zscore(desc_stats, axis=0)\n",
    "\n",
    "# desc_GM = np.hstack((zscore(desc_stats, axis=0), GMM_bics))\n",
    "\n",
    "shape_moments = np.hstack((segment_encoding_scaled_means, zscore_desc_stats)) # every column is standardized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "desc_stats = np.zeros((len(segments_counts), 6)) #mean, std, skew, kurt\n",
    "# desc_stats[:,0] = np.median(segments_counts, axis=1).flatten()\n",
    "desc_stats[:,0] = np.mean(segments_counts, axis=1).flatten()\n",
    "desc_stats[:,1] = np.std(segments_counts, axis=1).flatten()\n",
    "desc_stats[:,2] = np.mean(((np.squeeze(segments_counts)-np.mean(segments_counts, axis=1))/np.std(segments_counts, axis=1))**3, axis=1) #stats.skew(segments_counts, axis=1).flatten()\n",
    "desc_stats[:,3] = np.mean(((np.squeeze(segments_counts)-np.mean(segments_counts, axis=1))/np.std(segments_counts, axis=1))**4, axis=1)#stats.kurtosis(segments_counts, axis=1).flatten()\n",
    "desc_stats[:,4] = np.mean(((np.squeeze(segments_counts)-np.mean(segments_counts, axis=1))/np.std(segments_counts, axis=1))**5, axis=1)\n",
    "desc_stats[:,5] = np.mean(((np.squeeze(segments_counts)-np.mean(segments_counts, axis=1))/np.std(segments_counts, axis=1))**6, axis=1)\n",
    "zscore_desc_stats = zscore(desc_stats, axis=0)\n",
    "\n",
    "# desc_GM = np.hstack((zscore(desc_stats, axis=0), GMM_bics))\n",
    "\n",
    "shape_6moments = np.hstack((segment_encoding_scaled_means, zscore_desc_stats)) # every column is standardized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "desc_stats = np.zeros((len(segments_counts), 4)) #mean, std, skew, kurt\n",
    "# desc_stats[:,0] = np.median(segments_counts, axis=1).flatten()\n",
    "desc_stats[:,0] = np.mean(segments_counts, axis=1).flatten()\n",
    "desc_stats[:,1] = np.var(segments_counts, axis=1).flatten()\n",
    "desc_stats[:,2] = stats.skew(segments_counts, axis=1).flatten()\n",
    "desc_stats[:,3] = stats.kurtosis(segments_counts, axis=1).flatten()\n",
    "zscore_desc_stats = zscore(desc_stats, axis=0)\n",
    "\n",
    "# desc_GM = np.hstack((zscore(desc_stats, axis=0), GMM_bics))\n",
    "\n",
    "shape_moments_var = np.hstack((segment_encoding_scaled_means, zscore_desc_stats)) # every column is standardized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(segments_counts.squeeze()-np.mean(segments_counts, axis=1))/np.std(segments_counts, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(segments_counts, axis=1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats.kurtosis(X, fisher=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = segments_counts[0]\n",
    "stand_counts = (segments_counts[0]-np.mean(segments_counts[0]))/np.var(segments_counts[0])\n",
    "\n",
    "stats.moment(stand_counts, moment=4)\n",
    "np.mean(((X-np.mean(X))/np.var(X))**4)\n",
    "# these two give the same value\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "(stats.moment(X, moment=4)/(np.var(X)**2))-3 # gives the same value as Fisher kurtosis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stand_counts = (segments_counts[0]-np.mean(segments_counts[0]))/np.var(segments_counts[0])\n",
    "\n",
    "stats.moment(X, moment=3)/(np.var(X)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(((X-np.mean(X))/np.var(X))**4)#/(np.var(X)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(((X-np.mean(X))/np.std(X))**4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats.skew(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats.moment(X, moment=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(stats.moment(X, moment=4)/(np.var(X)**2))-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "UMAP_mapper = umap.UMAP(verbose=True)#n_neighbors=50, min_dist=0.0, local_connectivity, repulsion_strength, negative_sample_rate\n",
    "UMAP_mapper.fit(shape_moments)\n",
    "    \n",
    "# with open('../../../data_GRS1915/UMAPmapper_shape16_4moments_trainedonall.pkl', 'wb') as f:\n",
    "#     pickle.dump(UMAP_mapper, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('../../../data_GRS1915/UMAPmapper_20d_shape16_moments4_trainedonall_468202.pkl', 'wb') as f:\n",
    "#     pickle.dump(UMAP_mapper, f)\n",
    "\n",
    "with open('../../../data_GRS1915/UMAPmapper_moments4standardized_trainedonall_468202.pkl', 'rb') as f:\n",
    "    UMAP_mapper = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "umaped_data = UMAP_mapper.transform(shape_moments[:,16:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams.update({'font.size': 12})\n",
    "plt.rcParams['figure.figsize'] = (abs((np.min(umaped_data[:,0])-0.5) -(np.max(umaped_data[:,0])+0.5)), abs((np.min(umaped_data[:,1])-0.5)- (np.max(umaped_data[:,1])+0.5)))\n",
    "plt.scatter(umaped_data[:,0], umaped_data[:,1], s=0.05)\n",
    "plt.xlim([np.min(umaped_data[:,0])-0.5, np.max(umaped_data[:,0])+0.5])\n",
    "plt.ylim([np.min(umaped_data[:,1])-0.5, np.max(umaped_data[:,1])+0.5])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load observation classifications from Huppenkothen 2017\n",
    "# %matplotlib inline\n",
    "\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "clean_belloni = open('../../../data_GRS1915/1915Belloniclass_updated.dat')\n",
    "lines = clean_belloni.readlines()\n",
    "states = lines[0].split()\n",
    "belloni_clean = {}\n",
    "for h,l in zip(states, lines[1:]):\n",
    "    belloni_clean[h] = l.split()\n",
    "    #state: obsID1, obsID2...\n",
    "ob_state = {}\n",
    "for state, obs in belloni_clean.items():\n",
    "    if state == \"chi1\" or state == \"chi2\" or state == \"chi3\" or state == \"chi4\": state = \"chi\"\n",
    "    for ob in obs:\n",
    "        ob_state[ob] = state\n",
    "        \n",
    "# load segmented light curves\n",
    "\n",
    "import pickle\n",
    "with open('../../../data_GRS1915/468202_len128_s2_4cad_counts_errorfix.pkl', 'rb') as f:\n",
    "    segments = pickle.load(f)\n",
    "with open('../../../data_GRS1915/468202_len128_s2_4cad_ids_errorfix.pkl', 'rb') as f:\n",
    "    seg_ids = pickle.load(f)\n",
    "\n",
    "# # HF QPO observation ids\n",
    "# paper_obIDs = np.loadtxt(\"../../../data_GRS1915/Belloni_Altamirano_obsIDs.txt\", dtype=str)\n",
    "\n",
    "# qpo_colours = []\n",
    "\n",
    "# for seg_id in seg_ids:\n",
    "#     if seg_id.split(\"_\")[0] in paper_obIDs:\n",
    "#         qpo_colours.append(\"red\")\n",
    "#     else:\n",
    "#         qpo_colours.append(\"grey\")\n",
    "        \n",
    "# qpo_labels = []\n",
    "\n",
    "# for seg_id in seg_ids:\n",
    "#     if seg_id.split(\"_\")[0] in paper_obIDs:\n",
    "#         qpo_labels.append(\"QPO\")\n",
    "#     else:\n",
    "#         qpo_labels.append(\"other\")\n",
    "        \n",
    "        \n",
    "# qpo_scales = []\n",
    "\n",
    "# for seg_id in seg_ids:\n",
    "#     if seg_id.split(\"_\")[0] in paper_obIDs:\n",
    "#         qpo_scales.append(\"QPO\")\n",
    "#     else:\n",
    "#         qpo_scales.append(\"other\")\n",
    "        \n",
    "        \n",
    "xxx = [seg.split(\"_\")[0] for seg in seg_ids]\n",
    "\n",
    "classes = np.array([\"alpha\", \"beta\", \"gamma\", \"delta\", \"theta\", \"kappa\", \"lambda\", \"mu\", \"nu\", \"rho\", \"phi\", \"chi\", \"eta\", \"omega\"])\n",
    "class_colour = []\n",
    "for ob in xxx:\n",
    "    if ob in ob_state:\n",
    "        class_colour.append(np.where(classes == ob_state[ob])[0][0])\n",
    "    else:\n",
    "        class_colour.append(15)\n",
    "        \n",
    "classes = np.array([\"alpha\", \"beta\", \"gamma\", \"delta\", \"theta\", \"kappa\", \"lambda\", \"mu\", \"nu\", \"rho\", \"phi\", \"chi\", \"eta\", \"omega\"])\n",
    "scales = []\n",
    "segment_class = []\n",
    "for ob in xxx:\n",
    "    if ob in ob_state:\n",
    "        segment_class.append(ob_state[ob])\n",
    "        scales.append(5)\n",
    "    else:\n",
    "        segment_class.append(\"Unknown\")\n",
    "        scales.append(0.1)\n",
    "        \n",
    "        \n",
    "from matplotlib import cm\n",
    "cm.get_cmap(plt.get_cmap(\"Set1\"))\n",
    "\n",
    "\n",
    "colours = ['#ffd8b1', '#000075', '#808080', '#e6194b', '#3cb44b', '#ffe119', '#4363d8', '#f58231', '#911eb4', '#46f0f0', '#f032e6', '#bcf60c', '#fabebe', '#008080', '#e6beff', '#9a6324', '#fffac8', '#800000', '#aaffc3', '#808000', '#000000']\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "# sns.set_style(\"white\")\n",
    "plt.rcParams['figure.figsize'] = (30.0, 30.0)\n",
    "plt.rcParams.update({'font.size': 0})\n",
    "\n",
    "embeddings_lap = umaped_data\n",
    "\n",
    "# fig, ax = plt.subplots()\n",
    "\n",
    "fig, axs = plt.subplots(4, 4)\n",
    "axs = axs.flatten()\n",
    "\n",
    "for plot_class_ind, plot_class in enumerate(classes):\n",
    "    class_indices = np.where(np.array(segment_class) == \"Unknown\")[0]\n",
    "    class_data = embeddings_lap[class_indices]\n",
    "    axs[plot_class_ind].scatter(class_data[:,0], class_data[:,1], s = 0.2, c=\"grey\", label=\"Unknown\")\n",
    "\n",
    "    class_indices = np.where(np.array(segment_class) == plot_class)[0]\n",
    "    class_data = embeddings_lap[class_indices]\n",
    "    \n",
    "    axs[plot_class_ind].scatter(class_data[:,0], class_data[:,1], s = 25, c='red', label=plot_class)\n",
    "    \n",
    "# plt.legend()\n",
    "    axs[plot_class_ind].set_title(\"{}\".format(plot_class), fontsize=42)\n",
    "axs.reshape((4,4))\n",
    "# plt.savefig(\"classes_separate.png\")\n",
    "\n",
    "# plt.savefig(\"UMAP_embedding_separate_classes_model_2020-02-09_10-36-06.png\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# redint = np.where(np.array(qpo_colours) == \"red\")\n",
    "# greyint= np.where(np.array(qpo_colours) != \"red\")\n",
    "# plt.scatter(embeddings_lap[:,0][greyint], embeddings_lap[:,1][greyint], s=1, c=\"grey\", label= \"other\")\n",
    "# plt.scatter(embeddings_lap[:,0][redint], embeddings_lap[:,1][redint], s=1, c=\"red\", label= \"HF QPO\")\n",
    "# plt.title(\"UMAP embedding of the encoded GRS1915 segments, neighbors=50, min_dist=0.0, components=2\", fontsize=12)\n",
    "# plt.legend()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "UMAP_mapper = umap.UMAP(verbose=True)#n_neighbors=50, min_dist=0.0, local_connectivity, repulsion_strength, negative_sample_rate\n",
    "UMAP_mapper.fit(histogram_data[:50000,:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "umaped_data = UMAP_mapper.transform(histogram_data[:,:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams.update({'font.size': 12})\n",
    "plt.rcParams['figure.figsize'] = (abs((np.min(umaped_data[:,0])-0.5) -(np.max(umaped_data[:,0])+0.5)), abs((np.min(umaped_data[:,1])-0.5)- (np.max(umaped_data[:,1])+0.5)))\n",
    "plt.scatter(umaped_data[:,0], umaped_data[:,1], s=0.05)\n",
    "plt.xlim([np.min(umaped_data[:,0])-0.5, np.max(umaped_data[:,0])+0.5])\n",
    "plt.ylim([np.min(umaped_data[:,1])-0.5, np.max(umaped_data[:,1])+0.5])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "histograms_dir = '../../../data_GRS1915/468202_len128_s2_4cad_histograms_24bin_0-13k_errorfix.pkl'\n",
    "with open(histograms_dir, 'rb') as f:\n",
    "    histogram_data = pickle.load(f)\n",
    "# with open('../../data_GRS1915/94465_len512_s40_errors_errorfix.pkl', 'rb') as f:\n",
    "#     errors = pickle.load(f)\n",
    "    \n",
    "# errors = ((errors)/np.std(segments)).astype(np.float32)\n",
    "histogram_data = zscore(histogram_data, axis=None).astype(np.float32)  # standardize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('../../../data_GRS1915/UMAPmapper_raw_histograms_trainedonall.pkl', 'wb') as f:\n",
    "#     pickle.dump(UMAP_mapper, f)\n",
    "\n",
    "with open('../../../data_GRS1915/UMAPmapper_20d_shape16_moments4_trainedonall_468202.pkl', 'rb') as f:\n",
    "    UMAP_mapper = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "umaped_data = UMAP_mapper.transform(shape_moments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "histogram_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load observation classifications from Huppenkothen 2017\n",
    "# %matplotlib inline\n",
    "\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "clean_belloni = open('../../../data_GRS1915/1915Belloniclass_updated.dat')\n",
    "lines = clean_belloni.readlines()\n",
    "states = lines[0].split()\n",
    "belloni_clean = {}\n",
    "for h,l in zip(states, lines[1:]):\n",
    "    belloni_clean[h] = l.split()\n",
    "    #state: obsID1, obsID2...\n",
    "ob_state = {}\n",
    "for state, obs in belloni_clean.items():\n",
    "    if state == \"chi1\" or state == \"chi2\" or state == \"chi3\" or state == \"chi4\": state = \"chi\"\n",
    "    for ob in obs:\n",
    "        ob_state[ob] = state\n",
    "        \n",
    "# load segmented light curves\n",
    "\n",
    "import pickle\n",
    "with open('../../../data_GRS1915/468202_len128_s2_4cad_counts_errorfix.pkl', 'rb') as f:\n",
    "    segments = pickle.load(f)\n",
    "with open('../../../data_GRS1915/468202_len128_s2_4cad_ids_errorfix.pkl', 'rb') as f:\n",
    "    seg_ids = pickle.load(f)\n",
    "\n",
    "# # HF QPO observation ids\n",
    "# paper_obIDs = np.loadtxt(\"../../../data_GRS1915/Belloni_Altamirano_obsIDs.txt\", dtype=str)\n",
    "\n",
    "# qpo_colours = []\n",
    "\n",
    "# for seg_id in seg_ids:\n",
    "#     if seg_id.split(\"_\")[0] in paper_obIDs:\n",
    "#         qpo_colours.append(\"red\")\n",
    "#     else:\n",
    "#         qpo_colours.append(\"grey\")\n",
    "        \n",
    "# qpo_labels = []\n",
    "\n",
    "# for seg_id in seg_ids:\n",
    "#     if seg_id.split(\"_\")[0] in paper_obIDs:\n",
    "#         qpo_labels.append(\"QPO\")\n",
    "#     else:\n",
    "#         qpo_labels.append(\"other\")\n",
    "        \n",
    "        \n",
    "# qpo_scales = []\n",
    "\n",
    "# for seg_id in seg_ids:\n",
    "#     if seg_id.split(\"_\")[0] in paper_obIDs:\n",
    "#         qpo_scales.append(\"QPO\")\n",
    "#     else:\n",
    "#         qpo_scales.append(\"other\")\n",
    "        \n",
    "        \n",
    "xxx = [seg.split(\"_\")[0] for seg in seg_ids]\n",
    "\n",
    "classes = np.array([\"alpha\", \"beta\", \"gamma\", \"delta\", \"theta\", \"kappa\", \"lambda\", \"mu\", \"nu\", \"rho\", \"phi\", \"chi\", \"eta\", \"omega\"])\n",
    "class_colour = []\n",
    "for ob in xxx:\n",
    "    if ob in ob_state:\n",
    "        class_colour.append(np.where(classes == ob_state[ob])[0][0])\n",
    "    else:\n",
    "        class_colour.append(15)\n",
    "        \n",
    "classes = np.array([\"alpha\", \"beta\", \"gamma\", \"delta\", \"theta\", \"kappa\", \"lambda\", \"mu\", \"nu\", \"rho\", \"phi\", \"chi\", \"eta\", \"omega\"])\n",
    "scales = []\n",
    "segment_class = []\n",
    "for ob in xxx:\n",
    "    if ob in ob_state:\n",
    "        segment_class.append(ob_state[ob])\n",
    "        scales.append(5)\n",
    "    else:\n",
    "        segment_class.append(\"Unknown\")\n",
    "        scales.append(0.1)\n",
    "        \n",
    "        \n",
    "from matplotlib import cm\n",
    "cm.get_cmap(plt.get_cmap(\"Set1\"))\n",
    "\n",
    "\n",
    "colours = ['#ffd8b1', '#000075', '#808080', '#e6194b', '#3cb44b', '#ffe119', '#4363d8', '#f58231', '#911eb4', '#46f0f0', '#f032e6', '#bcf60c', '#fabebe', '#008080', '#e6beff', '#9a6324', '#fffac8', '#800000', '#aaffc3', '#808000', '#000000']\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "# sns.set_style(\"white\")\n",
    "plt.rcParams['figure.figsize'] = (30.0, 30.0)\n",
    "plt.rcParams.update({'font.size': 0})\n",
    "\n",
    "embeddings_lap = umaped_data\n",
    "\n",
    "# fig, ax = plt.subplots()\n",
    "\n",
    "fig, axs = plt.subplots(4, 4)\n",
    "axs = axs.flatten()\n",
    "\n",
    "for plot_class_ind, plot_class in enumerate(classes):\n",
    "    class_indices = np.where(np.array(segment_class) == \"Unknown\")[0]\n",
    "    class_data = embeddings_lap[class_indices]\n",
    "    axs[plot_class_ind].scatter(class_data[:,0], class_data[:,1], s = 0.2, c=\"grey\", label=\"Unknown\")\n",
    "\n",
    "    class_indices = np.where(np.array(segment_class) == plot_class)[0]\n",
    "    class_data = embeddings_lap[class_indices]\n",
    "    \n",
    "    axs[plot_class_ind].scatter(class_data[:,0], class_data[:,1], s = 25, c='red', label=plot_class)\n",
    "    \n",
    "# plt.legend()\n",
    "    axs[plot_class_ind].set_title(\"{}\".format(plot_class), fontsize=42)\n",
    "axs.reshape((4,4))\n",
    "# plt.savefig(\"classes_separate.png\")\n",
    "\n",
    "# plt.savefig(\"UMAP_embedding_separate_classes_model_2020-02-09_10-36-06.png\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# redint = np.where(np.array(qpo_colours) == \"red\")\n",
    "# greyint= np.where(np.array(qpo_colours) != \"red\")\n",
    "# plt.scatter(embeddings_lap[:,0][greyint], embeddings_lap[:,1][greyint], s=1, c=\"grey\", label= \"other\")\n",
    "# plt.scatter(embeddings_lap[:,0][redint], embeddings_lap[:,1][redint], s=1, c=\"red\", label= \"HF QPO\")\n",
    "# plt.title(\"UMAP embedding of the encoded GRS1915 segments, neighbors=50, min_dist=0.0, components=2\", fontsize=12)\n",
    "# plt.legend()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "histograms_dir = '../../../data_GRS1915/468202_len128_s2_4cad_histograms_24bin_0-13k_errorfix.pkl'\n",
    "with open(histograms_dir, 'rb') as f:\n",
    "    histogram_data = pickle.load(f)\n",
    "# with open('../../data_GRS1915/94465_len512_s40_errors_errorfix.pkl', 'rb') as f:\n",
    "#     errors = pickle.load(f)\n",
    "    \n",
    "# errors = ((errors)/np.std(segments)).astype(np.float32)\n",
    "histogram_data = zscore(histogram_data, axis=0).astype(np.float32)  # standardize\n",
    "\n",
    "UMAP_mapper = umap.UMAP(verbose=True)#n_neighbors=50, min_dist=0.0, local_connectivity, repulsion_strength, negative_sample_rate\n",
    "UMAP_mapper.fit(histogram_data[:50000,:,0])\n",
    "    \n",
    "# with open('../../../data_GRS1915/UMAPmapper_shape16_4moments_trainedonall.pkl', 'wb') as f:\n",
    "#     pickle.dump(UMAP_mapper, f)\n",
    "\n",
    "umaped_data = UMAP_mapper.transform(histogram_data[:,:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import plot_confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import plot_confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.std(shape_desc_GM, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = (15.0, 15.0)\n",
    "plt.rcParams.update({'font.size': 12})\n",
    "# plt.rcParams.update(plt.rcParamsDefault)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# shape_desc_normGM = shape_desc_GM\n",
    "# shape_desc_normGM[:,-3:] = zscore(shape_desc_GM[:,-3:])\n",
    "\n",
    "# shape_desc4 = np.hstack((shape_desc_GM[:,:16], shape_desc_GM[:,17:-3]))\n",
    "\n",
    "# shape_desc4_GMPCA2 = np.hstack((zscore(shape_desc4), zscore(Gaus_PCA)))\n",
    "\n",
    "experiment_list = [\n",
    "#     [SAI_data, \"40d: 16 shape and 24 histogram features\"],\n",
    "#     [zscore(SAI_data, axis=0), \"40d: 16 shape and 24 histogram features, all standardized\"],\n",
    "# #     [shape_desc_GM, \"24d: 16 shape features and 8 descriptive statistics\"],    #bad bad\n",
    "#     [zscore(shape_desc_GM, axis=0), \"24d: 16 shape features and 8 descriptive statistics, all standardized\"], \n",
    "#     [zscore(shape_desc_GM[:,:-3]), \"21d: 16 shape features and 5 descriptive statistics, all standardized\"], # repeat\n",
    "    [shape_moments, \"20d: 16 shape features and 4 descriptive statistics, all standardized\"],\n",
    "    [shape_6moments, \"22d: 16 shape features and 6 moments, all standardized\"],\n",
    "    \n",
    "# #     [shape_desc4_GMPCA2, \"22d: 16 shape, 4 desc. stats and 2 PCs of BIC\"] # inclusion of PCs did not help previously\n",
    "#     [shape_moments[:,16:], \"4 moments, all standardized\"],\n",
    "# #     [shape_desc_GM[:,16:], \"8 descriptive statistics\"], # bad bad\n",
    "#     [zscore(shape_desc_GM[:,16:], axis=0), \"8 descriptive statistics, all standardized\"],\n",
    "#     [SAI_data[:,:16],\"16 shape features\"],\n",
    "#     [zscore(SAI_data[:,:16], axis=0),\"16 shape features, all standardized\"],\n",
    "#     [SAI_data[:,16:], \"24 histogram features\"],\n",
    "#     [zscore(SAI_data[:,16:], axis=0), \"24 histogram features, all standardized\"]\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "for dataset, title in experiment_list:\n",
    "    SVC_clf = SVC(gamma=\"auto\").fit(dataset[train_seg_ids], np.array(segment_class)[train_seg_ids])\n",
    "    preds = SVC_clf.predict(dataset[test_seg_ids])\n",
    "    print(\"{}\".format(title))\n",
    "    print(classification_report(np.array(segment_class)[test_seg_ids], preds))\n",
    "    \n",
    "    \n",
    "    \n",
    "    disp = plot_confusion_matrix(SVC_clf, dataset[test_seg_ids], np.array(segment_class)[test_seg_ids],\n",
    "                             cmap=plt.cm.Blues,\n",
    "                             normalize='true')\n",
    "    disp.ax_.set_title(\"Normalized confusion matrix for {}\".format(title))\n",
    "    plt.show()\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(2, 2)\n",
    "axs = axs.flatten()\n",
    "\n",
    "moment_names = [\"mean\", \"st.dev.\", \"skew\", \"kurt\"]\n",
    "\n",
    "for plot_ind, moment_index in enumerate([-4, -3, -2, -1]):\n",
    "    axs[plot_ind].hist(shape_moments[:,moment_index], density=True)\n",
    "    axs[plot_ind].hist(shape_moments[np.where(np.array(segment_class) == \"phi\")[0]][:,moment_index], alpha=0.8, density=True)\n",
    "    axs[plot_ind].set_title(\"{}\".format(moment_names[plot_ind]), fontsize=42)\n",
    "axs.reshape((2,2))\n",
    "\n",
    "# plt.title(\"Normed distribution of chi class moments\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "shape_moments[np.where(np.array(segment_class) == \"chi\")[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(shape_moments[:,-4])\n",
    "plt.hist(shape_moments[np.where(np.array(segment_class) == \"chi\")[0]][:,-4], alpha=0.8)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(shape_moments[:,-3])\n",
    "plt.hist(shape_moments[np.where(np.array(segment_class) == \"chi\")[0]][:,-3], alpha=0.8)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(shape_moments[:,-2])\n",
    "plt.hist(shape_moments[np.where(np.array(segment_class) == \"chi\")[0]][:,-2], alpha=0.8)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.mixture import GaussianMixture\n",
    "\n",
    "comp_no_list = [5, 10, 30, 50, 70, 90, 100, 110, 120, 150, 170, 200]\n",
    "\n",
    "criteria = np.zeros((2,len(comp_no_list)))\n",
    "\n",
    "\n",
    "for ind, n_components in enumerate(comp_no_list):\n",
    "\n",
    "    clf = GaussianMixture(n_components=n_components, covariance_type='full', verbose=1)\n",
    "    clf.fit(shape_moments[:50000,:])\n",
    "    \n",
    "    criteria[0,ind] = clf.aic(shape_moments)\n",
    "    criteria[1,ind] = clf.bic(shape_moments)\n",
    "    print(n_components, criteria[:,ind])\n",
    "    \n",
    "#     with open('shape16_moments4_criteria_search_50k.pkl', 'wb') as f:\n",
    "#         pickle.dump(clf, f)\n",
    "    \n",
    "#     with open('../../../data_GRS1915/shape16_moments4_components{}_alldata.pkl'.format(n_components), 'wb') as f:\n",
    "#         pickle.dump(clf, f)\n",
    "    \n",
    "    with open('shape16_moments4_criteria_search_50k.pkl', 'wb') as f:\n",
    "        pickle.dump(criteria, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.mixture import GaussianMixture\n",
    "\n",
    "comp_no_list = np.arange(113,116, 1)\n",
    "\n",
    "criteria = np.zeros((2,len(comp_no_list)))\n",
    "\n",
    "\n",
    "for ind, n_components in enumerate(comp_no_list):\n",
    "\n",
    "    clf = GaussianMixture(n_components=n_components, covariance_type='full', verbose=1)\n",
    "    clf.fit(shape_moments[:,:])\n",
    "    \n",
    "    criteria[0,ind] = clf.aic(shape_moments)\n",
    "    criteria[1,ind] = clf.bic(shape_moments)\n",
    "    print(n_components, criteria[:,ind])\n",
    "    \n",
    "#     with open('shape16_moments4_criteria_search_arange110-150-2_50k.pkl', 'wb') as f:\n",
    "#         pickle.dump(clf, f)\n",
    "    \n",
    "    with open('../../../data_GRS1915/shape16_moments4_components{}_alldata.pkl'.format(n_components), 'wb') as f:\n",
    "        pickle.dump(clf, f)\n",
    "    \n",
    "#     with open('shape_desc_stat_24D_GMM_aic_bic_range145-160-2comps_50K.pkl', 'wb') as f:\n",
    "#         pickle.dump(criteria, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import DBSCAN\n",
    "clf_dbscan = DBSCAN(eps=1.5,min_samples=5, n_jobs=7)\n",
    "clf_dbscan.fit(shape_moments)\n",
    "with open('../../../data_GRS1915/DBSCAN_shape16_moments4_eps1-5_min_samp5_alldata.pkl', 'wb') as f:\n",
    "    pickle.dump(clf_dbscan, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# eps = 1.5, min_samp15\n",
    "\n",
    "# (array([-1,  0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15,\n",
    "#         16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32,\n",
    "#         33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43]),\n",
    "#  array([189574, 224915,  19939,  25596,   3785,    390,    172,   2230,\n",
    "#            361,    348,     26,    115,     47,     20,     32,     39,\n",
    "#             45,     25,     12,     45,     16,     15,     74,     20,\n",
    "#             23,     44,     20,     31,     12,     26,     15,     12,\n",
    "#              8,     14,     20,     16,     10,     13,     23,      8,\n",
    "#             16,      9,     15,     11,     15]))\n",
    "\n",
    "\n",
    "\n",
    "# eps = 1.3, min_samp40\n",
    "# (array([-1,  0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15,\n",
    "#         16, 17]),\n",
    "#  array([318569,  34645,   4110,  78114,   9796,  18123,    846,   2249,\n",
    "#            402,    420,     55,     85,    473,     42,     44,    103,\n",
    "#             40,     44,     42]))\n",
    "\n",
    "# eps = 1.5, min_samp=40\n",
    "# (array([-1,  0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15,\n",
    "#         16, 17, 18]),\n",
    "#  array([219747, 197332,   7912,   2613,  11103,  23748,   2352,    820,\n",
    "#            413,   1126,    121,    116,    141,    235,     93,    113,\n",
    "#             42,    104,     46,     25]))\n",
    "\n",
    "# eps = 1.6, min_samp=40\n",
    "# (array([-1,  0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15,\n",
    "#         16, 17, 18, 19, 20]),\n",
    "#  array([186871, 225196,   8361,   3542,  36848,     71,   3290,    154,\n",
    "#            186,   1787,    225,    512,    261,    313,     41,     59,\n",
    "#            116,    204,     52,     28,     40,     45]))\n",
    "\n",
    "# eps = 1.7, min_samp=40\n",
    "# (array([-1,  0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11]),\n",
    "#  array([159151, 253519,  47322,   3951,    414,    162,   2297,    308,\n",
    "#            574,     67,     58,    351,     28]))\n",
    "\n",
    "# eps = 1.8, min_samp=40\n",
    "# (array([-1,  0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13]),\n",
    "#  array([135415, 274741,  49058,   2690,   4388,    649,    171,    368,\n",
    "#             89,     67,    409,     40,     63,     34,     20]))\n",
    "\n",
    "# eps = 1.9, min_samp=40\n",
    "# (array([-1,  0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14]),\n",
    "#  array([113161, 294350,  50515,   4692,   2978,   1071,    411,    177,\n",
    "#            459,    124,     72,     45,     32,     34,     41,     40]))\n",
    "\n",
    "# eps = 1.9, min_samp=30\n",
    "# (array([-1,  0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15,\n",
    "#         16, 17, 18, 19]),\n",
    "#  array([106113, 300526,  54158,   4788,   1266,    431,    179,     49,\n",
    "#            189,     84,     37,     39,     96,     18,     35,     40,\n",
    "#             30,     17,     47,     26,     34]))\n",
    "\n",
    "# eps = 1.9, min_samp=25\n",
    "# (array([-1,  0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15,\n",
    "#         16, 17, 18, 19, 20, 21, 22]),\n",
    "#  array([101173, 304979,  54720,   4893,   1379,    277,    112,     65,\n",
    "#             55,     20,    199,     25,     28,     26,     26,     19,\n",
    "#             48,     25,     31,     25,     13,     22,     23,     19]))\n",
    "\n",
    "# eps = 1.9, min_samp=20\n",
    "# (array([-1,  0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15,\n",
    "#         16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28]),\n",
    "#  array([ 94687, 309998,  56803,   4987,    412,    169,     16,    316,\n",
    "#             46,     20,     71,     87,     46,     60,     58,     43,\n",
    "#             54,     72,     35,     11,     27,     21,     23,     16,\n",
    "#             31,     18,      8,     18,     30,     19]))\n",
    "\n",
    "# eps = 1.9, min_samp=15\n",
    "# (array([-1,  0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15,\n",
    "#         16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32,\n",
    "#         33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49,\n",
    "#         50, 51, 52, 53]),\n",
    "#  array([ 85861, 374589,   5108,     57,    537,    146,    222,     25,\n",
    "#             28,     74,    440,     12,     50,     10,     12,     67,\n",
    "#             14,     50,    122,     94,     60,     31,     15,     51,\n",
    "#             78,     15,     33,     25,     19,     11,     15,      9,\n",
    "#              8,     12,     21,     15,     15,     31,     14,     13,\n",
    "#             16,     12,      9,     13,      5,     13,     10,     15,\n",
    "#             13,     20,      8,     15,     16,     11,     17]))\n",
    "\n",
    "# eps = 2.0, min_samp=20\n",
    "# (array([-1,  0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15,\n",
    "#         16, 17, 18, 19, 20, 21, 22, 23, 24, 25]),\n",
    "#  array([ 71817, 388777,   5265,    189,    625,    591,     60,     12,\n",
    "#             34,    155,    130,     98,     16,     36,     19,     76,\n",
    "#             56,     23,     23,     64,     20,     19,     24,     16,\n",
    "#             21,     16,     20]))\n",
    "\n",
    "\n",
    "# eps = 2.0, min_samp=40\n",
    "# (array([-1,  0,  1,  2,  3,  4,  5,  6]),\n",
    "#  array([ 91131, 314434,  56881,   4938,    460,    193,    125,     40]))\n",
    "\n",
    "#eps = 2.1, min_samp=40\n",
    "# (array([-1,  0,  1,  2,  3,  4,  5,  6,  7,  8]),\n",
    "#  array([ 69832, 391938,   5250,    509,    370,     78,     68,     45,\n",
    "#             71,     41]))\n",
    "\n",
    "# eps = 2.2, min_samp=40\n",
    "# (array([-1,  0,  1,  2,  3,  4,  5,  6,  7,  8,  9]),\n",
    "#  array([ 51761, 414550,    744,    619,     71,    158,     57,    101,\n",
    "#             41,     39,     61]))\n",
    "\n",
    "np.unique(clf_dbscan.labels_, return_counts=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../../../data_GRS1915/UMAPmapper_20d_shape16_moments4_trainedonall_468202.pkl', 'rb') as f:\n",
    "    UMAP_mapper = pickle.load(f)\n",
    "    \n",
    "umaped_data = UMAP_mapper.transform(shape_moments[:,:])\n",
    "\n",
    "# clf = GaussianMixture(n_components=50, covariance_type='full', verbose=0)\n",
    "# clf.fit(shape_moments[:50000,:])\n",
    "# custer_labels = clf.predict(shape_moments[:,:])\n",
    "\n",
    "# NUM_COLORS = 50\n",
    "# cm = plt.get_cmap(\"winter\")#('gist_rainbow')\n",
    "# colors = [cm(1.*i/NUM_COLORS) for i in range(NUM_COLORS)]\n",
    "# mapped_colors = [colors[i] for i in custer_labels]\n",
    "\n",
    "\n",
    "fig, axes = plt.subplots(nrows=1, ncols=1)\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (10,10)\n",
    "\n",
    "axes.scatter(umaped_data[:,0], umaped_data[:,1], s=0.1, c=clf_dbscan.labels_, marker=\".\")\n",
    "\n",
    "axes.set_xlim([np.min(umaped_data[:,0])-0.5, np.max(umaped_data[:,0])+0.5])\n",
    "axes.set_ylim([np.min(umaped_data[:,1])-0.5, np.max(umaped_data[:,1])+0.5])\n",
    "\n",
    "\n",
    "axes.set_yticks([])\n",
    "axes.set_xticks([])\n",
    "\n",
    "# plt.savefig('figures/classes/shape_moments_50clusters_alldata_winter_10x10.png', dpi=600)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = (20.0, 20.0)\n",
    "plt.rcParams.update({'font.size': 12})\n",
    "\n",
    "plt.plot(distances[:,5], label=\"k=5\")\n",
    "plt.plot(distances[:,10], label=\"k=10\")\n",
    "plt.plot(distances[:,20], label=\"k=20\")\n",
    "plt.plot(distances[:,39], label=\"k=39\")\n",
    "plt.plot(distances[:,50], label=\"k=50\")\n",
    "plt.plot(distances[:,100], label=\"k=100\")\n",
    "plt.plot(distances[:,200], label=\"k=200\")\n",
    "plt.plot(distances[:,499], label=\"k=499\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "plt.title(\"Sorted k-distance for 468202 samples of 20d data\")\n",
    "plt.xlabel(\"k-order\")\n",
    "plt.ylabel(\"distance (Euclidean)\")\n",
    "plt.ylim((0,4))\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import NearestNeighbors\n",
    "# X = shape_desc_GM[:50000,:]\n",
    "\n",
    "# for k in [30]:\n",
    "neigh = NearestNeighbors(n_neighbors=500, n_jobs=7, metric='euclidean')\n",
    "nbrs = neigh.fit(shape_moments)\n",
    "distances, indices = nbrs.kneighbors(shape_moments)\n",
    "distances = np.sort(distances, axis=0)\n",
    "#distances = distances[:,-1]\n",
    "# with open('../../../data_GRS1915/k-NN_distances_all_data_20D_468202samples_k0to500.pkl', 'wb') as f:\n",
    "#     pickle.dump(distances, f)\n",
    "#     clear_output(wait=True)\n",
    "# plt.plot(distances)\n",
    "# plt.ylim((0,4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gaussian mixture model analysis (20d data, 114 gaussian components)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"{}/shape16_moments4_components114_alldata.pkl\".format(data_dir), 'rb') as f:\n",
    "    clf_GM114 = pickle.load(f)\n",
    "\n",
    "# v, w = np.linalg.eig(clf_GM114.covariances_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "means = clf_GM114.means_\n",
    "covs = clf_GM114.covariances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.random import multivariate_normal\n",
    "from scipy.spatial.distance import mahalanobis\n",
    "from scipy.spatial.distance import euclidean\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dists=np.zeros((114,100,114))\n",
    "for comp1 in range(114):\n",
    "    #for comp in range(114):\n",
    "    multivar_dist = multivariate_normal(mean=means[comp1], cov=covs[comp1])\n",
    "    samples = multivar_dist.rvs(size=100)\n",
    "    for ns, sample in enumerate(samples):\n",
    "        for comp2 in range(114):\n",
    "            dists[comp1,ns,comp2]=mahalanobis(sample, means[comp2], np.linalg.inv(covs[comp2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dists=np.zeros((114,100,114))\n",
    "for comp1 in range(114):\n",
    "    #for comp in range(114):\n",
    "    multivar_dist = multivariate_normal(mean=means[comp1], cov=covs[comp1])\n",
    "    samples = multivar_dist.rvs(size=100)\n",
    "    for ns, sample in enumerate(samples):\n",
    "        for comp2 in range(114):\n",
    "            dists[comp1,ns,comp2]=mahalanobis(sample, means[comp2], np.linalg.inv(covs[comp2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dists=np.zeros((1,10000,114))\n",
    "for comp1 in range(1):\n",
    "    #for comp in range(114):\n",
    "    multivar_dist = multivariate_normal(mean=means[comp1], cov=covs[comp1])\n",
    "    samples = multivar_dist.rvs(size=10000)\n",
    "#     for ns, sample in enumerate(samples):\n",
    "    for comp2 in range(114):\n",
    "        mahalanobis_l = lambda sample: mahalanobis(sample, means[comp2], np.linalg.inv(covs[comp2]))\n",
    "        mahal_mapper = map(mahalanobis_l,samples)\n",
    "        dists[comp1,:,comp2]=np.array(list(mahal_mapper))\n",
    "        print([comp1, comp2])\n",
    "        clear_output(wait=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dists=np.zeros((114,100000,20))\n",
    "for comp1 in range(114):\n",
    "    #for comp in range(114):\n",
    "    samples = multivariate_normal(mean=means[comp1], cov=covs[comp1], size=100000)\n",
    "    dists[comp1,:,:]=np.array(samples)\n",
    "#     samples = multivar_dist.rvs(size=10000)\n",
    "#     for ns, sample in enumerate(samples):\n",
    "#     for comp2 in range(114):\n",
    "#         mahalanobis_l = lambda sample: mahalanobis(sample, means[comp2], np.linalg.inv(covs[comp2]))\n",
    "#         mahal_mapper = map(mahalanobis_l,samples)\n",
    "#         dists[comp1,:,comp2]=np.array(list(mahal_mapper))\n",
    "    print([comp1])\n",
    "    clear_output(wait=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multivar_dist = multivariate_normal(mean=means[comp1], cov=covs[comp1])\n",
    "multivar_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = np.random.multivariate_normal(mean=means[comp1], cov=covs[comp1], size=10000)\n",
    "# samples = multivar_dist.rvs(size=10000)\n",
    "euclidean_l = lambda sample: euclidean(sample, means[comp1])\n",
    "euclid_mapper = map(euclidean_l,samples)\n",
    "euclid_dists=np.array(list(euclid_mapper))\n",
    "\n",
    "mahalanobis_l = lambda sample: mahalanobis(sample, means[comp1], np.linalg.inv(covs[comp1]))\n",
    "mahal_mapper = map(mahalanobis_l,samples)\n",
    "mahal_dists=np.array(list(mahal_mapper))\n",
    "\n",
    "plt.hist(mahal_dists, label=\"Mahalanobis\")\n",
    "plt.hist(euclid_dists, label=\"Euclidean\", alpha=0.5)\n",
    "plt.legend()\n",
    "plt.xlabel(\"distance\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import mahalanobis\n",
    "from scipy.spatial.distance import euclidean\n",
    "\n",
    "dimensions = 20\n",
    "mean = np.zeros(dimensions) # sampling from standard normal distribution\n",
    "cov = np.identity(dimensions)\n",
    "samples = np.random.multivariate_normal(mean=mean, cov=cov, size=100000)\n",
    "\n",
    "euclidean_l = lambda sample: euclidean(sample, mean)\n",
    "euclid_mapper = map(euclidean_l,samples) # euclidean distance calculation\n",
    "euclid_dists=np.array(list(euclid_mapper)) \n",
    "\n",
    "mahalanobis_l = lambda sample: mahalanobis(sample, mean, np.linalg.inv(cov))\n",
    "mahal_mapper = map(mahalanobis_l,samples) # mahalanobis distance calculation\n",
    "mahal_dists=np.array(list(mahal_mapper))\n",
    "\n",
    "plt.hist(mahal_dists, label=\"Mahalanobis\")\n",
    "plt.hist(euclid_dists, label=\"Euclidean\", alpha=0.5)\n",
    "plt.legend()\n",
    "plt.xlabel(\"distance from the mean/origin in {}d\".format(dimensions))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sqrt(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(euclid_dists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(samples, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import chi2\n",
    "df=20\n",
    "rv = chi2(df)\n",
    "r = chi2.rvs(df, size=10000)\n",
    "x = np.linspace(chi2.ppf(0.01, df), chi2.ppf(0.99, df), 100)\n",
    "plt.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')\n",
    "plt.hist(r, normed=True, histtype='stepfilled', alpha=0.2)\n",
    "plt.xlim([0,50])\n",
    "# plt.legend(loc='best', frameon=False)\n",
    "print(np.min(r))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.min(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('{}/GMM114_1000samples_mahalanobis_dists.pkl'.format(data_dir), 'wb') as f:\n",
    "#     pickle.dump(dists, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs=[]\n",
    "for comp1_ind, comp1 in enumerate(dists):\n",
    "    max_self = np.max(comp1.T[comp1_ind])\n",
    "    for comp12_ind, comp12 in enumerate(comp1.T):\n",
    "        if np.min(comp12) < max_self:\n",
    "            pairs.append((comp1_ind,comp12_ind))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "math.factorial(114)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mahalanobis_l = lambda sample, dist_ind: mahalanobis(sample, means[dist_ind], np.linalg.inv(covs[dist_ind]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dists_l = map(mahalanobis_l,samples, list(range(114)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(list(dists_l)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxes =[]\n",
    "for n in range(114):\n",
    "    maxes.append(np.max(dists[n,:,n]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(maxes)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n=0\n",
    "plt.hist(dists[n,:,n])\n",
    "plt.xlabel(\"Mahalanobis distance between mean of cluster 1 and points sampled from it\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mahalanobis_ML(x=None, data=None, cov=None):\n",
    "    \"\"\"Compute the Mahalanobis Distance between each row of x and the data  \n",
    "    x    : vector or matrix of data with, say, p columns.\n",
    "    data : ndarray of the distribution from which Mahalanobis distance of each observation of x is to be computed.\n",
    "    cov  : covariance matrix (p x p) of the distribution. If None, will be computed from data.\n",
    "    \"\"\"\n",
    "    x_minus_mu = x - data\n",
    "#     if not cov:\n",
    "#         cov = np.cov(data.values.T)\n",
    "    inv_covmat = np.linalg.inv(cov)\n",
    "    left_term = np.dot(x_minus_mu, inv_covmat)\n",
    "    mahal = np.dot(left_term, x_minus_mu.T)\n",
    "    return mahal.diagonal()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(samples-means[comp])[2] - (samples[2]-means[comp])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mahalanobis_ML(samples, means[comp], covs[comp])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.special import gamma\n",
    "import pandas as pd\n",
    "\n",
    "n=20\n",
    "component_vol_members = np.zeros((114, 2))\n",
    "\n",
    "\n",
    "for comp in range(114):\n",
    "    component_vol_members[comp,0] = (np.pi**(n/2)/gamma(n/2+1))*np.product(np.sqrt(v[comp])*3)\n",
    "    component_vol_members[comp,1] = np.unique(preds, return_counts=1)[1][comp]\n",
    "\n",
    "component_vol_members_df = pd.DataFrame(component_vol_members, columns=(\"volume\", \"members\"))\n",
    "component_vol_members_df[\"volume_per_member\"] = component_vol_members_df.volume/component_vol_members_df.members\n",
    "component_vol_members_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "component_vol_members_df.sort_values(by=['volume'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca = PCA()\n",
    "pca.fit(shape_moments)\n",
    "n=20\n",
    "dataset_vol = (np.pi**(n/2)/gamma(n/2+1))*np.product(np.sqrt(pca.explained_variance_)*3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_vol/(component_vol_members_df.volume.sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# $\\frac{\\pi^{\\frac{n}{2}}}{\\Gamma({\\frac{n}{2}+1})} \\prod_{a=1}^{n} c_{a}$\n",
    "\n",
    "where n is the number of dimensions and $c_a$ is the length of semi-axis in dimension a\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "np.cumsum(component_vol_members_df.sort_values(by=['volume']).volume.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cum_sort_comp_vols = np.cumsum(component_vol_members_df.sort_values(by=['volume']).volume.values)\n",
    "cum_sort_comp_members = np.cumsum(component_vol_members_df.sort_values(by=['volume']).members.values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create some mock data\n",
    "t = np.arange(0.01, 10.0, 0.01)\n",
    "data1 = np.exp(t)\n",
    "data2 = np.sin(2 * np.pi * t)\n",
    "\n",
    "fig, ax1 = plt.subplots()\n",
    "\n",
    "color = 'tab:red'\n",
    "ax1.set_xlabel('time (s)')\n",
    "ax1.set_ylabel('exp', color=color)\n",
    "ax1.plot(t, data1, color=color)\n",
    "ax1.tick_params(axis='y', labelcolor=color)\n",
    "\n",
    "ax2 = ax1.twinx()  # instantiate a second axes that shares the same x-axis\n",
    "\n",
    "color = 'tab:blue'\n",
    "ax2.set_ylabel('sin', color=color)  # we already handled the x-label with ax1\n",
    "ax2.plot(t, data2, color=color)\n",
    "ax2.tick_params(axis='y', labelcolor=color)\n",
    "\n",
    "fig.tight_layout()  # otherwise the right y-label is slightly clipped\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://matplotlib.org/examples/axes_grid/demo_parasite_axes2.html\n",
    "\n",
    "fig, ax1 = plt.subplots()\n",
    "\n",
    "ax2 = ax1.twinx()\n",
    "ax3 = ax1.twinx()\n",
    "\n",
    "# offset = 60\n",
    "# new_fixed_axis = ax3.get_grid_helper().new_fixed_axis\n",
    "# ax3.axis[\"right\"] = new_fixed_axis(loc=\"right\",\n",
    "#                                     axes=ax3,\n",
    "#                                     offset=(offset, 0))\n",
    "\n",
    "# ax3.axis[\"right\"].toggle(all=True)\n",
    "\n",
    "\n",
    "\n",
    "ax1.plot([0,113], [dataset_vol]*2, label=\"3-sigma data set vol\", color = \"black\")\n",
    "ax1.plot(cum_sort_comp_vols, label=\"3-sigma component vol\", color = \"tab:blue\")\n",
    "# ax1.tick_params(axis='y', color=\"tab:blue\")\n",
    "ax1.set_yscale('log')\n",
    "ax1.set_ylabel(\"Cumulative volume\", color=\"tab:blue\")\n",
    "\n",
    "\n",
    "ax2.plot(cum_sort_comp_members, label=\"number of members\", color = \"tab:red\")\n",
    "# ax2.tick_params(axis='y', color=\"tab:red\")\n",
    "ax2.set_ylabel(\"Cumulative number of members\", color=\"tab:red\")\n",
    "\n",
    "ax3.plot(component_vol_members_df.sort_values(by=['volume']).volume_per_member.values, label=\"volume per member\", color = \"tab:green\")\n",
    "ax3.set_yscale('log')\n",
    "ax3.set_ylabel(\"Volume per member\", color=\"tab:green\")\n",
    "\n",
    "ax3.spines[\"right\"].set_position((\"axes\", 1.2))\n",
    "\n",
    "fig.legend(loc='lower right', bbox_to_anchor=(0.8, 0.1))\n",
    "# plt.yscale(\"log\")\n",
    "# plt.xlabel(\"Gaussian components\")\n",
    "# plt.ylabel(\"Cumulative volume\")\n",
    "plt.draw()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_vols = np.log(component_vol_members_df.volume.values)\n",
    "log_vols+=abs(np.min(log_vols))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "umap_mapper_GM114 = umap.UMAP()\n",
    "umaped_data = umap_mapper_GM114.fit_transform(clf_GM114.means_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams.update({'font.size': 12})\n",
    "plt.rcParams['figure.figsize'] = (10,10)\n",
    "plt.scatter(umaped_data[:,0], umaped_data[:,1], s=log_vols)\n",
    "plt.xlim([np.min(umaped_data[:,0])-0.5, np.max(umaped_data[:,0])+0.5])\n",
    "plt.ylim([np.min(umaped_data[:,1])-0.5, np.max(umaped_data[:,1])+0.5])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import OPTICS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optics_shape_moments = OPTICS(max_eps=4,min_samples=500, metric=\"euclidean\", n_jobs=30)\n",
    "optics_shape_moments.fit(shape_moments)\n",
    "with open('{}/OPTICS_shape16_moments4_max_eps4_min_samp500_euclidean_alldata.pkl'.format(data_dir), 'wb') as f:\n",
    "    pickle.dump(optics_shape_moments, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(optics_shape_moments.labels_, return_counts=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('{}/OPTICS_shape16_moments4_max_eps2-5_min_samp300_euclidean_alldata.pkl'.format(data_dir), 'rb') as f:\n",
    "    optics_shape_moments = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reachability plot\n",
    "clust = optics_shape_moments\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (100,10)\n",
    "plt.rcParams.update({'font.size': 42})\n",
    "\n",
    "\n",
    "\n",
    "# labels_050 = cluster_optics_dbscan(reachability=clust.reachability_,\n",
    "#                                    core_distances=clust.core_distances_,\n",
    "#                                    ordering=clust.ordering_, eps=0.5)\n",
    "# labels_200 = cluster_optics_dbscan(reachability=clust.reachability_,\n",
    "#                                    core_distances=clust.core_distances_,\n",
    "#                                    ordering=clust.ordering_, eps=2)\n",
    "\n",
    "space = np.arange(len(shape_moments))\n",
    "reachability = clust.reachability_[clust.ordering_]\n",
    "labels = clust.labels_[clust.ordering_]\n",
    "\n",
    "breaks = [0,21430,199990,242305,303065,309196,371590,383575,391430,395315, 414530, 468201]#371590\n",
    "for nb, brejk in enumerate(breaks[:-1]):\n",
    "    Xk = space[labels == 0][brejk:breaks[nb+1]]\n",
    "    Rk = reachability[labels == 0][brejk:breaks[nb+1]]\n",
    "    plt.scatter(Xk, Rk, alpha=1)\n",
    "    \n",
    "\n",
    "plt.plot(space[labels == -1], reachability[labels == -1], 'white', alpha=0.3)\n",
    "# plt.plot(space, np.full_like(space, 2., dtype=float), 'k-', alpha=0.5)\n",
    "# plt.plot(space, np.full_like(space, 0.5, dtype=float), 'k-.', alpha=0.5)\n",
    "# plt.set_ylabel('Reachability (epsilon distance)')\n",
    "# plt.set_title('Reachability Plot')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(12**2)/7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "12/7*1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reachability plot\n",
    "clust = optics_shape_moments\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (100,10)\n",
    "plt.rcParams.update({'font.size': 42})\n",
    "\n",
    "\n",
    "\n",
    "# labels_050 = cluster_optics_dbscan(reachability=clust.reachability_,\n",
    "#                                    core_distances=clust.core_distances_,\n",
    "#                                    ordering=clust.ordering_, eps=0.5)\n",
    "# labels_200 = cluster_optics_dbscan(reachability=clust.reachability_,\n",
    "#                                    core_distances=clust.core_distances_,\n",
    "#                                    ordering=clust.ordering_, eps=2)\n",
    "\n",
    "space = np.arange(len(shape_moments))\n",
    "reachability = clust.reachability_[clust.ordering_]\n",
    "labels = clust.labels_[clust.ordering_]\n",
    "\n",
    "breaks = [0,21430,199990,242305,303065,309196,371590,383575,391430,395315, 414530, 468201]#371590\n",
    "for nb, brejk in enumerate(breaks[:-1]):\n",
    "    Xk = space[labels == 0][brejk:breaks[nb+1]]\n",
    "    Rk = reachability[labels == 0][brejk:breaks[nb+1]]\n",
    "    plt.scatter(Xk, Rk, alpha=1)\n",
    "    \n",
    "\n",
    "plt.plot(space[labels == -1], reachability[labels == -1], 'white', alpha=0.3)\n",
    "# plt.plot(space, np.full_like(space, 2., dtype=float), 'k-', alpha=0.5)\n",
    "# plt.plot(space, np.full_like(space, 0.5, dtype=float), 'k-.', alpha=0.5)\n",
    "# plt.set_ylabel('Reachability (epsilon distance)')\n",
    "# plt.set_title('Reachability Plot')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "space[labels == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('../../../data_GRS1915/UMAPmapper_raw_histograms_trainedonall.pkl', 'wb') as f:\n",
    "#     pickle.dump(UMAP_mapper, f)\n",
    "\n",
    "# with open('{}/UMAPmapper_20d_shape16_moments4_trainedonall_468202.pkl'.format(data_dir), 'rb') as f:\n",
    "#     UMAP_mapper = pickle.load(f)\n",
    "    \n",
    "with open('{}/UMAP_transformed_20d_shape16_moments4_trainedonall_468202.pkl'.format(data_dir), 'rb') as f:\n",
    "    umaped_data = pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "umaped_data[clust.ordering_]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams.update({'font.size': 12})\n",
    "plt.rcParams['figure.figsize'] = (abs((np.min(umaped_data[:,0])-0.5) -(np.max(umaped_data[:,0])+0.5)), abs((np.min(umaped_data[:,1])-0.5)- (np.max(umaped_data[:,1])+0.5)))\n",
    "for nb, brejk in enumerate(breaks[:-1]):\n",
    "    plt.scatter(umaped_data[clust.ordering_][labels == 0][brejk:breaks[nb+1],0], umaped_data[clust.ordering_][labels == 0][brejk:breaks[nb+1],1], s=0.1)\n",
    "plt.xlim([np.min(umaped_data[:,0])-0.5, np.max(umaped_data[:,0])+0.5])\n",
    "plt.ylim([np.min(umaped_data[:,1])-0.5, np.max(umaped_data[:,1])+0.5])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# from matplotlib import cm\n",
    "# cm.get_cmap(plt.get_cmap(\"Set1\"))\n",
    "\n",
    "\n",
    "colours = ['#ffd8b1', '#000075', '#808080', '#e6194b', '#3cb44b', '#ffe119', '#4363d8', '#f58231', '#911eb4', '#46f0f0', '#f032e6', '#bcf60c', '#fabebe', '#008080', '#e6beff', '#9a6324', '#fffac8', '#800000', '#aaffc3', '#808000', '#000000']\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "# sns.set_style(\"white\")\n",
    "plt.rcParams['figure.figsize'] = (30.0, 30.0)\n",
    "plt.rcParams.update({'font.size': 0})\n",
    "\n",
    "class_data = umaped_data[clust.ordering_][labels == 0]\n",
    "\n",
    "# fig, ax = plt.subplots()\n",
    "\n",
    "fig, axs = plt.subplots(4, 3)\n",
    "axs = axs.flatten()\n",
    "\n",
    "for plot_class_ind, plot_class in enumerate(breaks[:-1]):\n",
    "#     class_indices = np.where(np.array(segment_class) == \"Unknown\")[0]\n",
    "#     class_data = embeddings_lap[class_indices]\n",
    "    axs[plot_class_ind].scatter(class_data[:,0], class_data[:,1], s = 0.2, c=\"grey\", label=\"Unknown\")\n",
    "    \n",
    "    axs[plot_class_ind].scatter(class_data[plot_class:breaks[plot_class_ind+1],0], class_data[plot_class:breaks[plot_class_ind+1],1], s = 1, c='red', label=plot_class_ind)\n",
    "    \n",
    "# plt.legend()\n",
    "    axs[plot_class_ind].set_title(\"{}\".format(plot_class_ind), fontsize=42)\n",
    "    \n",
    "    \n",
    "axs[-1].scatter(class_data[:,0], class_data[:,1], s = 0.2, c=\"grey\", label=\"Unknown\")\n",
    "axs[-1].scatter(umaped_data[clust.ordering_][labels == 0][:,0], umaped_data[clust.ordering_][labels == 0][:,1], s = 1, c='red', label=plot_class_ind)\n",
    "axs[-1].set_title(\"{}\".format(\"noise\"), fontsize=42)\n",
    "axs.reshape((4,3))\n",
    "# plt.savefig(\"classes_separate.png\")\n",
    "\n",
    "# plt.savefig(\"UMAP_embedding_separate_classes_model_2020-02-09_10-36-06.png\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# redint = np.where(np.array(qpo_colours) == \"red\")\n",
    "# greyint= np.where(np.array(qpo_colours) != \"red\")\n",
    "# plt.scatter(embeddings_lap[:,0][greyint], embeddings_lap[:,1][greyint], s=1, c=\"grey\", label= \"other\")\n",
    "# plt.scatter(embeddings_lap[:,0][redint], embeddings_lap[:,1][redint], s=1, c=\"red\", label= \"HF QPO\")\n",
    "# plt.title(\"UMAP embedding of the encoded GRS1915 segments, neighbors=50, min_dist=0.0, components=2\", fontsize=12)\n",
    "# plt.legend()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import cluster_optics_dbscan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " min_samples=5, max_eps=inf, metric='minkowski', p=2, metric_params=None, cluster_method='xi', eps=None, xi=0.05,\n",
    "    predecessor_correction=True, min_cluster_size=None, algorithm='auto', leaf_size=30, n_jobs=None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_GM114.means_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "space_vol = 1\n",
    "for dim in range(20):\n",
    "    space_vol*= np.abs(np.max(shape_moments[dim])-np.min(shape_moments[dim]))\n",
    "print(space_vol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "43483574/71396378333"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with open(\"../../../data_GRS1915/shape16_moments4_components114_alldata.pkl\", 'rb') as f:\n",
    "    clf = pickle.load(f)\n",
    "    \n",
    "print(clf.aic(shape_moments),\n",
    "      clf.bic(shape_moments))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = clf.predict_proba(shape_moments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = clf.predict(shape_moments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(np.unique(preds, return_counts=1)[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.linalg.eigh(clf.covariances_)[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v, w = np.linalg.eigh(clf.covariances_)\n",
    "u = w[0] / np.linalg.norm(w[0])\n",
    "angle = np.arctan2(u[1], u[0])\n",
    "angle = 180 * angle / np.pi  # convert to degrees\n",
    "v = 2. * np.sqrt(2.) * np.sqrt(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca = PCA()\n",
    "pca.fit(shape_moments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "4/3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.special import gamma\n",
    "\n",
    "n=3\n",
    "(np.pi**(n/2)/gamma(n/2+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(np.pi**(n/2)/gamma(n/2+1))*np.product(np.sqrt(pca.explained_variance_)*2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sqrt(pca.explained_variance_)*3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "from shapely.geometry.point import Point\n",
    "from shapely import affinity\n",
    "from matplotlib.patches import Polygon\n",
    "import numpy as np\n",
    "\n",
    "def create_ellipse(center, lengths, angle=0):\n",
    "    \"\"\"\n",
    "    create a shapely ellipse. adapted from\n",
    "    https://gis.stackexchange.com/a/243462\n",
    "    \"\"\"\n",
    "    circ = Point(center).buffer(1)\n",
    "    ell = affinity.scale(circ, int(lengths[0]), int(lengths[1]))\n",
    "    ellr = affinity.rotate(ell, angle)\n",
    "    return ellr\n",
    "\n",
    "fig,ax = plt.subplots()\n",
    "\n",
    "##these next few lines are pretty important because\n",
    "##otherwise your ellipses might only be displayed partly\n",
    "##or may be distorted\n",
    "ax.set_xlim([-5,5])\n",
    "ax.set_ylim([-5,5])\n",
    "ax.set_aspect('equal')\n",
    "\n",
    "##first ellipse in blue\n",
    "ellipse1 = create_ellipse((0,0),(2,4),10)\n",
    "verts1 = np.array(ellipse1.exterior.coords.xy)\n",
    "patch1 = Polygon(verts1.T, color = 'blue', alpha = 0.5)\n",
    "ax.add_patch(patch1)\n",
    "\n",
    "##second ellipse in red    \n",
    "ellipse2 = create_ellipse((1,-1),(3,2),50)\n",
    "verts2 = np.array(ellipse2.exterior.coords.xy)\n",
    "patch2 = Polygon(verts2.T,color = 'red', alpha = 0.5)\n",
    "ax.add_patch(patch2)\n",
    "\n",
    "##the intersect will be outlined in black\n",
    "intersect = ellipse1.intersection(ellipse2)\n",
    "verts3 = np.array(intersect.exterior.coords.xy)\n",
    "patch3 = Polygon(verts3.T, facecolor = 'none', edgecolor = 'black')\n",
    "ax.add_patch(patch3)\n",
    "\n",
    "##compute areas and ratios \n",
    "print('area of ellipse 1:',ellipse1.area)\n",
    "print('area of ellipse 2:',ellipse2.area)\n",
    "print('area of intersect:',intersect.area)\n",
    "print('intersect/ellipse1:', intersect.area/ellipse1.area)\n",
    "print('intersect/ellipse2:', intersect.area/ellipse2.area)\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(clf.weights_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=114)\n",
    "pca.fit(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import euclidean_distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# euclidean_distances(clf.means_)\n",
    "plt.hist(euclidean_distances(clf.means_)[np.tril_indices(114)])\n",
    "plt.xlabel(\"Euclidean distance\")\n",
    "plt.ylabel(\"Ratio of explained variance\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(pca.explained_variance_ratio_[:60])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(pca.explained_variance_ratio_)\n",
    "# plt.plot(np.cumsum(pca.explained_variance_ratio_))\n",
    "\n",
    "plt.xlabel(\"No. of principal components\")\n",
    "plt.ylabel(\"Ratio of explained variance\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import DBSCAN\n",
    "clf_dbscan = DBSCAN(eps=1.9,min_samples=200, n_jobs=7)\n",
    "clf_dbscan.fit(shape_moments)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(clf_dbscan.labels_, return_counts=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "dbscan_grid = []\n",
    "\n",
    "for n_eps, eps in enumerate(np.arange(1.5, 2.2, 0.05)):\n",
    "    for n_min, min_samples in enumerate(range(40, 120, 5)):\n",
    "\n",
    "        clf = DBSCAN(eps=eps,min_samples=min_samples, n_jobs=7)\n",
    "        clf.fit(shape_desc_GM[:50000,:])\n",
    "        \n",
    "        dbscan_grid.append((np.unique(clf.labels_, return_counts=1), eps, min_samples))\n",
    "        \n",
    "        print(n_eps, n_min)\n",
    "        clear_output(wait=True)\n",
    "        \n",
    "# with open('dbscan_grid_search_50k.pkl', 'wb') as f:\n",
    "#     pickle.dump(dbscan_grid, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "plt.rcParams['figure.figsize'] =[20,10]\n",
    "\n",
    "plt.plot([5, 10, 30, 50, 70, 90, 100, 110, 120, 150, 170, 200], criteria[0,:], label=\"AIC\") #125\n",
    "plt.plot([5, 10, 30, 50, 70, 90, 100, 110, 120, 150, 170, 200], criteria[1,:], label=\"BIC\") #125\n",
    "plt.title(\"Information criteria for GMM trained on 50k samples of shape and descriptive statistics data\", pad=20, fontsize=24)\n",
    "plt.ylabel(\"Information criterion value\", fontsize=24)\n",
    "plt.xlabel(\"Number of Gaussian components\", fontsize=24)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] =[20,10]\n",
    "\n",
    "plt.plot(np.arange(110,150, 2), criteria[0,:], label=\"AIC\") #125\n",
    "plt.plot(np.arange(110,150, 2), criteria[1,:], label=\"BIC\") #125\n",
    "plt.title(\"Information criteria for GMM trained on 50k samples of shape and descriptive statistics data\", pad=20, fontsize=24)\n",
    "plt.ylabel(\"Information criterion value\", fontsize=24)\n",
    "plt.xlabel(\"Number of Gaussian components\", fontsize=24)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../../../data_GRS1915/468202_len128_s2_4cad_counts_errorfix.pkl', 'rb') as f:\n",
    "    segments = pickle.load(f)\n",
    "with open('../../../data_GRS1915/468202_len128_s2_4cad_ids_errorfix.pkl', 'rb') as f:\n",
    "    seg_ids = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.mixture import GaussianMixture\n",
    "\n",
    "clf = GaussianMixture(n_components=1, covariance_type='full', verbose=1)\n",
    "clf.fit(segments[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_gmm(gmm, X, label=True, ax=None):\n",
    "    \"\"\"https://jakevdp.github.io/PythonDataScienceHandbook/05.12-gaussian-mixtures.html\"\"\"\n",
    "    \n",
    "    ax = ax or plt.gca()\n",
    "    labels = gmm.fit(X).predict(X)\n",
    "    if label:\n",
    "        ax.scatter(X[:, 0], X[:, 1], c=labels, s=40, cmap='viridis', zorder=2)\n",
    "    else:\n",
    "        ax.scatter(X[:, 0], X[:, 1], s=40, zorder=2)\n",
    "    ax.axis('equal')\n",
    "    \n",
    "    w_factor = 0.2 / gmm.weights_.max()\n",
    "    for pos, covar, w in zip(gmm.means_, gmm.covars_, gmm.weights_):\n",
    "        draw_ellipse(pos, covar, alpha=w * w_factor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(segments[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('{}/468202_len128_s2_4cad_counts_errorfix.pkl'.format(data_dir), 'rb') as f:\n",
    "    segments_counts = pickle.load(f)\n",
    "# with open('../../../data_GRS1915/468202_len128_s2_4cad_errors_errorfix.pkl', 'rb') as f:\n",
    "#     segments_errors = pickle.load(f)\n",
    "# with open('../../../data_GRS1915/468202_len128_s2_4cad_ids_errorfix.pkl', 'rb') as f:\n",
    "#     id_per_seg = pickle.load(f)\n",
    "\n",
    "weights_dir = \"../../../model_weights/model_2020-04-29_09-12-23.h5\"\n",
    "segments_dir = '../../../data_GRS1915/468202_len128_s2_4cad_counts_errorfix.pkl'\n",
    "segment_encoding_dir = '{}/segment_encoding_{}_segments_{}.pkl'.format(data_dir, weights_dir.split(\"/\")[-1].split(\".\")[0], segments_dir.split(\"/\")[-1].split(\".\")[0])\n",
    "\n",
    "with open(segment_encoding_dir, 'rb') as f:\n",
    "    segment_encoding = pickle.load(f)\n",
    "    \n",
    "segment_encoding_scaled_means = zscore(segment_encoding[:,0,:], axis=0).astype(np.float32)  # standardize per feature\n",
    "\n",
    "\n",
    "desc_stats = np.zeros((len(segments_counts), 4)) #mean, std, skew, kurt\n",
    "# desc_stats[:,0] = np.median(segments_counts, axis=1).flatten()\n",
    "desc_stats[:,0] = np.mean(segments_counts, axis=1).flatten()\n",
    "desc_stats[:,1] = np.std(segments_counts, axis=1).flatten()\n",
    "desc_stats[:,2] = stats.skew(segments_counts, axis=1).flatten()\n",
    "desc_stats[:,3] = stats.kurtosis(segments_counts, axis=1).flatten()\n",
    "zscore_desc_stats = zscore(desc_stats, axis=0)\n",
    "\n",
    "# desc_GM = np.hstack((zscore(desc_stats, axis=0), GMM_bics))\n",
    "\n",
    "shape_moments = np.hstack((segment_encoding_scaled_means, zscore_desc_stats)) # every column is standardized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = GaussianMixture(n_components=500, covariance_type='full', verbose=1)\n",
    "clf.fit(shape_moments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('{}/GMM_shape16_moments4_components500_alldata.pkl'.format(data_dir), 'wb') as f:\n",
    "#     pickle.dump(clf, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
