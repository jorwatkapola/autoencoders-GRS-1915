{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from IPython.display import clear_output\n",
    "import os\n",
    "import fnmatch\n",
    "import numpy as np\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import umap\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from scipy import stats\n",
    "# from sklearn.cluster import OPTICS\n",
    "from copy import deepcopy\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# import tensorflow as tf\n",
    "# from tensorflow.keras import layers\n",
    "# from tensorflow.keras.backend import mean\n",
    "# from tensorflow.keras.backend import square\n",
    "# from tensorflow.keras.models import Sequential\n",
    "# from tensorflow.keras.layers import CuDNNLSTM\n",
    "# from tensorflow.keras.layers import Dense\n",
    "# from tensorflow.keras.layers import RepeatVector\n",
    "# from tensorflow.keras.layers import TimeDistributed\n",
    "# from tensorflow.keras.callbacks import EarlyStopping\n",
    "# from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "# from tensorflow.keras.layers import Flatten\n",
    "\n",
    "# from tensorflow.keras.utils import Sequence\n",
    "# from tensorflow.keras import Input\n",
    "# from tensorflow.keras import Model\n",
    "# from tensorflow.keras.layers import BatchNormalization\n",
    "# from tensorflow.keras.layers import Conv1D\n",
    "from scipy.stats import zscore\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (5.0, 5.0)\n",
    "plt.rcParams.update({'font.size': 12})\n",
    "plt.rcParams.update(plt.rcParamsDefault)\n",
    "\n",
    "np.random.seed(seed=11)\n",
    "\n",
    "\n",
    "cwd = os.getcwd()\n",
    "\n",
    "if cwd.split(\"/\")[1] == \"export\":\n",
    "    data_dir = \"../../../files_from_snuffy\"\n",
    "else:\n",
    "    data_dir = \"../../../data_GRS1915\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('{}/1776_light_curves_1s_bin_errorfix.pkl'.format(data_dir), 'rb') as f:\n",
    "    lcs = pickle.load(f)\n",
    "with open('{}/1776_light_curves_1s_bin_ids_errorfix.pkl'.format(data_dir), 'rb') as f:\n",
    "    ids = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load light curve segments\n",
    "with open('{}/468202_len128_s2_4cad_counts_errorfix.pkl'.format(data_dir), 'rb') as f:\n",
    "    segments_counts = pickle.load(f)\n",
    "    \n",
    "# load latent variables for light curve segments\n",
    "weights_dir = \"../../../model_weights/model_2020-08-30_11-42-38.h5\"\n",
    "segments_dir = '../../../data_GRS1915/468202_len128_s2_4cad_counts_errorfix.pkl'\n",
    "segment_encoding_dir = '{}/segment_encoding_{}_segments_{}.pkl'.format(data_dir, weights_dir.split(\"/\")[-1].split(\".\")[0], segments_dir.split(\"/\")[-1].split(\".\")[0])\n",
    "with open(segment_encoding_dir, 'rb') as f:\n",
    "    segment_encoding = pickle.load(f)\n",
    "\n",
    "# take latent variable means, i.e. 16 values per segment\n",
    "segment_encoding_scaled_means = zscore(segment_encoding[:,0,:], axis=0).astype(np.float32)  # standardize per feature\n",
    "\n",
    "# calculate statistical moments for the segments\n",
    "desc_stats = np.zeros((len(segments_counts), 4)) #mean, std, skew, kurt\n",
    "desc_stats[:,0] = np.mean(segments_counts, axis=1).flatten()\n",
    "desc_stats[:,1] = np.std(segments_counts, axis=1).flatten()\n",
    "desc_stats[:,2] = stats.skew(segments_counts, axis=1).flatten()\n",
    "desc_stats[:,3] = stats.kurtosis(segments_counts, axis=1).flatten()\n",
    "zscore_desc_stats = zscore(desc_stats, axis=0)\n",
    "\n",
    "# merge the two types of features; shape of shape_moments is [468202, 20]\n",
    "shape_moments = np.hstack((segment_encoding_scaled_means, zscore_desc_stats)) # every column is standardized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit a number of models for 10k subsets of segments, and for a range of numbers of component\n",
    "# this was run over a couple of attempts, but using this exact loop, \n",
    "# so I put together the bic values and plotted the together in the next cell\n",
    "# [10,25,50,70,80,90,100,110,120,130,140]\n",
    "# [110,115,120,125,130,135,140]#50k\n",
    "component_no_list = np.concatenate(([10, 25, 50, 70, 80, 90, 100], np.arange(110,142, 2), [150]))\n",
    "subset_size = 50e3\n",
    "no_subsets = 5\n",
    "data_subsets = [(int(start*subset_size), int((1+start)*subset_size)) for start in range(no_subsets)][3:]\n",
    "# gaussian_mixture_bics = np.zeros((len(data_subsets),len(component_no_list)))\n",
    "\n",
    "gaussian_mixture_bics= np.loadtxt(\"gaussian_mixture_bics_new_model_10-150_50k.csv\", delimiter=\",\")\n",
    "\n",
    "for set_iteration, data_set in enumerate(data_subsets):\n",
    "    for comp_iteration, component_no in enumerate(component_no_list):\n",
    "        if gaussian_mixture_bics[set_iteration,comp_iteration] != 0.:\n",
    "            continue\n",
    "        GMmodel = GaussianMixture(n_components=component_no, random_state=0)\n",
    "        GMmodel.fit(shape_moments[data_set[0]:data_set[1],:])\n",
    "        gaussian_mixture_bics[set_iteration,comp_iteration] = GMmodel.bic(shape_moments)\n",
    "\n",
    "        np.savetxt('gaussian_mixture_bics_new_model_10-150_50k.csv', gaussian_mixture_bics, delimiter =', ') \n",
    "        print(set_iteration, comp_iteration)\n",
    "        \n",
    "# component_no_list = [150]\n",
    "# subset_size = 50e3\n",
    "# no_subsets = 3\n",
    "# data_subsets = [(int(start*subset_size), int((1+start)*subset_size)) for start in range(no_subsets)]\n",
    "# gaussian_mixture_bics = np.zeros((len(data_subsets),len(component_no_list)))\n",
    "\n",
    "# for set_iteration, data_set in enumerate(data_subsets):\n",
    "#     for comp_iteration, component_no in enumerate(component_no_list):\n",
    "#         if gaussian_mixture_bics[set_iteration,comp_iteration] != 0.:\n",
    "#             continue\n",
    "#         GMmodel = GaussianMixture(n_components=component_no, random_state=0)\n",
    "#         GMmodel.fit(shape_moments[data_set[0]:data_set[1],:])\n",
    "#         gaussian_mixture_bics[set_iteration,comp_iteration] = GMmodel.bic(shape_moments)\n",
    "\n",
    "#         np.savetxt('gaussian_mixture_bics_new_model_150comps_50k.csv', gaussian_mixture_bics, delimiter =', ') \n",
    "#         print(set_iteration, comp_iteration)\n",
    "# #         clear_output(wait=True)\n",
    "# # index 8 did not converge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit a couple extra models at 160 and 170 components\n",
    "component_no_list = [180,190, 200]\n",
    "subset_size = 50e3\n",
    "no_subsets = 5\n",
    "data_subsets = [(int(start*subset_size), int((1+start)*subset_size)) for start in range(no_subsets)]\n",
    "gaussian_mixture_bics = np.zeros((len(data_subsets),len(component_no_list)))\n",
    "\n",
    "# gaussian_mixture_bics= np.loadtxt(\"gaussian_mixture_bics_new_model_10-150_50k.csv\", delimiter=\",\")\n",
    "\n",
    "for set_iteration, data_set in enumerate(data_subsets):\n",
    "    for comp_iteration, component_no in enumerate(component_no_list):\n",
    "        if gaussian_mixture_bics[set_iteration,comp_iteration] != 0.:\n",
    "            continue\n",
    "        GMmodel = GaussianMixture(n_components=component_no, random_state=0)\n",
    "        GMmodel.fit(shape_moments[data_set[0]:data_set[1],:])\n",
    "        gaussian_mixture_bics[set_iteration,comp_iteration] = GMmodel.bic(shape_moments)\n",
    "\n",
    "        np.savetxt('gaussian_mixture_bics_160-170_50k.csv', gaussian_mixture_bics, delimiter =', ') \n",
    "        print(set_iteration, comp_iteration)\n",
    "        \n",
    "# component_no_list = [150]\n",
    "# subset_size = 50e3\n",
    "# no_subsets = 3\n",
    "# data_subsets = [(int(start*subset_size), int((1+start)*subset_size)) for start in range(no_subsets)]\n",
    "# gaussian_mixture_bics = np.zeros((len(data_subsets),len(component_no_list)))\n",
    "\n",
    "# for set_iteration, data_set in enumerate(data_subsets):\n",
    "#     for comp_iteration, component_no in enumerate(component_no_list):\n",
    "#         if gaussian_mixture_bics[set_iteration,comp_iteration] != 0.:\n",
    "#             continue\n",
    "#         GMmodel = GaussianMixture(n_components=component_no, random_state=0)\n",
    "#         GMmodel.fit(shape_moments[data_set[0]:data_set[1],:])\n",
    "#         gaussian_mixture_bics[set_iteration,comp_iteration] = GMmodel.bic(shape_moments)\n",
    "\n",
    "#         np.savetxt('gaussian_mixture_bics_new_model_150comps_50k.csv', gaussian_mixture_bics, delimiter =', ') \n",
    "#         print(set_iteration, comp_iteration)\n",
    "# #         clear_output(wait=True)\n",
    "# # index 8 did not converge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gaussian_mixture_bics= np.loadtxt(\"gaussian_mixture_bics_new_model_10-150_50k.csv\", delimiter=\",\")\n",
    "\n",
    "gaussian_mixture_bics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bics = [1.853893282449822500e+07, 1.722017647717545927e+07, 1.658319324933229759e+07, 1.630327464005833492e+07, 1.622106282425292768e+07, 1.616687566882342100e+07, 1.615730210213817097e+07, 1.613055099964568764e+07, 1.611427361123152077e+07, 1.614833229564627074e+07, 1.612438706897565909e+07, 1.610314177309925482e+07, 1.611599805476822704e+07, 1.611190407219867036e+07, 1.613187932035312243e+07, 1.612686166295694374e+07, 1.610543674698158540e+07, 1.610464430899482593e+07, 1.612001376647870429e+07, 1.613567941551903449e+07, 1.613709874137604795e+07, 1.613797303043186106e+07, 1.613711653342354484e+07]\n",
    "comps = np.concatenate(([10, 25, 50, 70, 80, 90, 100], np.arange(110,142, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_subsets = [(0,50000), (50000, 100000), (100000,150000),(150000,200000), (200000, 250000)]\n",
    "# component_no_list = [10,25,50,70,80,90,100,105,110,111,112,113,114,115,116,117,118,119,120,125,130,140,150,200,300]\n",
    "# bics = np.array([\n",
    "# [1.513076963327877596e+07, 1.373494201573342085e+07, 1.310223591745524108e+07, 1.287644645175125077e+07, 1.274916968428172357e+07, 1.275498408091743104e+07, 1.272725532548042759e+07,1.273327056713533960e+07, 1.273110944201433845e+07, 1.271215291174531914e+07, 1.269974317490880564e+07, 1.269413576213483885e+07, 1.269831422006917745e+07, 1.270286540602960251e+07, 1.272652081320444494e+07, 1.268854252120269835e+07, 1.271878362980804965e+07, 1.268805628370952979e+07, 1.268268194913305156e+07,1.269743282206927985e+07, 1.269253136956013553e+07, 1.269973950701179914e+07, 1.271462149632845633e+07, 1.292783153119732440e+07, 1.339544358913458139e+07],\n",
    "# [1.517668787782970630e+07, 1.375655116087884270e+07, 1.301894537214316800e+07, 1.278502388570801169e+07, 1.274546161574119143e+07, 1.272605305849449150e+07, 1.265987061649004370e+07,1.273297575760359690e+07, 1.264832313142326102e+07, 1.267118953481068090e+07, 1.264825245518371463e+07, 1.269095693081272021e+07, 1.266793768580078520e+07, 1.268845881720614433e+07, 1.266137312288295850e+07, 1.268985047919468395e+07, 1.271996471582287923e+07, 1.273072799564955197e+07, 1.269101905685216561e+07,1.270770922392238863e+07, 1.270597545123521052e+07, 1.268562237603535876e+07, 1.274607550342624076e+07, 1.291487403943038359e+07, 1.345206031202976964e+07],\n",
    "# [1.513147027530102991e+07, 1.372693726637074538e+07, 1.312636607615234889e+07, 1.280763613113106042e+07, 1.276878601520187221e+07, 1.274302309312643856e+07, 1.271846136874406226e+07,1.267622275037734210e+07, 1.266478125283669680e+07, 1.272365127627760172e+07, 1.270341859422520176e+07, 1.274606540429916233e+07, 1.270035337163357809e+07, 1.269462134574233554e+07, 1.264591281519959494e+07, 1.266692204580633529e+07, 1.268642471454811655e+07, 1.267109306289681979e+07, 1.267652125702439621e+07,1.268893210948343575e+07, 1.268379260471562855e+07, 1.266906201551120169e+07, 1.271607317416701652e+07, 1.287800719977548718e+07, 1.346352189637094550e+07],\n",
    "# [1.513075834188815393e+07, 1.372975220068028197e+07, 1.305765112890582718e+07, 1.283192171104326844e+07, 1.274878631334129721e+07, 1.271947265357426368e+07, 1.270828195666158944e+07,1.271268768232382834e+07, 1.263517875820080563e+07, 1.269493846218724176e+07, 1.264023021297656186e+07, 1.266014644823007658e+07, 1.266488662979799323e+07, 1.264187690211539157e+07, 1.264225268117410690e+07, 1.263162761737927422e+07, 1.262308756661838479e+07, 1.260968587468877248e+07, 1.261931922283876687e+07,1.265345469640533440e+07, 1.265581449083781987e+07, 1.267555561351045780e+07, 1.271492636046844162e+07, 1.291254769987491146e+07, 1.341979996590973809e+07],\n",
    "# [1.509176813460635953e+07, 1.379692069009907730e+07, 1.309409092464722134e+07, 1.283283915952128172e+07, 1.278231797994311526e+07, 1.271217661442268454e+07, 1.264713496262365766e+07,1.270169793278488889e+07, 1.265528527531241067e+07, 1.265825935773974657e+07, 1.268149589179967158e+07, 1.265726369034977630e+07, 1.265107941581576318e+07, 1.266242553794561140e+07, 1.266178248237481155e+07, 1.267541880952490866e+07, 1.269726119435581937e+07, 1.270745073380917870e+07, 1.265447378898119554e+07,1.270840458273771219e+07, 1.268563932986871898e+07, 1.268250343190071173e+07, 1.269332119142067619e+07, 1.287152213201368600e+07, 1.345943013665321097e+07]\n",
    "# ])\n",
    "bics = gaussian_mixture_bics[:2]\n",
    "minimums = []\n",
    "for iteration, dataset_bics in enumerate(bics):\n",
    "    minimums.append(component_no_list[np.argmin(dataset_bics)])\n",
    "    plt.plot(component_no_list,dataset_bics, label=data_subsets[iteration])\n",
    "print(\"Minimum BIC values: \", minimums)\n",
    "print(\"Mean of minimum BIC values: \", np.mean(minimums))\n",
    "print(\"Minimum of mean BIC values (minimum of cyan): \",component_no_list[np.argmin(np.mean(bics, axis=0))])\n",
    "plt.plot(component_no_list,np.mean(bics, axis=0), marker = \"o\", c=\"cyan\", label=\"mean\")\n",
    "plt.xlim([90,140])\n",
    "plt.ylim([1.61e7, 1.62e7])\n",
    "plt.legend()\n",
    "plt.title(\"BIC values for Gaussian mixture models fit to 5 data subsets of 50k segments\")\n",
    "plt.xlabel(\"number of Gaussian components\")\n",
    "plt.ylabel(\"Bayesian information criterion\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GMmodel = GaussianMixture(n_components=128, random_state=0,verbose=2)\n",
    "GMmodel.fit(shape_moments)\n",
    "GMmodel_dir = \"{}/GMM_128comp_20d_alldata_model_2020-08-30_11-42-38.pkl\".format(data_dir)\n",
    "with open(GMmodel_dir, 'wb') as f:\n",
    "    pickle.dump(GMmodel, f)\n",
    "    \n",
    "# copy of the segment encoding in /export/data/jakubok/GRS1915+105>\n",
    "# pickled GMM is named segment_encoding_model_2020-08-30_11-42-38_segments_468202_len128_s2_4cad_counts_errorfix.pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "16696+6*12000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# shape_moments_GMM122_labels = GMmodel.predict(shape_moments)\n",
    "# with open(\"{}/shape_moments_GMM122_labels\".format(data_dir), 'wb') as f:\n",
    "#     pickle.dump(shape_moments_GMM122_labels, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparison 122 components Gaussian Mixture Model vs Belloni classifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"{}/shape_moments_GMM122_labels.pkl\".format(data_dir), 'rb') as f: # output of LSTM autoencoder's decoder\n",
    "    shape_moments_GMM122_labels = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load observation classifications from Huppenkothen 2017\n",
    "clean_belloni = open('{}/1915Belloniclass_updated.dat'.format(data_dir))\n",
    "lines = clean_belloni.readlines()\n",
    "states = lines[0].split()\n",
    "belloni_clean = {}\n",
    "for h,l in zip(states, lines[1:]):\n",
    "    belloni_clean[h] = l.split()\n",
    "    #state: obsID1, obsID2...\n",
    "ob_state = {}\n",
    "for state, obs in belloni_clean.items():\n",
    "    if state == \"chi1\" or state == \"chi2\" or state == \"chi3\" or state == \"chi4\": state = \"chi\"\n",
    "    for ob in obs:\n",
    "        ob_state[ob] = state\n",
    "        \n",
    "# make a dict of Tomaso's classifications  (Daniela's set) against observation ids,\n",
    "# i.e. ob_state == {'20187-02-01-00': 'alpha', '20187-02-01-01': 'alpha', '20402-01-22-00': 'alpha', ...}\n",
    "# lines = clean_belloni.readlines()\n",
    "# states = lines[0].split()\n",
    "# belloni_clean = {}\n",
    "# for h,l in zip(states, lines[1:]):\n",
    "#     belloni_clean[h] = l.split()\n",
    "#     #state: obsID1, obsID2...\n",
    "# ob_state = {}\n",
    "# for state, obs in belloni_clean.items():\n",
    "#     if state == \"chi1\" or state == \"chi2\" or state == \"chi3\" or state == \"chi4\": state = \"chi\"\n",
    "#     for ob in obs:\n",
    "#         ob_state[ob] = state\n",
    "        \n",
    "        \n",
    "# load IDs of segmented light curves: observationsID_segmentIndex\n",
    "with open('{}/468202_len128_s2_4cad_ids_errorfix.pkl'.format(data_dir), 'rb') as f:\n",
    "    seg_ids = pickle.load(f)\n",
    "\n",
    "        \n",
    "seg_ObIDs = [seg.split(\"_\")[0] for seg in seg_ids] # get rid of the within-observation segment indices and create a degenerate list of observation IDs\n",
    "\n",
    "classes = np.array([\"alpha\", \"beta\", \"gamma\", \"delta\", \"theta\", \"kappa\", \"lambda\", \"mu\", \"nu\", \"rho\", \"phi\", \"chi\", \"eta\", \"omega\"])\n",
    "scales = []\n",
    "segment_class = []\n",
    "for ob in seg_ObIDs:\n",
    "    if ob in ob_state:\n",
    "        segment_class.append(ob_state[ob])\n",
    "    else:\n",
    "        segment_class.append(\"Unknown\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_classification = shape_moments_GMM122_labels\n",
    "\n",
    "Belloni_classes = np.array([\"alpha\", \"beta\", \"gamma\", \"delta\", \"theta\", \"kappa\", \"lambda\", \"mu\", \"nu\", \"rho\", \"phi\", \"chi\", \"eta\", \"omega\", \"Unknown\"])\n",
    "\n",
    "comparison_matrix = np.zeros((len(np.unique(new_classification)), len(Belloni_classes)), dtype=int)\n",
    "\n",
    "comparison_matrix_df = pd.DataFrame(comparison_matrix, columns=Belloni_classes, index=np.unique(new_classification))\n",
    "\n",
    "for n_Bc, Belloni_class in enumerate(Belloni_classes):\n",
    "    Belloni_class_indices = np.where(np.array(segment_class) == Belloni_class)[0]\n",
    "    count_clusters_for_class = np.unique(np.take(new_classification, Belloni_class_indices), return_counts=True)\n",
    "    for cluster_ind, cluster in enumerate(count_clusters_for_class[0]):\n",
    "        comparison_matrix_df[Belloni_class][cluster] = count_clusters_for_class[1][cluster_ind]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_normalized_comparison_matrix_df=(comparison_matrix_df-comparison_matrix_df.min())/(comparison_matrix_df.max()-comparison_matrix_df.min())\n",
    "known_comparison_matrix_df = comparison_matrix_df.drop(columns=['Unknown']).T\n",
    "component_normalized_comparison_matrix_df = (known_comparison_matrix_df-known_comparison_matrix_df.min())/(known_comparison_matrix_df.max()-known_comparison_matrix_df.min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find particularly class-homogeneous components\n",
    "\n",
    "good_comps = {}\n",
    "for comp in range(122):\n",
    "    comp_counts = comparison_matrix_df.iloc[comp,:].sort_values(ascending=False)\n",
    "    comp_counts_nonzero = comp_counts.where(comp_counts>0).dropna().astype(int)\n",
    "    if comp_counts_nonzero.index[0] == \"Unknown\":\n",
    "        comp_class_proportion = comp_counts_nonzero[1:]/comp_counts_nonzero[1:].sum()\n",
    "        if comp_class_proportion[0]>0.999:\n",
    "            good_comps[comp] = comp_counts_nonzero\n",
    "    else:\n",
    "        good_comps[comp] = comp_counts_nonzero\n",
    "print(len(good_comps))\n",
    "# dominated_comps = []\n",
    "# for i,v in good_comps.items():\n",
    "#     if v.index[1] == \"chi\":\n",
    "#         dominated_comps.append(i)\n",
    "# print(dominated_comps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chi is >95% bar unknowns : (10), 3, 5, 7, 15, 24, 28, 37, 44, 53, 57, 60, 69, 76, 91, 103, 112\n",
    "# kappa : (79),  4, 42, 78, 104 (87 has 90.8% kappa)\n",
    "# theta : 25, 26, 71\n",
    "# lambda : 66\n",
    "# rho : 12, 18, 19, 20, 22, 23, 30, 38, 45, 49, 50, 54, 56, 63, 65, 74, 77, 86, 97, 107\n",
    "\n",
    "print(good_comps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comparison_matrix_df.iloc[98]#/(comparison_matrix_df.iloc[87].sum()-2628)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comp_list = []\n",
    "for i,v in good_comps.items():\n",
    "    if len(v)>1:\n",
    "        comp_list.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(comp_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comp_counts = comparison_matrix_df.iloc[7,:].sort_values(ascending=False)\n",
    "comp_counts_nonzero = comp_counts.where(comp_counts>0).dropna().astype(int)\n",
    "comp_counts_nonzero[1:]/comp_counts_nonzero[1:].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comparison_matrix_df.sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comparison_matrix_df.T.iloc[:,66]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(0.002524/1.002524)*7617"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(shape_moments_GM114_labels, return_counts=True)[1][10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.where(comparison_matrix_df.T.idxmax().values != \"Unknown\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = (6.97, 6.97*(1/3))\n",
    "import seaborn as sns\n",
    "\n",
    "from matplotlib.ticker import (MultipleLocator, FormatStrFormatter,\n",
    "                               AutoMinorLocator)\n",
    "\n",
    "\n",
    "\n",
    "ax = sns.heatmap(class_normalized_comparison_matrix_df.T, xticklabels=True, yticklabels=True, cmap='coolwarm')#, linewidth=0.5)\n",
    "# # ax.set_xticklabels(ax.get_xmajorticklabels(), fontsize = 6)\n",
    "# ax.tick_params(axis=\"x\", which=\"major\", length=2, width=0.75, labelsize=6, direction=\"out\")\n",
    "\n",
    "ax.xaxis.set_major_locator(MultipleLocator(4))\n",
    "ax.xaxis.set_major_formatter(FormatStrFormatter('%d'))\n",
    "\n",
    "# For the minor ticks, use no labels; default NullFormatter.\n",
    "# ax.xaxis.set_minor_locator(MultipleLocator(1.5))\n",
    "ax.xaxis.set_minor_locator(AutoMinorLocator())\n",
    "\n",
    "\n",
    "# for label in ax.xaxis.get_majorticklabels():\n",
    "#     label.set_transform(label.get_transform() + 0.5)\n",
    "    \n",
    "# for label in ax.xaxis.get_minorticklabels():\n",
    "#     label.set_transform(label.get_transform() + 0.5)\n",
    "\n",
    "# plt.title(\"Gaussian mixture components' populations in terms of classified data (component-wise min-maxed)\")\n",
    "plt.savefig(\"figures/GMM122vsBelloni_heatmap.png\", dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualise Gaussian components' populations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GMmodel_dir = \"{}/GMM_122comp_20d_alldata_model_2020-08-30_11-42-38.pkl\".format(data_dir)\n",
    "with open(GMmodel_dir, 'rb') as f:\n",
    "    GMmodel122 = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"{}/shape_moments_GMM122_labels.pkl\".format(data_dir), 'rb') as f: # output of LSTM autoencoder's decoder\n",
    "    shape_moments_GMM122_labels = pickle.load(f)\n",
    "    \n",
    "# load light curve segments\n",
    "with open('{}/468202_len128_s2_4cad_counts_errorfix.pkl'.format(data_dir), 'rb') as f:\n",
    "    segments_counts = pickle.load(f)\n",
    "    \n",
    "# segments_dir = '{}/468202_len128_s2_4cad_counts_errorfix.pkl'.format(data_dir)\n",
    "# errors_dir = '{}/468202_len128_s2_4cad_errors_errorfix.pkl'.format(data_dir)\n",
    "recos_dir = \"{}/reconstructions_from_model_2020-08-30_11-42-38.pkl\".format(data_dir)\n",
    "\n",
    "# with open(segments_dir, 'rb') as f:\n",
    "#     segments = pickle.load(f)\n",
    "# with open(errors_dir, 'rb') as f:\n",
    "#     errors = pickle.load(f)\n",
    "with open(recos_dir, 'rb') as f:\n",
    "    segment_reconstructions = pickle.load(f)\n",
    "\n",
    "segment_reconstructions= segment_reconstructions*np.std(segments_counts, axis=1) + np.mean(segments_counts, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.max(shape_moments[:,-4:], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "plt.close()\n",
    "\n",
    "# counts_todo = [i for i,v in class_normalized_comparison_matrix_df[\"omega\"].sort_values(ascending=False)[:10].items() if v>=0.5]\n",
    "\n",
    "# if len(counts_todo) > 3:\n",
    "#     how_many_to_plot = 10\n",
    "# else:\n",
    "how_many_to_plot = 15\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (3.32*3,3.32*(2)*3)\n",
    "plt.rcParams.update({'font.size': 6})\n",
    "\n",
    "component_counts = np.unique(shape_moments_GMM122_labels, return_counts=1)\n",
    "\n",
    "max_stats = np.max(shape_moments[:,-4:],axis=0)\n",
    "min_stats = np.min(shape_moments[:,-4:],axis=0)\n",
    "\n",
    "for plot_component in [43,88,89,98,114]:#range(9):\n",
    "    fig, axes = plt.subplots(nrows=how_many_to_plot, ncols=3)\n",
    "    component_segment_indices = np.where(shape_moments_GMM122_labels == plot_component)[0]\n",
    "    for plot_ind in range(how_many_to_plot):\n",
    "        data = segments_counts[component_segment_indices[plot_ind]]\n",
    "        reconstruction = segment_reconstructions[component_segment_indices[plot_ind]]\n",
    "        axes[plot_ind,0].plot(np.array(list(range(128)))*4,data/1000)#, c=\"green\", linewidth=0.5, zorder=-5)\n",
    "        axes[plot_ind,0].plot(np.array(list(range(128)))*4, reconstruction/1000)\n",
    "        axes[plot_ind,1].plot(np.array(list(range(128)))*4,data/1000)#, c=\"green\", linewidth=0.5, zorder=-5)\n",
    "        axes[plot_ind,1].plot(np.array(list(range(128)))*4, reconstruction/1000)\n",
    "    #     axes[subplot, 1].plot(np.array(list(range(128)))*4,segments_counts[class_segments[subplot]])\n",
    "    #     axes[subplot, 1].plot(np.array(list(range(128)))*4, reconstruction*np.std(data)+np.mean(data))\n",
    "        axes[plot_ind, 1].set_ylim([0, 12])\n",
    "        axes[plot_ind, 1].yaxis.tick_right()\n",
    "        \n",
    "#         bar_heights=np.zeros(4)\n",
    "#         bar_heights[0] = np.mean(data)\n",
    "#         bar_heights[1] = np.std(data)\n",
    "#         bar_heights[2] = stats.skew(data)\n",
    "#         bar_heights[3] = stats.kurtosis(data)\n",
    "\n",
    "#         color = [\"red\" if x else \"blue\" for x in shape_moments[component_segment_indices[plot_ind],-4:]<0]\n",
    "        bar_heights = (shape_moments[component_segment_indices[plot_ind],-4:]-min_stats)/ (max_stats-min_stats)\n",
    "        axes[plot_ind, 2].bar(x=range(4), height=bar_heights)#, color=color)\n",
    "        axes[plot_ind, 2].set_ylim([0,1])#[np.min(shape_moments[:,-4:]), np.max(shape_moments[:,-4:])])\n",
    "\n",
    "    #     if class_name == \"Unknown\": \n",
    "    #         class_name = \"??\"\n",
    "    #     else:\n",
    "    #         class_name = r\"$\\{}$\".format(class_name)\n",
    "    #     axes[subplot, 1].text(x=510, y=9500, s=class_name)\n",
    "\n",
    "        if plot_ind == 2:\n",
    "            axes[plot_ind,0].set_ylabel(\"Rate (kcounts/s)\", size=6)\n",
    "        if plot_ind == 4:#plot_ind == 12 or plot_ind == 13\n",
    "            axes[plot_ind,0].tick_params(axis=\"x\", which=\"major\", length=2, width=0.75, labelsize=6, direction=\"in\")\n",
    "            axes[plot_ind,1].tick_params(axis=\"x\", which=\"major\", length=2, width=0.75, labelsize=6, direction=\"in\")\n",
    "            axes[plot_ind,0].set_xlabel(\"Time (s)\", size=6, x=1)\n",
    "\n",
    "        else:\n",
    "            axes[plot_ind,0].tick_params(axis=\"x\", which=\"major\", length=2, width=0.75, labelsize=0, direction=\"in\")\n",
    "            axes[plot_ind,1].tick_params(axis=\"x\", which=\"major\", length=2, width=0.75, labelsize=0, direction=\"in\")\n",
    "\n",
    "    plt.suptitle(\"component {}, population {}\".format(plot_component, len(component_segment_indices)), y=1.01)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GMmodel122.covariances_[[0,1,2]].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "filter_indices = [0,32,40,50,60,73,100,103,110,113,118]\n",
    "for ind, component in enumerate(GMmodel122.covariances_[filter_indices]):\n",
    "#     print(GMmodel122.means_[filter_indices][ind])\n",
    "    plt.bar(x=np.concatenate((np.array(range(16)), np.array(range(4))+17)), height= np.diagonal(component))\n",
    "    plt.title(\"{}\".format(filter_indices[ind]))\n",
    "    plt.ylim((0,3))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reducer = umap.UMAP(random_state=42)\n",
    "reducer.fit(GMmodel122.means_)\n",
    "\n",
    "\n",
    "embedding = reducer.transform(GMmodel122.means_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = (3.32,3.32)\n",
    "\n",
    "zorders = [-1]*122\n",
    "labels = [\"Other\"]*122\n",
    "\n",
    "s=5\n",
    "\n",
    "plt.scatter(embedding[:,0], embedding[:,1], c=\"black\", s=s)\n",
    "\n",
    "\n",
    "filter_indices = [6,10,16,21,22,24,37,38,41,45,56,57,59,63,79,81,82,83,90,94,96,105,108,115,119,121]# regular peaks\n",
    "\n",
    "plt.scatter(embedding[filter_indices,0], embedding[filter_indices,1], c=\"orange\", label= \"periodic flare\", s=s)\n",
    "\n",
    "\n",
    "colour_filter = [\"orange\" if x in filter_indices else \"black\" for x in range(122)]\n",
    "zorders = [1 if ind in filter_indices else x for ind, x in enumerate(zorders)]\n",
    "labels = [\"Periodic flare\" if ind in filter_indices else x for ind, x in enumerate(zorders)]\n",
    "\n",
    "filter_indices = [0,32,40,50,60,73,100,103,110,113,118]# mid-flats\n",
    "\n",
    "plt.scatter(embedding[filter_indices,0], embedding[filter_indices,1], c=\"magenta\", label= \"mid random\", s=s)\n",
    "\n",
    "\n",
    "colour_filter = [\"magenta\" if ind in filter_indices else x for ind, x in enumerate(colour_filter)]\n",
    "zorders = [1 if ind in filter_indices else x for ind, x in enumerate(zorders)]\n",
    "labels = [\"Mid random\" if ind in filter_indices else x for ind, x in enumerate(zorders)]\n",
    "\n",
    "filter_indices = [1,2,4,5,7,12,14,18,27, 30,31,47,49,51,54,67,71,76,80,99,106,109]# low-flats\n",
    "\n",
    "plt.scatter(embedding[filter_indices,0], embedding[filter_indices,1], c=\"red\", label= \"low random\", s=s)\n",
    "\n",
    "\n",
    "filter_indices = [8,11, 52,66,68,117]# low-flats\n",
    "\n",
    "plt.scatter(embedding[filter_indices,0], embedding[filter_indices,1], c=\"cyan\", label= \"+ve gradient\", s=s)\n",
    "\n",
    "filter_indices = [43,88,89,98,114]\n",
    "plt.scatter(embedding[filter_indices,0], embedding[filter_indices,1], c=\"khaki\", label= \"\\'square wave\\'\", s=s)\n",
    "\n",
    "colour_filter = [\"cyan\" if ind in filter_indices else x for ind, x in enumerate(colour_filter)]\n",
    "zorders = [1 if ind in filter_indices else x for ind, x in enumerate(zorders)]\n",
    "labels = [\"Low random\" if ind in filter_indices else x for ind, x in enumerate(zorders)]\n",
    "\n",
    "plt.xlabel(\"UMAP axis 1\")\n",
    "plt.ylabel(\"UMAP axis 2\")\n",
    "\n",
    "# filter_indices = [19,34,39,70]# irregular peaks\n",
    "\n",
    "# colour_filter = [\"red\" if ind in filter_indices else x for ind, x in enumerate(colour_filter)]\n",
    "\n",
    "# plt.scatter(embedding[:,0], embedding[:,1], c=colour_filter)\n",
    "plt.legend()\n",
    "# plt.savefig('figures/UMAP_component_means.png', dpi=300, bbox_inches = 'tight',pad_inches = 0.01)\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len((0,32,40,50,60,73,100,103,110,113,118))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "means = []\n",
    "for plot_component in (1,2,4,5,7,12,14,18,27, 30,31,47,49,51,54,67,71,76,80,99,106,109,111,112,116):#range(9):\n",
    "    component_segment_indices = np.where(shape_moments_GMM122_labels == plot_component)[0]\n",
    "    mean = np.mean(segments_counts[component_segment_indices], axis=1)\n",
    "    means = np.concatenate((means, mean.flatten()))\n",
    "    \n",
    "means2 = []\n",
    "for plot_component in (0,32,40,50,60,73,100,103,110,113,118):#range(9):\n",
    "    component_segment_indices = np.where(shape_moments_GMM122_labels == plot_component)[0]\n",
    "    mean = np.mean(segments_counts[component_segment_indices], axis=1)\n",
    "    means2 = np.concatenate((means2, mean.flatten()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(means)\n",
    "plt.hist(means2, alpha=0.5)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(means2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build a dataframe containing counts of light curve segments in each Gaussian mixture component for each observation\n",
    "intended shape of the data frame: [1738, 122]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with open(\"{}/ObID_GaussComps_dict_122_chrono.pkl\".format(data_dir), 'rb') as f: # same as ObID_GaussComps_dict_122 below, but segments are in chronological order\n",
    "    ObID_GaussComps_dict_122_chrono = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"{}/shape_moments_GMM122_labels.pkl\".format(data_dir), 'rb') as f: # output of LSTM autoencoder's decoder\n",
    "    shape_moments_GMM122_labels = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load (shuffled) list of unique IDs for the 468202 light curve segments. Format: observationID_segmentIndex,\n",
    "# i.e. ['96701-01-48-00_3','20402-01-02-02_122','70703-01-01-000_1420', ...]\n",
    "with open('{}/468202_len128_s2_4cad_ids_errorfix.pkl'.format(data_dir), 'rb') as f:\n",
    "    seg_ids = pickle.load(f)\n",
    "\n",
    "# make a list of observation ids; for each of the 468202 segments, get the ID of the observation that it comes from\n",
    "#i.e. ['96701-01-48-00','20402-01-02-02', '70703-01-01-000',\n",
    "seg_ObIDs = [seg.split(\"_\")[0] for seg in seg_ids] # gets rid of the within-observation segment indices and creates a degenerate list of observation IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a dict that groups indices of segments of the same observation \n",
    "# i.e. where each observation id can be found in seg_ObIDs\n",
    "#i.e. ObID_SegIndices_dict == {'10258-01-01-00': [916, 949, 1046...467528, 467578], ....}\n",
    "ObID_SegIndices_dict = {key:[] for key in np.unique(seg_ObIDs)}\n",
    "for ID_index, ObID in enumerate(seg_ObIDs):\n",
    "    ObID_SegIndices_dict.setdefault(ObID, []).append(ID_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a dictionary of Gaussian component labels instead of segment indices  \n",
    "#i.e. ObID_GaussComps_dict_500 == {'10258-01-01-00': [401, 433, 382...101, 152], ....}\n",
    "ObID_GaussComps_dict_122 = {}\n",
    "for ObID, Indices in ObID_SegIndices_dict.items():\n",
    "    ObID_GaussComps_dict_122[ObID] = [shape_moments_GMM122_labels[ind] for ind in Indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a data frame containing the counts of light curve segments in each of the Gaussian components, for each observation\n",
    "obs_component_counts_df_122 = pd.DataFrame(np.zeros((len(ObID_GaussComps_dict_122),len(np.unique(shape_moments_GMM122_labels)))), index=np.unique(seg_ObIDs), columns=list(range(122)), dtype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# populate the data frame\n",
    "for ObID, GaussComps in ObID_GaussComps_dict_122.items():\n",
    "    for comp_id, comp_count in np.array(np.unique(GaussComps, return_counts=True)).T:\n",
    "        obs_component_counts_df_122.loc[ObID][comp_id] = comp_count\n",
    "obs_component_counts_df_122"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sanity check; count of the number of segments in each Gaussian component, as returned by the Gaussian_mixture_model.predict() method,\n",
    "#matches the  count in the columns of the data frame, so we transferred the counts from list to dataframe successfully\n",
    "print((np.unique(shape_moments_GMM122_labels, return_counts= True)[1] == obs_component_counts_df_122.sum().values).all())\n",
    "#total number of segment counts matches the size of our dataset\n",
    "print(obs_component_counts_df_122.sum().sum())\n",
    "#each component has at least 59 counts in it, so I didn't mis-name \"shape_moments_GM114_labels\" as \"shape_moments_GM500_labels\" \n",
    "print(obs_component_counts_df_122.sum().values.min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply min-max normalisation to values in each row of the data frame\n",
    "from sklearn.preprocessing import Normalizer\n",
    "normalized_obs_component_counts_df_122 = pd.DataFrame(Normalizer(norm='max').fit_transform(obs_component_counts_df_122), index=np.unique(seg_ObIDs), columns=list(range(122)))\n",
    "normalized_obs_component_counts_df_122"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apply center log ratio transformation to the counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats.mstats import gmean\n",
    "# multiply each value by 10 to set true counts apart from pseudo-counts, then replace all zeros with 0.5 (half of the lowest non-zero value)\n",
    "pseudo_counts = (obs_component_counts_df_122).replace(to_replace=0, value=0.5) \n",
    "# apply closure normalisation; each row should sum up to 1\n",
    "closed_pseudo_counts = pseudo_counts.div(np.sum(pseudo_counts, axis=1), axis=\"rows\")\n",
    "#divide each row by its geometric mean and take a natural log. this is the CLR transformation, inverse of a softmax\n",
    "scaled_pseudo_counts = closed_pseudo_counts.div(gmean(closed_pseudo_counts, axis=1), axis=\"rows\")\n",
    "pseudo_CLR_transformed = np.log(scaled_pseudo_counts) # pseudo-counts are now negative, and real counts positive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pseudo_CLR_transformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hdbscan\n",
    "clusterer = hdbscan.HDBSCAN(min_cluster_size=20)\n",
    "# clusterer.fit(pseudo_CLR_transformed)\n",
    "\n",
    "# clusterer.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reducer = umap.UMAP(random_state=2, n_components=12, n_neighbors=10, min_dist=0.)#300)\n",
    "reducer.fit(pseudo_CLR_transformed)\n",
    "embedding = reducer.transform(pseudo_CLR_transformed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusterer.fit(embedding)\n",
    "\n",
    "np.unique(clusterer.labels_, return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reducer2 = umap.UMAP(random_state=2, n_components=2, n_neighbors=10, min_dist=0.)#300)\n",
    "reducer2.fit(embedding)\n",
    "embedding = reducer.transform(embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_inds = np.where(clusterer.labels_==-1)[0]\n",
    "cluster_data = np.take(embedding,cluster_inds, axis=0)\n",
    "plt.scatter(cluster_data[:,0], cluster_data[:,1], c=\"black\", s=0.5)\n",
    "for cluster in np.unique(clusterer.labels_)[1:]:\n",
    "    cluster_inds = np.where(clusterer.labels_==cluster)[0]\n",
    "    cluster_data = np.take(embedding,cluster_inds, axis=0)\n",
    "    plt.scatter(cluster_data[:,0], cluster_data[:,1], s=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats.mstats import gmean\n",
    "# multiply each value by 10 to set true counts apart from pseudo-counts, then replace all zeros with 0.5 (half of the lowest non-zero value)\n",
    "pseudo_counts = (obs_component_counts_df_122).replace(to_replace=0, value=0.5) \n",
    "# apply closure normalisation; each row should sum up to 1\n",
    "closed_pseudo_counts = pseudo_counts.div(np.sum(pseudo_counts, axis=1), axis=\"rows\")\n",
    "#divide each row by its geometric mean and take a natural log. this is the CLR transformation, inverse of a softmax\n",
    "scaled_pseudo_counts = closed_pseudo_counts.div(gmean(closed_pseudo_counts, axis=1), axis=\"rows\")\n",
    "pseudo_CLR_transformed = np.log(scaled_pseudo_counts) # pseudo-counts are now negative, and real counts positive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_component_counts_df_122"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = (10.0, 5.0)\n",
    "plt.plot((obs_component_counts_df_122 == 0).astype(int).sum(axis=0).values)\n",
    "plt.ylim((0,1738))\n",
    "plt.ylabel(\"Number of observations with a zero\")\n",
    "plt.xlabel(\"Feature ID\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# multiplicative strategy from Martin-Fernandez\n",
    "closed_df = obs_component_counts_df_122.div(np.sum(obs_component_counts_df_122, axis=1), axis=\"rows\")\n",
    "deltaj = 0.65*closed_df[closed_df>0].min(skipna=True).min()\n",
    "sum_no_zeros_in_rows = (obs_component_counts_df_122 == 0).astype(int).sum(axis=1)\n",
    "scaled_nonzero = closed_df.multiply((1-sum_no_zeros_in_rows*deltaj), axis=\"rows\")\n",
    "zeros_replaced =scaled_nonzero.replace(to_replace=0, value=deltaj)\n",
    "scaled_xs = zeros_replaced.div(gmean(zeros_replaced, axis=1), axis=\"rows\")\n",
    "pseudo_CLR_transformed_x = np.log(scaled_xs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# not sure\n",
    "deltaj = 0.65/obs_component_counts_df_122.iloc[0,:].sum()\n",
    "x = obs_component_counts_df_122.div(np.sum(obs_component_counts_df_122, axis=1), axis=\"rows\")\n",
    "sum_no_zeros_in_rows = (x == 0).astype(int).sum(axis=1)\n",
    "x = x.multiply((1-sum_no_zeros_in_rows*deltaj), axis=\"rows\")\n",
    "x=x.replace(to_replace=0, value=deltaj)\n",
    "scaled_xs = closed_pseudo_counts.div(gmean(x, axis=1), axis=\"rows\")\n",
    "pseudo_CLR_transformed_x = np.log(scaled_xs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.close()\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (30,30)\n",
    "plt.rcParams.update({'font.size': 10})\n",
    "\n",
    "fig, axes = plt.subplots(nrows=5, ncols=5, sharex=True, sharey=True)\n",
    "\n",
    "tril = np.array(np.tril_indices(5)).T\n",
    "\n",
    "data = pseudo_CLR_transformed_x\n",
    "\n",
    "for x,y in tril:\n",
    "    axes[x,y].scatter(data.iloc[:,x],data.iloc[:,y])\n",
    "#     axes[x,y].set_ylim((-0.1,1))\n",
    "#     axes[x,y].set_xlim((-0.1,1))\n",
    "    axes[x,y].text(0.99, 0.99, np.round(np.corrcoef(data.iloc[:,x],data.iloc[:,y])[0,1], decimals=3), ha='right', va='top', transform=axes[x,y].transAxes, size=30)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.close()\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (30,30)\n",
    "plt.rcParams.update({'font.size': 10})\n",
    "\n",
    "fig, axes = plt.subplots(nrows=5, ncols=5, sharex=True, sharey=True)\n",
    "\n",
    "tril = np.array(np.tril_indices(5)).T\n",
    "\n",
    "data = obs_component_counts_df_122\n",
    "\n",
    "for x,y in tril:\n",
    "    axes[x,y].scatter(data.iloc[:,x],data.iloc[:,y])\n",
    "#     axes[x,y].set_ylim((-0.1,1))\n",
    "#     axes[x,y].set_xlim((-0.1,1))\n",
    "    axes[x,y].text(0.99, 0.99, np.round(np.corrcoef(data.iloc[:,x],data.iloc[:,y])[0,1], decimals=3), ha='right', va='top', transform=axes[x,y].transAxes, size=30)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reducer = umap.UMAP(random_state=2, n_components=20, n_neighbors=300)#300)\n",
    "reducer.fit(pseudo_CLR_transformed_x)\n",
    "embedding = reducer.transform(pseudo_CLR_transformed_x)\n",
    "\n",
    "plt.rcParams['figure.figsize'] = [5,5]\n",
    "\n",
    "plt.scatter(embedding[:,0], embedding[:,1], c=\"black\", s=s)\n",
    "plt.show()\n",
    "\n",
    "data = embedding\n",
    "metric = \"euclidean\"\n",
    "\n",
    "linkage = sch.linkage(data, method=\"ward\", metric=metric)\n",
    "plt.rcParams['figure.figsize'] = (6.97, 3)\n",
    "plt.rcParams.update({'font.size': 6})\n",
    "\n",
    "dendrogram = sch.dendrogram(linkage, truncate_mode=None, p=1, leaf_font_size = 0)\n",
    "axis = plt.gca()\n",
    "# axis.tick_params(axis=\"x\", which=\"major\", length=0, width=0, labelsize=0, direction=\"in\")\n",
    "axis.set_xticks([])\n",
    "axis.set_xticklabels([])\n",
    "axis.set_ylabel(\"euclidean distance\", size=6, x=1)\n",
    "\n",
    "# plt.savefig('figures/agglomerative_dendogram.png', dpi=300, bbox_inches = 'tight',pad_inches = 0)\n",
    "\n",
    "\n",
    "plt.show()\n",
    "\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.metrics import calinski_harabasz_score\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (3.32, 3.32*(1/1))\n",
    "\n",
    "\n",
    "y_top_limit = linkage[-1][2]\n",
    "y_bottom_limit = linkage[0][2]\n",
    "\n",
    "no_clusters = []\n",
    "silhouette_scores = []\n",
    "for distance in np.linspace(y_bottom_limit,y_top_limit,1000)[:-1]:\n",
    "    clusters = sch.fcluster(linkage, distance, criterion='distance')\n",
    "    silhouette_scores.append(calinski_harabasz_score(data, clusters))#, metric=metric, random_state=0))\n",
    "    no_clusters.append(len(np.unique(clusters)))\n",
    "\n",
    "fig, ax1 = plt.subplots()\n",
    "\n",
    "\n",
    "plt.rcParams['figure.figsize'] = [5,5]\n",
    "# plt.axhline(114, c=\"cyan\")\n",
    "# ax1.axvline(4.84678277, c=\"black\")\n",
    "# ax1.axvline(4.291764, c=\"black\")\n",
    "# ax1.axhline(60, c=\"black\")\n",
    "ax1.plot(np.linspace(y_bottom_limit,y_top_limit,1000)[:-1], no_clusters, c=\"magenta\")\n",
    "\n",
    "ax2 = ax1.twinx()\n",
    "ax2.plot(np.linspace(y_bottom_limit,y_top_limit,1000)[:-1], silhouette_scores, c=\"cyan\")\n",
    "# plt.title(\"No. of clusters as the function of distance for hierarchical clustering\")\n",
    "# ax3 = ax1.twinx()\n",
    "# ax3.plot(np.linspace(0,y_limit,1000)[:-1], calinski_harabasz_scores, c=\"orange\")\n",
    "# ax4 = ax1.twinx()\n",
    "# ax4.plot(np.linspace(0,y_limit,1000)[:-2], np.diff(silhouette_scores, 1), c=\"green\")\n",
    "\n",
    "ax1.set_ylabel(\"No. clusters\", c=\"magenta\")\n",
    "ax2.set_ylabel(\"Silhouette score\", c=\"cyan\")\n",
    "ax1.set_xlabel(\"Distance threshold\")\n",
    "# plt.ylim((0,50))\n",
    "# plt.savefig('figures/agglomerative_silhouette.png', dpi=300, bbox_inches = 'tight',pad_inches = 0)\n",
    "print(\"Best 20 cluster numbers: \", np.array(no_clusters)[np.argsort(silhouette_scores)[-20:]])\n",
    "print(\"Best 20 distances: \", np.array(np.linspace(y_bottom_limit,y_top_limit,1000)[:-1])[np.argsort(silhouette_scores)[-20:]])\n",
    "from kneed import KneeLocator\n",
    "kn = KneeLocator(np.linspace(y_bottom_limit,y_top_limit,1000)[:-1], no_clusters, curve='convex', direction='decreasing')\n",
    "print(np.unique(sch.fcluster(linkage, kn.knee, criterion='distance'), return_counts=False).shape)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reducer = umap.UMAP(random_state=0, n_components=2, n_neighbors=15)#300)\n",
    "# reducer.fit(pseudo_CLR_transformed)\n",
    "# embedding = reducer.transform(pseudo_CLR_transformed)\n",
    "\n",
    "# plt.rcParams['figure.figsize'] = [5,5]\n",
    "\n",
    "# plt.scatter(embedding[:,0], embedding[:,1], c=\"black\", s=s)\n",
    "# plt.show()\n",
    "\n",
    "data = pseudo_CLR_transformed\n",
    "metric = \"euclidean\"\n",
    "\n",
    "linkage = sch.linkage(data, method=\"ward\", metric=metric)\n",
    "plt.rcParams['figure.figsize'] = (6.97, 3)\n",
    "plt.rcParams.update({'font.size': 6})\n",
    "\n",
    "dendrogram = sch.dendrogram(linkage, truncate_mode=None, p=1, leaf_font_size = 0)\n",
    "axis = plt.gca()\n",
    "# axis.tick_params(axis=\"x\", which=\"major\", length=0, width=0, labelsize=0, direction=\"in\")\n",
    "axis.set_xticks([])\n",
    "axis.set_xticklabels([])\n",
    "axis.set_ylabel(\"euclidean distance\", size=6, x=1)\n",
    "\n",
    "# plt.savefig('figures/agglomerative_dendogram.png', dpi=300, bbox_inches = 'tight',pad_inches = 0)\n",
    "\n",
    "\n",
    "plt.show()\n",
    "\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.metrics import calinski_harabasz_score\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (3.32, 3.32*(1/1))\n",
    "\n",
    "\n",
    "y_top_limit = linkage[-1][2]\n",
    "y_bottom_limit = linkage[0][2]\n",
    "\n",
    "no_clusters = []\n",
    "silhouette_scores = []\n",
    "for distance in np.linspace(y_bottom_limit,y_top_limit,1000)[:-1]:\n",
    "    clusters = sch.fcluster(linkage, distance, criterion='distance')\n",
    "    silhouette_scores.append(calinski_harabasz_score(data, clusters))#, metric=metric, random_state=0))\n",
    "    no_clusters.append(len(np.unique(clusters)))\n",
    "\n",
    "fig, ax1 = plt.subplots()\n",
    "\n",
    "\n",
    "plt.rcParams['figure.figsize'] = [5,5]\n",
    "# plt.axhline(114, c=\"cyan\")\n",
    "# ax1.axvline(4.84678277, c=\"black\")\n",
    "# ax1.axvline(4.291764, c=\"black\")\n",
    "# ax1.axhline(60, c=\"black\")\n",
    "ax1.plot(np.linspace(y_bottom_limit,y_top_limit,1000)[:-1], no_clusters, c=\"magenta\")\n",
    "\n",
    "ax2 = ax1.twinx()\n",
    "ax2.plot(np.linspace(y_bottom_limit,y_top_limit,1000)[:-1], silhouette_scores, c=\"cyan\")\n",
    "# plt.title(\"No. of clusters as the function of distance for hierarchical clustering\")\n",
    "# ax3 = ax1.twinx()\n",
    "# ax3.plot(np.linspace(0,y_limit,1000)[:-1], calinski_harabasz_scores, c=\"orange\")\n",
    "# ax4 = ax1.twinx()\n",
    "# ax4.plot(np.linspace(0,y_limit,1000)[:-2], np.diff(silhouette_scores, 1), c=\"green\")\n",
    "\n",
    "ax1.set_ylabel(\"No. clusters\", c=\"magenta\")\n",
    "ax2.set_ylabel(\"Silhouette score\", c=\"cyan\")\n",
    "ax1.set_xlabel(\"Distance threshold\")\n",
    "# plt.ylim((0,50))\n",
    "# plt.savefig('figures/agglomerative_silhouette.png', dpi=300, bbox_inches = 'tight',pad_inches = 0)\n",
    "print(\"Best 20 cluster numbers: \", np.array(no_clusters)[np.argsort(silhouette_scores)[-20:]])\n",
    "print(\"Best 20 distances: \", np.array(np.linspace(y_bottom_limit,y_top_limit,1000)[:-1])[np.argsort(silhouette_scores)[-20:]])\n",
    "from kneed import KneeLocator\n",
    "kn = KneeLocator(np.linspace(y_bottom_limit,y_top_limit,1000)[:-1], no_clusters, curve='convex', direction='decreasing')\n",
    "print(np.unique(sch.fcluster(linkage, kn.knee, criterion='distance'), return_counts=False).shape)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(sch.fcluster(linkage, kn.knee, criterion='distance'), return_counts=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kn.knee"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.metrics import silhouette_score\n",
    "# from sklearn.metrics import calinski_harabasz_score\n",
    "# plt.rcParams['figure.figsize'] = (3.32, 3.32*(1/1))\n",
    "# y_top_limit = linkage[-1][2]\n",
    "# y_bottom_limit = linkage[0][2]\n",
    "# # no_clusters = []\n",
    "# # silhouette_scores0 = []\n",
    "# # for distance in np.linspace(y_bottom_limit,y_top_limit,1000)[:-1]:\n",
    "# #     clusters = sch.fcluster(linkage, distance, criterion='distance')\n",
    "# #     silhouette_scores0.append(silhouette_score(data, clusters))#, metric=metric, random_state=0))\n",
    "# #     no_clusters.append(len(np.unique(clusters)))\n",
    "\n",
    "# fig, ax1 = plt.subplots()\n",
    "# plt.rcParams['figure.figsize'] = [3.32,3.32]\n",
    "# ax1.axvline(17.834425221673342, c=\"cyan\")\n",
    "# ax1.axhline(113, c=\"cyan\")\n",
    "\n",
    "# ax1.plot(np.linspace(y_bottom_limit,y_top_limit,1000)[:-1], no_clusters, c=\"magenta\")\n",
    "# # ax2 = ax1.twinx()\n",
    "# # ax2.plot(np.linspace(y_bottom_limit,y_top_limit,1000)[:-1], silhouette_scores0, c=\"cyan\")\n",
    "# ax1.set_ylabel(\"No. clusters\")\n",
    "# # ax2.set_ylabel(\"Silhouette score\", c=\"cyan\")\n",
    "# ax1.set_xlabel(\"Distance threshold\")\n",
    "# print(\"Best 20 cluster numbers: \", np.array(no_clusters0)[np.argsort(silhouette_scores0)[-20:]])\n",
    "# print(\"Best 20 distances: \", np.array(np.linspace(y_bottom_limit,y_top_limit,1000)[:-1])[np.argsort(silhouette_scores0)[-20:]])\n",
    "# # plt.savefig('figures/agglomerative_knee_113.png', dpi=300, bbox_inches = 'tight',pad_inches = 0.01)\n",
    "\n",
    "# plt.show()\n",
    "\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.metrics import calinski_harabasz_score\n",
    "plt.rcParams['figure.figsize'] = (3.32, 3.32*(1/1))\n",
    "y_top_limit = linkage[-1][2]\n",
    "y_bottom_limit = linkage[0][2]\n",
    "# no_clusters = []\n",
    "# silhouette_scores0 = []\n",
    "# for distance in np.linspace(y_bottom_limit,y_top_limit,1000)[:-1]:\n",
    "#     clusters = sch.fcluster(linkage, distance, criterion='distance')\n",
    "#     silhouette_scores0.append(silhouette_score(data, clusters))#, metric=metric, random_state=0))\n",
    "#     no_clusters.append(len(np.unique(clusters)))\n",
    "\n",
    "fig, ax1 = plt.subplots()\n",
    "plt.rcParams['figure.figsize'] = [3.32,3.32]\n",
    "# ax1.axvline(kn.knee, c=\"cyan\")\n",
    "# ax1.axhline(53, c=\"cyan\")\n",
    "\n",
    "ax1.axvline(kn.knee, c=\"cyan\")\n",
    "ax1.axhline(113, c=\"cyan\")\n",
    "\n",
    "ax1.text(0.90,0.6,\"Knee distance: {:.3f}\\n Knee no. clusters: {}\".format(kn.knee, np.unique(sch.fcluster(linkage, kn.knee, criterion='distance'), return_counts=False).shape[0]),\n",
    "                    ha='right', va='top', transform=ax1.transAxes, size=6)\n",
    "\n",
    "ax1.plot(np.linspace(y_bottom_limit,y_top_limit,1000)[:-1], no_clusters, c=\"magenta\")\n",
    "# ax2 = ax1.twinx()\n",
    "# ax2.plot(np.linspace(y_bottom_limit,y_top_limit,1000)[:-1], silhouette_scores0, c=\"cyan\")\n",
    "ax1.set_ylabel(\"No. clusters\")\n",
    "# ax2.set_ylabel(\"Silhouette score\", c=\"cyan\")\n",
    "ax1.set_xlabel(\"Euclidean distance threshold\")\n",
    "# print(\"Best 20 cluster numbers: \", np.array(no_clusters)[np.argsort(silhouette_scores0)[-20:]])\n",
    "# print(\"Best 20 distances: \", np.array(np.linspace(y_bottom_limit,y_top_limit,1000)[:-1])[np.argsort(silhouette_scores0)[-20:]])\n",
    "plt.savefig('figures/agglomerative_knee_113.png', dpi=300, bbox_inches = 'tight',pad_inches = 0.01)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reducer = umap.UMAP(random_state=0, min_dist=0., n_neighbors=300)\n",
    "reducer.fit(pseudo_CLR_transformed_x)\n",
    "embedding = reducer.transform(pseudo_CLR_transformed_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"UMAP of CLR transformed data, Euclidean distance\")\n",
    "plt.rcParams['figure.figsize'] = [5,5]\n",
    "\n",
    "plt.scatter(embedding[:,0], embedding[:,1], c=\"black\", s=s)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "UMAP_n_components = [2,5,10,20]\n",
    "UMAP_n_neighbors = [15, 300, 1737]\n",
    "UMAP_seeds = list(range(100))\n",
    "\n",
    "UMAP_mappers_list = []\n",
    "linkage_list = []\n",
    "knee_distance_list = []\n",
    "knee_no_cluster_list = []\n",
    "results_list = []\n",
    "\n",
    "results_list.append((\"List of hyperparameters: \", \"UMAP_n_component\", UMAP_n_components, \"UMAP_n_neightbor\", UMAP_n_neighbors, \"UMAP_seed\", UMAP_seeds, \n",
    "                     \"Saving number of clusters at knee point of Euclidean distance threshold curve for ward agglomerative clustering of UMAPed data\"))\n",
    "\n",
    "\n",
    "for UMAP_n_component in UMAP_n_components:\n",
    "    for UMAP_n_neightbor in UMAP_n_neighbors:\n",
    "        for UMAP_seed in UMAP_seeds:\n",
    "#             pca = PCA(n_components=PCA_var)\n",
    "#             pca.fit(pseudo_CLR_transformed)\n",
    "#             pca_pseudo_CLR_transformed = pca.transform(pseudo_CLR_transformed)\n",
    "            UMAP_reducer = umap.UMAP(random_state=UMAP_seed, n_neighbors=UMAP_n_neightbor,n_components=UMAP_n_component)\n",
    "            #2,min_dist=0.1, n_neighbors=1737, n_components=30, n_epochs=1000, transform_queue_size=10)\n",
    "            UMAP_reducer.fit(pseudo_CLR_transformed)\n",
    "            UMAP_embedding = UMAP_reducer.transform(pseudo_CLR_transformed)\n",
    "\n",
    "            data = UMAP_embedding\n",
    "            metric = \"euclidean\"\n",
    "            method=\"ward\"\n",
    "            linkage = sch.linkage(data, method=method, metric=metric)\n",
    "            y_top_limit = linkage[-1][2]\n",
    "            y_bottom_limit = linkage[0][2]\n",
    "            no_clusters = []\n",
    "            for distance in np.linspace(y_bottom_limit,y_top_limit,1000):\n",
    "                clusters = sch.fcluster(linkage, distance, criterion='distance')\n",
    "                no_clusters.append(len(np.unique(clusters)))\n",
    "\n",
    "            from kneed import KneeLocator\n",
    "            kn = KneeLocator(np.linspace(y_bottom_limit,y_top_limit,1000), no_clusters, curve='convex', direction='decreasing')\n",
    "            knee_no_clusters = len(np.unique(sch.fcluster(linkage, kn.knee, criterion='distance')))\n",
    "\n",
    "            results_list.append(((UMAP_n_component, UMAP_n_neightbor, UMAP_seed, knee_no_clusters)))\n",
    "            print(results_list[-1])\n",
    "                \n",
    "with open(\"UMAP_agglo_clustering_results4x3x100.pkl\", 'wb') as f:\n",
    "    pickle.dump(results_list, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "results_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"UMAP_agglo_clustering_results.pkl\", 'wb') as f:\n",
    "#     pickle.dump(results_list, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_arr[:,:,:,:].flatten().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\"UMAP_n_component\", \"UMAP_n_neightbor\", \"UMAP_seed\"), (\"UMAP_reducer\", \"linkage\", \"kn.knee\", \"knee_no_clusters\"\n",
    "# UMAP_n_components = [2,5,10,20]\n",
    "# UMAP_n_neighbors = [15, 300, 1737]\n",
    "# UMAP_seeds = list(range(10))\n",
    "\n",
    "results_arr = np.zeros((4,3,100)).flatten()\n",
    "for res_ind, res in enumerate(results_list[1:]):\n",
    "    results_arr[res_ind] = res[-1]\n",
    "#     print(res[0], res[1][-1])\n",
    "results_arr = results_arr.reshape((4,3,100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.max(results_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_this=results_arr[3,3,0,:]\n",
    "\n",
    "# UMAP_n_components = [2,5,10,20]\n",
    "# UMAP_n_neighbors = [15, 300, 1737]\n",
    "\n",
    "fig, axes = plt.subplots(nrows=2, ncols=2)\n",
    "axes = axes.flatten()\n",
    "\n",
    "for comp_ind in range(4):\n",
    "    for neigh_ind in range(3):\n",
    "        mean_no_clusters = np.mean(results_arr[comp_ind,neigh_ind,:])\n",
    "        axes[comp_ind].hist(results_arr[comp_ind,neigh_ind,:].flatten(), alpha=0.5, range=(30,100),\n",
    "                            bins=20, label=\"{} ({:.2f})\".format(UMAP_n_neighbors[neigh_ind], mean_no_clusters))#, range=(0,1737), bins=100 \n",
    "        axes[comp_ind].set_xlim((30,100))\n",
    "        axes[comp_ind].set_ylim((0,50))\n",
    "    axes[comp_ind].text(0.90,0.6,\"No. UMAP\\ncomponents: {}\".format(UMAP_n_components[comp_ind]),\n",
    "                        ha='right', va='top', transform=axes[comp_ind].transAxes, size=6)\n",
    "    axes[comp_ind].legend(loc='upper right', title=\"No. neighbours (mean)\")\n",
    "    print( np.mean(results_arr[comp_ind,:,:]))\n",
    "axes[2].set_ylabel(\"Population\", size=6, y=1)\n",
    "axes[2].set_xlabel(\"No. clusters at the knee point\", size=6, x=1)\n",
    "axes.reshape((2,2))\n",
    "plt.savefig('figures/No_cluster_histograms.png', dpi=300, bbox_inches = 'tight',pad_inches = 0.01)\n",
    "plt.show()\n",
    "    \n",
    "# plt.hist(results_arr[:,:,:,:].flatten(), bins=100, range=(0,1737))\n",
    "# plt.hist(test_this.flatten(), alpha=0.8, bins=100,range=(0,1737))\n",
    "\n",
    "\n",
    "\n",
    "# plt.show()\n",
    "# plt.hist(results_arr[:,:,:,:].flatten(), bins=100,range=(0,1737))\n",
    "# plt.hist(test_this.flatten(), alpha=0.8, bins=100,range=(0,1737))\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.cluster.hierarchy as sch\n",
    "\n",
    "\n",
    "reducer = umap.UMAP(random_state=0, n_components=20, n_neighbors=300)\n",
    "reducer.fit(pseudo_CLR_transformed)\n",
    "embedding = reducer.transform(pseudo_CLR_transformed)\n",
    "s=0.5\n",
    "\n",
    "plt.rcParams['figure.figsize'] = [5,5]\n",
    "\n",
    "plt.scatter(embedding[:,0], embedding[:,1], c=\"black\", s=s)\n",
    "plt.show()\n",
    "\n",
    "data = embedding\n",
    "metric = \"euclidean\"\n",
    "\n",
    "linkage = sch.linkage(data, method=\"ward\", metric=metric)\n",
    "plt.rcParams['figure.figsize'] = (6.97, 3)\n",
    "plt.rcParams.update({'font.size': 6})\n",
    "\n",
    "dendrogram = sch.dendrogram(linkage, truncate_mode=None, p=1, leaf_font_size = 0)\n",
    "axis = plt.gca()\n",
    "# axis.tick_params(axis=\"x\", which=\"major\", length=0, width=0, labelsize=0, direction=\"in\")\n",
    "axis.set_xticks([])\n",
    "axis.set_xticklabels([])\n",
    "axis.set_ylabel(\"euclidean distance\", size=6, x=1)\n",
    "\n",
    "# plt.savefig('figures/agglomerative_dendogram.png', dpi=300, bbox_inches = 'tight',pad_inches = 0)\n",
    "\n",
    "\n",
    "plt.show()\n",
    "\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.metrics import calinski_harabasz_score\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (3.32, 3.32*(1/1))\n",
    "\n",
    "\n",
    "y_top_limit = linkage[-1][2]\n",
    "y_bottom_limit = linkage[0][2]\n",
    "\n",
    "no_clusters = []\n",
    "silhouette_scores = []\n",
    "for distance in np.linspace(y_bottom_limit,y_top_limit,1000)[:-1]:\n",
    "    clusters = sch.fcluster(linkage, distance, criterion='distance')\n",
    "    silhouette_scores.append(calinski_harabasz_score(data, clusters))#, metric=metric, random_state=0))\n",
    "    no_clusters.append(len(np.unique(clusters)))\n",
    "\n",
    "fig, ax1 = plt.subplots()\n",
    "\n",
    "\n",
    "plt.rcParams['figure.figsize'] = [5,5]\n",
    "# plt.axhline(114, c=\"cyan\")\n",
    "# ax1.axvline(4.84678277, c=\"black\")\n",
    "# ax1.axvline(4.291764, c=\"black\")\n",
    "# ax1.axhline(60, c=\"black\")\n",
    "ax1.plot(np.linspace(y_bottom_limit,y_top_limit,1000)[:-1], no_clusters, c=\"magenta\")\n",
    "\n",
    "ax2 = ax1.twinx()\n",
    "ax2.plot(np.linspace(y_bottom_limit,y_top_limit,1000)[:-1], silhouette_scores, c=\"cyan\")\n",
    "# plt.title(\"No. of clusters as the function of distance for hierarchical clustering\")\n",
    "# ax3 = ax1.twinx()\n",
    "# ax3.plot(np.linspace(0,y_limit,1000)[:-1], calinski_harabasz_scores, c=\"orange\")\n",
    "# ax4 = ax1.twinx()\n",
    "# ax4.plot(np.linspace(0,y_limit,1000)[:-2], np.diff(silhouette_scores, 1), c=\"green\")\n",
    "\n",
    "ax1.set_ylabel(\"No. clusters\", c=\"magenta\")\n",
    "ax2.set_ylabel(\"Silhouette score\", c=\"cyan\")\n",
    "ax1.set_xlabel(\"Distance threshold\")\n",
    "# plt.ylim((0,50))\n",
    "# plt.savefig('figures/agglomerative_silhouette.png', dpi=300, bbox_inches = 'tight',pad_inches = 0)\n",
    "print(\"Best 20 cluster numbers: \", np.array(no_clusters)[np.argsort(silhouette_scores)[-20:]])\n",
    "print(\"Best 20 distances: \", np.array(np.linspace(y_bottom_limit,y_top_limit,1000)[:-1])[np.argsort(silhouette_scores)[-20:]])\n",
    "from kneed import KneeLocator\n",
    "kn = KneeLocator(np.linspace(y_bottom_limit,y_top_limit,1000)[:-1], no_clusters, curve='convex', direction='decreasing')\n",
    "print(np.unique(sch.fcluster(linkage, kn.knee, criterion='distance'), return_counts=False).shape)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(sch.fcluster(linkage, kn.knee, criterion='distance'), return_counts=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kn.knee"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.metrics import calinski_harabasz_score\n",
    "plt.rcParams['figure.figsize'] = (3.32, 3.32*(1/1))\n",
    "y_top_limit = linkage[-1][2]\n",
    "y_bottom_limit = linkage[0][2]\n",
    "# no_clusters = []\n",
    "# silhouette_scores0 = []\n",
    "# for distance in np.linspace(y_bottom_limit,y_top_limit,1000)[:-1]:\n",
    "#     clusters = sch.fcluster(linkage, distance, criterion='distance')\n",
    "#     silhouette_scores0.append(silhouette_score(data, clusters))#, metric=metric, random_state=0))\n",
    "#     no_clusters.append(len(np.unique(clusters)))\n",
    "\n",
    "fig, ax1 = plt.subplots()\n",
    "plt.rcParams['figure.figsize'] = [3.32,3.32]\n",
    "ax1.axvline(kn.knee, c=\"cyan\")\n",
    "ax1.axhline(53, c=\"cyan\")\n",
    "\n",
    "ax1.text(0.90,0.6,\"Knee distance: {:.3f}\\n Knee no. clusters: {}\".format(kn.knee, np.unique(sch.fcluster(linkage, kn.knee, criterion='distance'), return_counts=False).shape[0]),\n",
    "                    ha='right', va='top', transform=ax1.transAxes, size=6)\n",
    "\n",
    "ax1.plot(np.linspace(y_bottom_limit,y_top_limit,1000)[:-1], no_clusters, c=\"magenta\")\n",
    "# ax2 = ax1.twinx()\n",
    "# ax2.plot(np.linspace(y_bottom_limit,y_top_limit,1000)[:-1], silhouette_scores0, c=\"cyan\")\n",
    "ax1.set_ylabel(\"No. clusters\")\n",
    "# ax2.set_ylabel(\"Silhouette score\", c=\"cyan\")\n",
    "ax1.set_xlabel(\"Euclidean distance threshold\")\n",
    "# print(\"Best 20 cluster numbers: \", np.array(no_clusters)[np.argsort(silhouette_scores0)[-20:]])\n",
    "# print(\"Best 20 distances: \", np.array(np.linspace(y_bottom_limit,y_top_limit,1000)[:-1])[np.argsort(silhouette_scores0)[-20:]])\n",
    "plt.savefig('figures/agglomerative_knee.png', dpi=300, bbox_inches = 'tight',pad_inches = 0.01)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kn.knee"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = embedding\n",
    "metric = \"euclidean\"\n",
    "method=\"ward\"\n",
    "linkage = sch.linkage(data, method=method, metric=metric)\n",
    "plt.rcParams['figure.figsize'] = (6.97, 3)\n",
    "plt.rcParams.update({'font.size': 6})\n",
    "dendrogram = sch.dendrogram(linkage, truncate_mode=None, p=1, leaf_font_size = 0, color_threshold=kn.knee)\n",
    "axis = plt.gca()\n",
    "axis.set_xticks([])\n",
    "axis.set_xticklabels([])\n",
    "axis.set_ylabel(\"Euclidean distance threshold\", size=6, x=1)\n",
    "axis.set_xlabel(\"Leaves of the dendogram\", size=6)\n",
    "plt.savefig('figures/agglomerative_dendogram.png', dpi=300, bbox_inches = 'tight',pad_inches = 0.01)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(sch.fcluster(linkage, kn.knee, criterion='distance'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reducer = umap.UMAP(random_state=0, n_components=2, n_neighbors=300)\n",
    "reducer.fit(pseudo_CLR_transformed)\n",
    "embedding = reducer.transform(pseudo_CLR_transformed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reducer = umap.UMAP(random_state=0, n_components=2, n_neighbors=300)\n",
    "# reducer.fit(embedding)\n",
    "# embedding = reducer.transform(embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters53 = sch.fcluster(linkage, kn.knee, criterion='distance')-1\n",
    "clusters53_split = np.split(np.argsort(clusters53), indices_or_sections=np.cumsum(np.unique(clusters53, return_counts=True)[1][:-1]))\n",
    "\n",
    "plt.rcParams['figure.figsize'] = [3.32,3.32]\n",
    "\n",
    "print(\"UMAP of CLR transformed data, PCA (65PCs, 0.95 variance), Euclidean distance\")\n",
    "# plt.scatter(embedding[:,0], embedding[:,1], c=\"black\", s=s)\n",
    "for cluster in clusters53_split:\n",
    "    plt.scatter(embedding[:,0][cluster], embedding[:,1][cluster], s=1)\n",
    "plt.xlabel(\"UMAP axis 1\")\n",
    "plt.ylabel(\"UMAP axis 2\")\n",
    "\n",
    "# plt.savefig('figures/UMAP_agglomerative.png', dpi=300, bbox_inches = 'tight',pad_inches = 0.01)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "plt.hist([x for x in pseudo_CLR_transformed.to_numpy().flatten() if x>0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "plt.hist([x for x in obs_component_counts_df_122.to_numpy().flatten() if x>0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reducer = umap.UMAP(random_state=0)\n",
    "reducer.fit(pseudo_CLR_transformed)\n",
    "embedding = reducer.transform(pseudo_CLR_transformed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"UMAP of CLR transformed data, Euclidean distance\")\n",
    "plt.scatter(embedding[:,0], embedding[:,1], c=\"black\", s=s)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for rng in range(10):\n",
    "    print(\"RNG: \", rng)\n",
    "\n",
    "    from sklearn.decomposition import PCA\n",
    "    pca = PCA(n_components=122)\n",
    "    pca.fit(pseudo_CLR_transformed)\n",
    "\n",
    "    print(pca.explained_variance_ratio_.shape)\n",
    "\n",
    "    reducer = umap.UMAP(random_state=rng, n_components=20)\n",
    "    reducer.fit(pca.transform(pseudo_CLR_transformed))\n",
    "    embedding = reducer.transform(pca.transform(pseudo_CLR_transformed))\n",
    "\n",
    "    plt.rcParams['figure.figsize'] = [5,5]\n",
    "\n",
    "    print(\"UMAP of CLR transformed data, PCA (65PCs, 0.95 variance), Euclidean distance\")\n",
    "    plt.scatter(embedding[:,0], embedding[:,1], c=\"black\", s=s)\n",
    "    plt.show()\n",
    "\n",
    "    data = embedding\n",
    "    metric = \"euclidean\"\n",
    "\n",
    "    linkage = sch.linkage(data, method=\"ward\", metric=metric)\n",
    "    plt.rcParams['figure.figsize'] = (6.97, 3)\n",
    "    plt.rcParams.update({'font.size': 6})\n",
    "\n",
    "    dendrogram = sch.dendrogram(linkage, truncate_mode=None, p=1, leaf_font_size = 0)\n",
    "    axis = plt.gca()\n",
    "    # axis.tick_params(axis=\"x\", which=\"major\", length=0, width=0, labelsize=0, direction=\"in\")\n",
    "    axis.set_xticks([])\n",
    "    axis.set_xticklabels([])\n",
    "    axis.set_ylabel(\"euclidean distance\", size=6, x=1)\n",
    "\n",
    "    # plt.savefig('figures/agglomerative_dendogram.png', dpi=300, bbox_inches = 'tight',pad_inches = 0)\n",
    "\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    from sklearn.metrics import silhouette_score\n",
    "    from sklearn.metrics import calinski_harabasz_score\n",
    "\n",
    "    plt.rcParams['figure.figsize'] = (3.32, 3.32*(1/1))\n",
    "\n",
    "\n",
    "    y_top_limit = linkage[-1][2]\n",
    "    y_bottom_limit = linkage[0][2]\n",
    "\n",
    "    no_clusters = []\n",
    "    silhouette_scores = []\n",
    "    for distance in np.linspace(y_bottom_limit,y_top_limit,1000)[:-1]:\n",
    "        clusters = sch.fcluster(linkage, distance, criterion='distance')\n",
    "        silhouette_scores.append(calinski_harabasz_score(data, clusters))#, metric=metric, random_state=0))\n",
    "        no_clusters.append(len(np.unique(clusters)))\n",
    "\n",
    "    fig, ax1 = plt.subplots()\n",
    "\n",
    "\n",
    "    plt.rcParams['figure.figsize'] = [5,5]\n",
    "    # plt.axhline(114, c=\"cyan\")\n",
    "    # ax1.axvline(4.84678277, c=\"black\")\n",
    "    # ax1.axvline(4.291764, c=\"black\")\n",
    "    # ax1.axhline(60, c=\"black\")\n",
    "    ax1.plot(np.linspace(y_bottom_limit,y_top_limit,1000)[:-1], no_clusters, c=\"magenta\")\n",
    "\n",
    "    ax2 = ax1.twinx()\n",
    "    ax2.plot(np.linspace(y_bottom_limit,y_top_limit,1000)[:-1], silhouette_scores, c=\"cyan\")\n",
    "    # plt.title(\"No. of clusters as the function of distance for hierarchical clustering\")\n",
    "    # ax3 = ax1.twinx()\n",
    "    # ax3.plot(np.linspace(0,y_limit,1000)[:-1], calinski_harabasz_scores, c=\"orange\")\n",
    "    # ax4 = ax1.twinx()\n",
    "    # ax4.plot(np.linspace(0,y_limit,1000)[:-2], np.diff(silhouette_scores, 1), c=\"green\")\n",
    "\n",
    "    ax1.set_ylabel(\"No. clusters\", c=\"magenta\")\n",
    "    ax2.set_ylabel(\"Silhouette score\", c=\"cyan\")\n",
    "    ax1.set_xlabel(\"Distance threshold\")\n",
    "    # plt.ylim((0,50))\n",
    "    # plt.savefig('figures/agglomerative_silhouette.png', dpi=300, bbox_inches = 'tight',pad_inches = 0)\n",
    "    print(\"Best 20 cluster numbers: \", np.array(no_clusters)[np.argsort(silhouette_scores)[-20:]])\n",
    "    print(\"Best 20 distances: \", np.array(np.linspace(y_bottom_limit,y_top_limit,1000)[:-1])[np.argsort(silhouette_scores)[-20:]])\n",
    "    from kneed import KneeLocator\n",
    "    kn = KneeLocator(np.linspace(y_bottom_limit,y_top_limit,1000)[:-1], no_clusters, curve='convex', direction='decreasing')\n",
    "    print(np.unique(sch.fcluster(linkage, kn.knee, criterion='distance'), return_counts=False).shape)\n",
    "\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_this = sch.fcluster(linkage, 3.27677289, criterion='distance')\n",
    "clusters53_split = np.split(np.argsort(split_this), indices_or_sections=np.cumsum(np.unique(split_this, return_counts=True)[1][:-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = [5,5]\n",
    "\n",
    "print(\"UMAP of CLR transformed data, PCA (65PCs, 0.95 variance), Euclidean distance\")\n",
    "# plt.scatter(embedding[:,0], embedding[:,1], c=\"black\", s=s)\n",
    "for cluster in clusters53_split:\n",
    "    plt.scatter(embedding[:,0][cluster], embedding[:,1][cluster], s=1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pca.transform(pseudo_CLR_transformed)\n",
    "metric = \"euclidean\"\n",
    "\n",
    "linkage = sch.linkage(data, method=\"ward\", metric=metric)\n",
    "plt.rcParams['figure.figsize'] = (6.97, 3)\n",
    "plt.rcParams.update({'font.size': 6})\n",
    "\n",
    "dendrogram = sch.dendrogram(linkage, truncate_mode=None, p=1, leaf_font_size = 0)\n",
    "axis = plt.gca()\n",
    "# axis.tick_params(axis=\"x\", which=\"major\", length=0, width=0, labelsize=0, direction=\"in\")\n",
    "axis.set_xticks([])\n",
    "axis.set_xticklabels([])\n",
    "axis.set_ylabel(\"euclidean distance\", size=6, x=1)\n",
    "\n",
    "# plt.savefig('figures/agglomerative_dendogram.png', dpi=300, bbox_inches = 'tight',pad_inches = 0)\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.metrics import davies_bouldin_score\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (3.32, 3.32*(1/1))\n",
    "\n",
    "\n",
    "y_top_limit = linkage[-1][2]\n",
    "y_bottom_limit = linkage[0][2]\n",
    "\n",
    "no_clusters = []\n",
    "silhouette_scores = []\n",
    "for distance in np.linspace(y_bottom_limit,y_top_limit,1000)[:-1]:\n",
    "    clusters = sch.fcluster(linkage, distance, criterion='distance')\n",
    "    silhouette_scores.append(silhouette_score(data, clusters, metric=metric, random_state=0))\n",
    "    no_clusters.append(len(np.unique(clusters)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax1 = plt.subplots()\n",
    "\n",
    "\n",
    "plt.rcParams['figure.figsize'] = [5,5]\n",
    "# plt.axhline(114, c=\"cyan\")\n",
    "# ax1.axvline(4.84678277, c=\"black\")\n",
    "# ax1.axvline(4.291764, c=\"black\")\n",
    "# ax1.axhline(60, c=\"black\")\n",
    "ax1.plot(np.linspace(y_bottom_limit,y_top_limit,1000)[:-1], no_clusters, c=\"magenta\")\n",
    "\n",
    "ax2 = ax1.twinx()\n",
    "ax2.plot(np.linspace(y_bottom_limit,y_top_limit,1000)[:-1], silhouette_scores, c=\"cyan\")\n",
    "# plt.title(\"No. of clusters as the function of distance for hierarchical clustering\")\n",
    "# ax3 = ax1.twinx()\n",
    "# ax3.plot(np.linspace(0,y_limit,1000)[:-1], calinski_harabasz_scores, c=\"orange\")\n",
    "# ax4 = ax1.twinx()\n",
    "# ax4.plot(np.linspace(0,y_limit,1000)[:-2], np.diff(silhouette_scores, 1), c=\"green\")\n",
    "\n",
    "ax1.set_ylabel(\"No. clusters\", c=\"magenta\")\n",
    "ax2.set_ylabel(\"Silhouette score\", c=\"cyan\")\n",
    "ax1.set_xlabel(\"Distance threshold\")\n",
    "# plt.ylim((0,50))\n",
    "# plt.savefig('figures/agglomerative_silhouette.png', dpi=300, bbox_inches = 'tight',pad_inches = 0)\n",
    "print(\"Best 20 cluster numbers: \", np.array(no_clusters)[np.argsort(silhouette_scores)[-20:]])\n",
    "print(\"Best 20 distances: \", np.array(np.linspace(y_bottom_limit,y_top_limit,1000)[:-1])[np.argsort(silhouette_scores)[-20:]])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_this = sch.fcluster(linkage, 61.96563317, criterion='distance')\n",
    "clusters29_split = np.split(np.argsort(split_this), indices_or_sections=np.cumsum(np.unique(split_this, return_counts=True)[1][:-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = [5,5]\n",
    "\n",
    "print(\"UMAP of CLR transformed data, PCA (65PCs, 0.95 variance), Euclidean distance\")\n",
    "plt.scatter(embedding[:,0], embedding[:,1], c=\"black\", s=s)\n",
    "for cluster in clusters29_split[n]:\n",
    "    plt.scatter(embedding[:,0][cluster], embedding[:,1][cluster], s=1, c=\"cyan\")\n",
    "plt.show()\n",
    "n+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "pseudo_CLR_transformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reducer = umap.UMAP(random_state=1, n_neighbors=1737,min_dist=0.0, n_components=50, n_epochs=10000, transform_queue_size=10)\n",
    "reducer.fit(pseudo_CLR_transformed)\n",
    "embedding = reducer.transform(pseudo_CLR_transformed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = embedding\n",
    "metric = \"euclidean\"\n",
    "\n",
    "linkage = sch.linkage(data, method=\"ward\", metric=metric)\n",
    "plt.rcParams['figure.figsize'] = (6.97, 3)\n",
    "plt.rcParams.update({'font.size': 6})\n",
    "\n",
    "dendrogram = sch.dendrogram(linkage, truncate_mode=None, p=1, leaf_font_size = 0)\n",
    "axis = plt.gca()\n",
    "# axis.tick_params(axis=\"x\", which=\"major\", length=0, width=0, labelsize=0, direction=\"in\")\n",
    "axis.set_xticks([])\n",
    "axis.set_xticklabels([])\n",
    "axis.set_ylabel(\"euclidean distance\", size=6, x=1)\n",
    "\n",
    "# plt.savefig('figures/agglomerative_dendogram.png', dpi=300, bbox_inches = 'tight',pad_inches = 0)\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.metrics import davies_bouldin_score\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (3.32, 3.32*(1/1))\n",
    "\n",
    "\n",
    "y_top_limit = linkage[-1][2]\n",
    "y_bottom_limit = linkage[0][2]\n",
    "\n",
    "no_clusters = []\n",
    "silhouette_scores = []\n",
    "for distance in np.linspace(y_bottom_limit,y_top_limit,1000)[:-1]:\n",
    "    clusters = sch.fcluster(linkage, distance, criterion='distance')\n",
    "    silhouette_scores.append(silhouette_score(data, clusters, metric=metric, random_state=0))\n",
    "    no_clusters.append(len(np.unique(clusters)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax1 = plt.subplots()\n",
    "\n",
    "\n",
    "plt.rcParams['figure.figsize'] = [5,5]\n",
    "# plt.axhline(114, c=\"cyan\")\n",
    "# ax1.axvline(4.84678277, c=\"black\")\n",
    "# ax1.axvline(4.291764, c=\"black\")\n",
    "# ax1.axhline(60, c=\"black\")\n",
    "ax1.plot(np.linspace(y_bottom_limit,y_top_limit,1000)[:-1], no_clusters, c=\"magenta\")\n",
    "\n",
    "ax2 = ax1.twinx()\n",
    "ax2.plot(np.linspace(y_bottom_limit,y_top_limit,1000)[:-1], silhouette_scores, c=\"cyan\")\n",
    "# plt.title(\"No. of clusters as the function of distance for hierarchical clustering\")\n",
    "# ax3 = ax1.twinx()\n",
    "# ax3.plot(np.linspace(0,y_limit,1000)[:-1], calinski_harabasz_scores, c=\"orange\")\n",
    "# ax4 = ax1.twinx()\n",
    "# ax4.plot(np.linspace(0,y_limit,1000)[:-2], np.diff(silhouette_scores, 1), c=\"green\")\n",
    "\n",
    "ax1.set_ylabel(\"No. clusters\", c=\"magenta\")\n",
    "ax2.set_ylabel(\"Silhouette score\", c=\"cyan\")\n",
    "ax1.set_xlabel(\"Distance threshold\")\n",
    "# plt.ylim((0,50))\n",
    "# plt.savefig('figures/agglomerative_silhouette.png', dpi=300, bboobcy z marsa te s spoko, ale na wenus znaleziono sygnatury fosforowodorw; powstay albo przez jakie ekstremalne procesy chemiczne albo w brzuszkach kosmitw PogChamp_inches = 'tight',pad_inches = 0)\n",
    "print(\"Best 20 cluster numbers: \", np.array(no_clusters)[np.argsort(silhouette_scores)[-20:]])\n",
    "print(\"Best 20 distances: \", np.array(np.linspace(y_bottom_limit,y_top_limit,1000)[:-1])[np.argsort(silhouette_scores)[-20:]])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reducer = umap.UMAP(random_state=0, n_neighbors=1737,min_dist=0.0, n_components=5, n_epochs=10000, transform_queue_size=10)\n",
    "reducer.fit(pseudo_CLR_transformed)\n",
    "embedding = reducer.transform(pseudo_CLR_transformed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = embedding\n",
    "metric = \"euclidean\"\n",
    "\n",
    "linkage = sch.linkage(data, method=\"ward\", metric=metric)\n",
    "plt.rcParams['figure.figsize'] = (6.97, 3)\n",
    "plt.rcParams.update({'font.size': 6})\n",
    "\n",
    "dendrogram = sch.dendrogram(linkage, truncate_mode=None, p=1, leaf_font_size = 0)\n",
    "axis = plt.gca()\n",
    "# axis.tick_params(axis=\"x\", which=\"major\", length=0, width=0, labelsize=0, direction=\"in\")\n",
    "axis.set_xticks([])\n",
    "axis.set_xticklabels([])\n",
    "axis.set_ylabel(\"euclidean distance\", size=6, x=1)\n",
    "\n",
    "# plt.savefig('figures/agglomerative_dendogram.png', dpi=300, bbox_inches = 'tight',pad_inches = 0)\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.metrics import davies_bouldin_score\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (3.32, 3.32*(1/1))\n",
    "\n",
    "\n",
    "y_top_limit = linkage[-1][2]\n",
    "y_bottom_limit = linkage[0][2]\n",
    "\n",
    "no_clusters = []\n",
    "silhouette_scores = []\n",
    "for distance in np.linspace(y_bottom_limit,y_top_limit,1000)[:-1]:\n",
    "    clusters = sch.fcluster(linkage, distance, criterion='distance')\n",
    "    silhouette_scores.append(silhouette_score(data, clusters, metric=metric, random_state=0))\n",
    "    no_clusters.append(len(np.unique(clusters)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax1 = plt.subplots()\n",
    "\n",
    "\n",
    "plt.rcParams['figure.figsize'] = [5,5]\n",
    "# plt.axhline(114, c=\"cyan\")\n",
    "# ax1.axvline(4.84678277, c=\"black\")\n",
    "# ax1.axvline(4.291764, c=\"black\")\n",
    "# ax1.axhline(60, c=\"black\")\n",
    "ax1.plot(np.linspace(y_bottom_limit,y_top_limit,1000)[:-1], no_clusters, c=\"magenta\")\n",
    "\n",
    "ax2 = ax1.twinx()\n",
    "ax2.plot(np.linspace(y_bottom_limit,y_top_limit,1000)[:-1], silhouette_scores, c=\"cyan\")\n",
    "# plt.title(\"No. of clusters as the function of distance for hierarchical clustering\")\n",
    "# ax3 = ax1.twinx()\n",
    "# ax3.plot(np.linspace(0,y_limit,1000)[:-1], calinski_harabasz_scores, c=\"orange\")\n",
    "# ax4 = ax1.twinx()\n",
    "# ax4.plot(np.linspace(0,y_limit,1000)[:-2], np.diff(silhouette_scores, 1), c=\"green\")\n",
    "\n",
    "ax1.set_ylabel(\"No. clusters\", c=\"magenta\")\n",
    "ax2.set_ylabel(\"Silhouette score\", c=\"cyan\")\n",
    "ax1.set_xlabel(\"Distance threshold\")\n",
    "# plt.ylim((0,50))\n",
    "# plt.savefig('figures/agglomerative_silhouette.png', dpi=300, bboobcy z marsa te s spoko, ale na wenus znaleziono sygnatury fosforowodorw; powstay albo przez jakie ekstremalne procesy chemiczne albo w brzuszkach kosmitw PogChamp_inches = 'tight',pad_inches = 0)\n",
    "print(\"Best 20 cluster numbers: \", np.array(no_clusters)[np.argsort(silhouette_scores)[-20:]])\n",
    "print(\"Best 20 distances: \", np.array(np.linspace(y_bottom_limit,y_top_limit,1000)[:-1])[np.argsort(silhouette_scores)[-20:]])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Init signature:\n",
    "umap.UMAP(\n",
    "    n_neighbors=15,\n",
    "    n_components=2,\n",
    "    metric='euclidean',\n",
    "    metric_kwds=None,\n",
    "    output_metric='euclidean',\n",
    "    output_metric_kwds=None,\n",
    "    n_epochs=None,\n",
    "    learning_rate=1.0,\n",
    "    init='spectral',\n",
    "    min_dist=0.1,\n",
    "    spread=1.0,\n",
    "    low_memory=False,\n",
    "    set_op_mix_ratio=1.0,\n",
    "    local_connectivity=1.0,\n",
    "    repulsion_strength=1.0,\n",
    "    negative_sample_rate=5,\n",
    "    transform_queue_size=4.0,\n",
    "    a=None,\n",
    "    b=None,\n",
    "    random_state=None,\n",
    "    angular_rp_forest=False,\n",
    "    target_n_neighbors=-1,\n",
    "    target_metric='categorical',\n",
    "    target_metric_kwds=None,\n",
    "    target_weight=0.5,\n",
    "    transform_seed=42,\n",
    "    force_approximation_algorithm=False,\n",
    "    verbose=False,\n",
    "    unique=False,\n",
    ")\n",
    "Docstring:     \n",
    "Uniform Manifold Approximation and Projection\n",
    "\n",
    "Finds a low dimensional embedding of the data that approximates\n",
    "an underlying manifold.\n",
    "\n",
    "Parameters\n",
    "----------\n",
    "n_neighbors: float (optional, default 15)\n",
    "    The size of local neighborhood (in terms of number of neighboring\n",
    "    sample points) used for manifold approximation. Larger values\n",
    "    result in more global views of the manifold, while smaller\n",
    "    values result in more local data being preserved. In general\n",
    "    values should be in the range 2 to 100.\n",
    "\n",
    "n_components: int (optional, default 2)\n",
    "    The dimension of the space to embed into. This defaults to 2 to\n",
    "    provide easy visualization, but can reasonably be set to any\n",
    "    integer value in the range 2 to 100.\n",
    "\n",
    "metric: string or function (optional, default 'euclidean')\n",
    "    The metric to use to compute distances in high dimensional space.\n",
    "    If a string is passed it must match a valid predefined metric. If\n",
    "    a general metric is required a function that takes two 1d arrays and\n",
    "    returns a float can be provided. For performance purposes it is\n",
    "    required that this be a numba jit'd function. Valid string metrics\n",
    "    include:\n",
    "        * euclidean\n",
    "        * manhattan\n",
    "        * chebyshev\n",
    "        * minkowski\n",
    "        * canberra\n",
    "        * braycurtis\n",
    "        * mahalanobis\n",
    "        * wminkowski\n",
    "        * seuclidean\n",
    "        * cosine\n",
    "        * correlation\n",
    "        * haversine\n",
    "        * hamming\n",
    "        * jaccard\n",
    "        * dice\n",
    "        * russelrao\n",
    "        * kulsinski\n",
    "        * ll_dirichlet\n",
    "        * hellinger\n",
    "        * rogerstanimoto\n",
    "        * sokalmichener\n",
    "        * sokalsneath\n",
    "        * yule\n",
    "    Metrics that take arguments (such as minkowski, mahalanobis etc.)\n",
    "    can have arguments passed via the metric_kwds dictionary. At this\n",
    "    time care must be taken and dictionary elements must be ordered\n",
    "    appropriately; this will hopefully be fixed in the future.\n",
    "\n",
    "n_epochs: int (optional, default None)\n",
    "    The number of training epochs to be used in optimizing the\n",
    "    low dimensional embedding. Larger values result in more accurate\n",
    "    embeddings. If None is specified a value will be selected based on\n",
    "    the size of the input dataset (200 for large datasets, 500 for small).\n",
    "\n",
    "learning_rate: float (optional, default 1.0)\n",
    "    The initial learning rate for the embedding optimization.\n",
    "\n",
    "init: string (optional, default 'spectral')\n",
    "    How to initialize the low dimensional embedding. Options are:\n",
    "        * 'spectral': use a spectral embedding of the fuzzy 1-skeleton\n",
    "        * 'random': assign initial embedding positions at random.\n",
    "        * A numpy array of initial embedding positions.\n",
    "\n",
    "min_dist: float (optional, default 0.1)\n",
    "    The effective minimum distance between embedded points. Smaller values\n",
    "    will result in a more clustered/clumped embedding where nearby points\n",
    "    on the manifold are drawn closer together, while larger values will\n",
    "    result on a more even dispersal of points. The value should be set\n",
    "    relative to the ``spread`` value, which determines the scale at which\n",
    "    embedded points will be spread out.\n",
    "\n",
    "spread: float (optional, default 1.0)\n",
    "    The effective scale of embedded points. In combination with ``min_dist``\n",
    "    this determines how clustered/clumped the embedded points are.\n",
    "\n",
    "low_memory: bool (optional, default False)\n",
    "    For some datasets the nearest neighbor computation can consume a lot of\n",
    "    memory. If you find that UMAP is failing due to memory constraints\n",
    "    consider setting this option to True. This approach is more\n",
    "    computationally expensive, but avoids excessive memory use.\n",
    "\n",
    "set_op_mix_ratio: float (optional, default 1.0)\n",
    "    Interpolate between (fuzzy) union and intersection as the set operation\n",
    "    used to combine local fuzzy simplicial sets to obtain a global fuzzy\n",
    "    simplicial sets. Both fuzzy set operations use the product t-norm.\n",
    "    The value of this parameter should be between 0.0 and 1.0; a value of\n",
    "    1.0 will use a pure fuzzy union, while 0.0 will use a pure fuzzy\n",
    "    intersection.\n",
    "\n",
    "local_connectivity: int (optional, default 1)\n",
    "    The local connectivity required -- i.e. the number of nearest\n",
    "    neighbors that should be assumed to be connected at a local level.\n",
    "    The higher this value the more connected the manifold becomes\n",
    "    locally. In practice this should be not more than the local intrinsic\n",
    "    dimension of the manifold.\n",
    "\n",
    "repulsion_strength: float (optional, default 1.0)\n",
    "    Weighting applied to negative samples in low dimensional embedding\n",
    "    optimization. Values higher than one will result in greater weight\n",
    "    being given to negative samples.\n",
    "\n",
    "negative_sample_rate: int (optional, default 5)\n",
    "    The number of negative samples to select per positive sample\n",
    "    in the optimization process. Increasing this value will result\n",
    "    in greater repulsive force being applied, greater optimization\n",
    "    cost, but slightly more accuracy.\n",
    "\n",
    "transform_queue_size: float (optional, default 4.0)\n",
    "    For transform operations (embedding new points using a trained model_\n",
    "    this will control how aggressively to search for nearest neighbors.\n",
    "    Larger values will result in slower performance but more accurate\n",
    "    nearest neighbor evaluation.\n",
    "\n",
    "a: float (optional, default None)\n",
    "    More specific parameters controlling the embedding. If None these\n",
    "    values are set automatically as determined by ``min_dist`` and\n",
    "    ``spread``.\n",
    "b: float (optional, default None)\n",
    "    More specific parameters controlling the embedding. If None these\n",
    "    values are set automatically as determined by ``min_dist`` and\n",
    "    ``spread``.\n",
    "\n",
    "random_state: int, RandomState instance or None, optional (default: None)\n",
    "    If int, random_state is the seed used by the random number generator;\n",
    "    If RandomState instance, random_state is the random number generator;\n",
    "    If None, the random number generator is the RandomState instance used\n",
    "    by `np.random`.\n",
    "\n",
    "metric_kwds: dict (optional, default None)\n",
    "    Arguments to pass on to the metric, such as the ``p`` value for\n",
    "    Minkowski distance. If None then no arguments are passed on.\n",
    "\n",
    "angular_rp_forest: bool (optional, default False)\n",
    "    Whether to use an angular random projection forest to initialise\n",
    "    the approximate nearest neighbor search. This can be faster, but is\n",
    "    mostly on useful for metric that use an angular style distance such\n",
    "    as cosine, correlation etc. In the case of those metrics angular forests\n",
    "    will be chosen automatically.\n",
    "\n",
    "target_n_neighbors: int (optional, default -1)\n",
    "    The number of nearest neighbors to use to construct the target simplcial\n",
    "    set. If set to -1 use the ``n_neighbors`` value.\n",
    "\n",
    "target_metric: string or callable (optional, default 'categorical')\n",
    "    The metric used to measure distance for a target array is using supervised\n",
    "    dimension reduction. By default this is 'categorical' which will measure\n",
    "    distance in terms of whether categories match or are different. Furthermore,\n",
    "    if semi-supervised is required target values of -1 will be trated as\n",
    "    unlabelled under the 'categorical' metric. If the target array takes\n",
    "    continuous values (e.g. for a regression problem) then metric of 'l1'\n",
    "    or 'l2' is probably more appropriate.\n",
    "\n",
    "target_metric_kwds: dict (optional, default None)\n",
    "    Keyword argument to pass to the target metric when performing\n",
    "    supervised dimension reduction. If None then no arguments are passed on.\n",
    "\n",
    "target_weight: float (optional, default 0.5)\n",
    "    weighting factor between data topology and target topology. A value of\n",
    "    0.0 weights entirely on data, a value of 1.0 weights entirely on target.\n",
    "    The default of 0.5 balances the weighting equally between data and target.\n",
    "\n",
    "transform_seed: int (optional, default 42)\n",
    "    Random seed used for the stochastic aspects of the transform operation.\n",
    "    This ensures consistency in transform operations.\n",
    "\n",
    "verbose: bool (optional, default False)\n",
    "    Controls verbosity of logging.\n",
    "\n",
    "unique: bool (optional, default False)\n",
    "    Controls if the rows of your data should be uniqued before being\n",
    "    embedded.  If you have more duplicates than you have n_neighbour\n",
    "    you can have the identical data points lying in different regions of\n",
    "    your space.  It also violates the definition of a metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=0.95)\n",
    "pca.fit(pseudo_CLR_transformed)\n",
    "pca_pseudo_CLR_transformed = pca.transform(pseudo_CLR_transformed)\n",
    "print(\"PCA components: \",pca.explained_variance_ratio_.shape)\n",
    "UMAP_reducer = umap.UMAP(random_state=1, n_neighbors=1737,min_dist=0.0, n_components=5, n_epochs=1000, transform_queue_size=10)\n",
    "UMAP_reducer.fit(pca_pseudo_CLR_transformed)\n",
    "UMAP_embedding = UMAP_reducer.transform(pca_pseudo_CLR_transformed)\n",
    "\n",
    "plt.rcParams['figure.figsize'] = [5,5]\n",
    "print(\"UMAP of CLR transformed data, PCA (65PCs, 0.95 variance), Euclidean distance\")\n",
    "plt.scatter(UMAP_embedding[:,0], UMAP_embedding[:,1], c=\"black\", s=s)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "data = UMAP_embedding\n",
    "metric = \"euclidean\"\n",
    "method=\"ward\"\n",
    "linkage = sch.linkage(data, method=method, metric=metric)\n",
    "plt.rcParams['figure.figsize'] = (6.97, 3)\n",
    "plt.rcParams.update({'font.size': 6})\n",
    "dendrogram = sch.dendrogram(linkage, truncate_mode=None, p=1, leaf_font_size = 0)\n",
    "axis = plt.gca()\n",
    "axis.set_xticks([])\n",
    "axis.set_xticklabels([])\n",
    "axis.set_ylabel(\"distance\", size=6, x=1)\n",
    "plt.show()\n",
    "\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.metrics import davies_bouldin_score\n",
    "plt.rcParams['figure.figsize'] = (3.32, 3.32*(1/1))\n",
    "y_top_limit = linkage[-1][2]\n",
    "y_bottom_limit = linkage[0][2]\n",
    "no_clusters = []\n",
    "silhouette_scores = []\n",
    "for distance in np.linspace(y_bottom_limit,y_top_limit,1000)[:-1]:\n",
    "    clusters = sch.fcluster(linkage, distance, criterion='distance')\n",
    "    silhouette_scores.append(silhouette_score(data, clusters, metric=metric, random_state=0))\n",
    "    no_clusters.append(len(np.unique(clusters)))\n",
    "\n",
    "fig, ax1 = plt.subplots()\n",
    "plt.rcParams['figure.figsize'] = [5,5]\n",
    "ax1.plot(np.linspace(y_bottom_limit,y_top_limit,1000)[:-1], no_clusters, c=\"magenta\")\n",
    "ax2 = ax1.twinx()\n",
    "ax2.plot(np.linspace(y_bottom_limit,y_top_limit,1000)[:-1], silhouette_scores, c=\"cyan\")\n",
    "ax1.set_ylabel(\"No. clusters\", c=\"magenta\")\n",
    "ax2.set_ylabel(\"Silhouette score\", c=\"cyan\")\n",
    "ax1.set_xlabel(\"Distance threshold\")\n",
    "print(\"Best 20 cluster numbers: \", np.array(no_clusters)[np.argsort(silhouette_scores)[-20:]])\n",
    "print(\"Best 20 distances: \", np.array(np.linspace(y_bottom_limit,y_top_limit,1000)[:-1])[np.argsort(silhouette_scores)[-20:]])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=0.95)\n",
    "pca.fit(pseudo_CLR_transformed)\n",
    "pca_pseudo_CLR_transformed = pca.transform(pseudo_CLR_transformed)\n",
    "print(\"PCA components: \",pca.explained_variance_ratio_.shape)\n",
    "UMAP_reducer = umap.UMAP(random_state=1, n_neighbors=1737,min_dist=0.0, n_components=2, n_epochs=1000, transform_queue_size=10)\n",
    "UMAP_reducer.fit(pca_pseudo_CLR_transformed)\n",
    "UMAP_embedding = UMAP_reducer.transform(pca_pseudo_CLR_transformed)\n",
    "\n",
    "plt.rcParams['figure.figsize'] = [5,5]\n",
    "print(\"UMAP of CLR transformed data, PCA (65PCs, 0.95 variance), Euclidean distance\")\n",
    "plt.scatter(UMAP_embedding[:,0], UMAP_embedding[:,1], c=\"black\", s=s)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "data = UMAP_embedding\n",
    "metric = \"euclidean\"\n",
    "method=\"ward\"\n",
    "linkage = sch.linkage(data, method=method, metric=metric)\n",
    "plt.rcParams['figure.figsize'] = (6.97, 3)\n",
    "plt.rcParams.update({'font.size': 6})\n",
    "dendrogram = sch.dendrogram(linkage, truncate_mode=None, p=1, leaf_font_size = 0)\n",
    "axis = plt.gca()\n",
    "axis.set_xticks([])\n",
    "axis.set_xticklabels([])\n",
    "axis.set_ylabel(\"distance\", size=6, x=1)\n",
    "plt.show()\n",
    "\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.metrics import davies_bouldin_score\n",
    "plt.rcParams['figure.figsize'] = (3.32, 3.32*(1/1))\n",
    "y_top_limit = linkage[-1][2]\n",
    "y_bottom_limit = linkage[0][2]\n",
    "no_clusters = []\n",
    "silhouette_scores = []\n",
    "for distance in np.linspace(y_bottom_limit,y_top_limit,1000)[:-1]:\n",
    "    clusters = sch.fcluster(linkage, distance, criterion='distance')\n",
    "    silhouette_scores.append(silhouette_score(data, clusters, metric=metric, random_state=0))\n",
    "    no_clusters.append(len(np.unique(clusters)))\n",
    "\n",
    "fig, ax1 = plt.subplots()\n",
    "plt.rcParams['figure.figsize'] = [5,5]\n",
    "ax1.plot(np.linspace(y_bottom_limit,y_top_limit,1000)[:-1], no_clusters, c=\"magenta\")\n",
    "ax2 = ax1.twinx()\n",
    "ax2.plot(np.linspace(y_bottom_limit,y_top_limit,1000)[:-1], silhouette_scores, c=\"cyan\")\n",
    "ax1.set_ylabel(\"No. clusters\", c=\"magenta\")\n",
    "ax2.set_ylabel(\"Silhouette score\", c=\"cyan\")\n",
    "ax1.set_xlabel(\"Distance threshold\")\n",
    "print(\"Best 20 cluster numbers: \", np.array(no_clusters)[np.argsort(silhouette_scores)[-20:]])\n",
    "print(\"Best 20 distances: \", np.array(np.linspace(y_bottom_limit,y_top_limit,1000)[:-1])[np.argsort(silhouette_scores)[-20:]])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=0.95)\n",
    "pca.fit(pseudo_CLR_transformed)\n",
    "pca_pseudo_CLR_transformed = pca.transform(pseudo_CLR_transformed)\n",
    "print(\"PCA components: \",pca.explained_variance_ratio_.shape)\n",
    "UMAP_reducer = umap.UMAP(random_state=1, n_neighbors=100,min_dist=0.0, n_components=5, n_epochs=1000, transform_queue_size=10)\n",
    "UMAP_reducer.fit(pca_pseudo_CLR_transformed)\n",
    "UMAP_embedding = UMAP_reducer.transform(pca_pseudo_CLR_transformed)\n",
    "\n",
    "plt.rcParams['figure.figsize'] = [5,5]\n",
    "print(\"UMAP of CLR transformed data, PCA (65PCs, 0.95 variance), Euclidean distance\")\n",
    "plt.scatter(UMAP_embedding[:,0], UMAP_embedding[:,1], c=\"black\", s=s)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "data = UMAP_embedding\n",
    "metric = \"euclidean\"\n",
    "method=\"ward\"\n",
    "linkage = sch.linkage(data, method=method, metric=metric)\n",
    "plt.rcParams['figure.figsize'] = (6.97, 3)\n",
    "plt.rcParams.update({'font.size': 6})\n",
    "dendrogram = sch.dendrogram(linkage, truncate_mode=None, p=1, leaf_font_size = 0)\n",
    "axis = plt.gca()\n",
    "axis.set_xticks([])\n",
    "axis.set_xticklabels([])\n",
    "axis.set_ylabel(\"distance\", size=6, x=1)\n",
    "plt.show()\n",
    "\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.metrics import davies_bouldin_score\n",
    "plt.rcParams['figure.figsize'] = (3.32, 3.32*(1/1))\n",
    "y_top_limit = linkage[-1][2]\n",
    "y_bottom_limit = linkage[0][2]\n",
    "no_clusters = []\n",
    "silhouette_scores = []\n",
    "for distance in np.linspace(y_bottom_limit,y_top_limit,1000)[:-1]:\n",
    "    clusters = sch.fcluster(linkage, distance, criterion='distance')\n",
    "    silhouette_scores.append(silhouette_score(data, clusters, metric=metric, random_state=0))\n",
    "    no_clusters.append(len(np.unique(clusters)))\n",
    "\n",
    "fig, ax1 = plt.subplots()\n",
    "plt.rcParams['figure.figsize'] = [5,5]\n",
    "ax1.plot(np.linspace(y_bottom_limit,y_top_limit,1000)[:-1], no_clusters, c=\"magenta\")\n",
    "ax2 = ax1.twinx()\n",
    "ax2.plot(np.linspace(y_bottom_limit,y_top_limit,1000)[:-1], silhouette_scores, c=\"cyan\")\n",
    "ax1.set_ylabel(\"No. clusters\", c=\"magenta\")\n",
    "ax2.set_ylabel(\"Silhouette score\", c=\"cyan\")\n",
    "ax1.set_xlabel(\"Distance threshold\")\n",
    "print(\"Best 20 cluster numbers: \", np.array(no_clusters)[np.argsort(silhouette_scores)[-20:]])\n",
    "print(\"Best 20 distances: \", np.array(np.linspace(y_bottom_limit,y_top_limit,1000)[:-1])[np.argsort(silhouette_scores)[-20:]])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=0.95)\n",
    "pca.fit(pseudo_CLR_transformed)\n",
    "pca_pseudo_CLR_transformed = pca.transform(pseudo_CLR_transformed)\n",
    "print(\"PCA components: \",pca.explained_variance_ratio_.shape)\n",
    "UMAP_reducer = umap.UMAP(random_state=1, n_neighbors=50,min_dist=0.0, n_components=5, n_epochs=1000, transform_queue_size=10)\n",
    "UMAP_reducer.fit(pca_pseudo_CLR_transformed)\n",
    "UMAP_embedding = UMAP_reducer.transform(pca_pseudo_CLR_transformed)\n",
    "\n",
    "plt.rcParams['figure.figsize'] = [5,5]\n",
    "print(\"UMAP of CLR transformed data, PCA (65PCs, 0.95 variance), Euclidean distance\")\n",
    "plt.scatter(UMAP_embedding[:,0], UMAP_embedding[:,1], c=\"black\", s=s)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "data = UMAP_embedding\n",
    "metric = \"euclidean\"\n",
    "method=\"ward\"\n",
    "linkage = sch.linkage(data, method=method, metric=metric)\n",
    "plt.rcParams['figure.figsize'] = (6.97, 3)\n",
    "plt.rcParams.update({'font.size': 6})\n",
    "dendrogram = sch.dendrogram(linkage, truncate_mode=None, p=1, leaf_font_size = 0)\n",
    "axis = plt.gca()\n",
    "axis.set_xticks([])\n",
    "axis.set_xticklabels([])\n",
    "axis.set_ylabel(\"distance\", size=6, x=1)\n",
    "plt.show()\n",
    "\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.metrics import davies_bouldin_score\n",
    "plt.rcParams['figure.figsize'] = (3.32, 3.32*(1/1))\n",
    "y_top_limit = linkage[-1][2]\n",
    "y_bottom_limit = linkage[0][2]\n",
    "no_clusters = []\n",
    "silhouette_scores = []\n",
    "for distance in np.linspace(y_bottom_limit,y_top_limit,1000)[:-1]:\n",
    "    clusters = sch.fcluster(linkage, distance, criterion='distance')\n",
    "    silhouette_scores.append(silhouette_score(data, clusters, metric=metric, random_state=0))\n",
    "    no_clusters.append(len(np.unique(clusters)))\n",
    "\n",
    "fig, ax1 = plt.subplots()\n",
    "plt.rcParams['figure.figsize'] = [5,5]\n",
    "ax1.plot(np.linspace(y_bottom_limit,y_top_limit,1000)[:-1], no_clusters, c=\"magenta\")\n",
    "ax2 = ax1.twinx()\n",
    "ax2.plot(np.linspace(y_bottom_limit,y_top_limit,1000)[:-1], silhouette_scores, c=\"cyan\")\n",
    "ax1.set_ylabel(\"No. clusters\", c=\"magenta\")\n",
    "ax2.set_ylabel(\"Silhouette score\", c=\"cyan\")\n",
    "ax1.set_xlabel(\"Distance threshold\")\n",
    "print(\"Best 20 cluster numbers: \", np.array(no_clusters)[np.argsort(silhouette_scores)[-20:]])\n",
    "print(\"Best 20 distances: \", np.array(np.linspace(y_bottom_limit,y_top_limit,1000)[:-1])[np.argsort(silhouette_scores)[-20:]])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=0.95)\n",
    "pca.fit(pseudo_CLR_transformed)\n",
    "pca_pseudo_CLR_transformed = pca.transform(pseudo_CLR_transformed)\n",
    "print(\"PCA components: \",pca.explained_variance_ratio_.shape)\n",
    "UMAP_reducer = umap.UMAP(random_state=1)#, n_neighbors=50,min_dist=0.0, n_components=5, n_epochs=1000, transform_queue_size=10)\n",
    "UMAP_reducer.fit(pca_pseudo_CLR_transformed)\n",
    "UMAP_embedding = UMAP_reducer.transform(pca_pseudo_CLR_transformed)\n",
    "\n",
    "plt.rcParams['figure.figsize'] = [5,5]\n",
    "print(\"UMAP of CLR transformed data, PCA (65PCs, 0.95 variance), Euclidean distance\")\n",
    "plt.scatter(UMAP_embedding[:,0], UMAP_embedding[:,1], c=\"black\", s=s)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "data = UMAP_embedding\n",
    "metric = \"euclidean\"\n",
    "method=\"ward\"\n",
    "linkage = sch.linkage(data, method=method, metric=metric)\n",
    "plt.rcParams['figure.figsize'] = (6.97, 3)\n",
    "plt.rcParams.update({'font.size': 6})\n",
    "dendrogram = sch.dendrogram(linkage, truncate_mode=None, p=1, leaf_font_size = 0)\n",
    "axis = plt.gca()\n",
    "axis.set_xticks([])\n",
    "axis.set_xticklabels([])\n",
    "axis.set_ylabel(\"distance\", size=6, x=1)\n",
    "plt.show()\n",
    "\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.metrics import davies_bouldin_score\n",
    "plt.rcParams['figure.figsize'] = (3.32, 3.32*(1/1))\n",
    "y_top_limit = linkage[-1][2]\n",
    "y_bottom_limit = linkage[0][2]\n",
    "no_clusters = []\n",
    "silhouette_scores = []\n",
    "for distance in np.linspace(y_bottom_limit,y_top_limit,1000)[:-1]:\n",
    "    clusters = sch.fcluster(linkage, distance, criterion='distance')\n",
    "    silhouette_scores.append(silhouette_score(data, clusters, metric=metric, random_state=0))\n",
    "    no_clusters.append(len(np.unique(clusters)))\n",
    "\n",
    "fig, ax1 = plt.subplots()\n",
    "plt.rcParams['figure.figsize'] = [5,5]\n",
    "ax1.plot(np.linspace(y_bottom_limit,y_top_limit,1000)[:-1], no_clusters, c=\"magenta\")\n",
    "ax2 = ax1.twinx()\n",
    "ax2.plot(np.linspace(y_bottom_limit,y_top_limit,1000)[:-1], silhouette_scores, c=\"cyan\")\n",
    "ax1.set_ylabel(\"No. clusters\", c=\"magenta\")\n",
    "ax2.set_ylabel(\"Silhouette score\", c=\"cyan\")\n",
    "ax1.set_xlabel(\"Distance threshold\")\n",
    "print(\"Best 20 cluster numbers: \", np.array(no_clusters)[np.argsort(silhouette_scores)[-20:]])\n",
    "print(\"Best 20 distances: \", np.array(np.linspace(y_bottom_limit,y_top_limit,1000)[:-1])[np.argsort(silhouette_scores)[-20:]])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=0.95)\n",
    "pca.fit(pseudo_CLR_transformed)\n",
    "pca_pseudo_CLR_transformed = pca.transform(pseudo_CLR_transformed)\n",
    "print(\"PCA components: \",pca.explained_variance_ratio_.shape)\n",
    "UMAP_reducer = umap.UMAP(random_state=1,min_dist=0.0)#, n_neighbors=50,min_dist=0.0, n_components=5, n_epochs=1000, transform_queue_size=10)\n",
    "UMAP_reducer.fit(pca_pseudo_CLR_transformed)\n",
    "UMAP_embedding = UMAP_reducer.transform(pca_pseudo_CLR_transformed)\n",
    "\n",
    "plt.rcParams['figure.figsize'] = [5,5]\n",
    "print(\"UMAP of CLR transformed data, PCA (65PCs, 0.95 variance), Euclidean distance\")\n",
    "plt.scatter(UMAP_embedding[:,0], UMAP_embedding[:,1], c=\"black\", s=s)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "data = UMAP_embedding\n",
    "metric = \"euclidean\"\n",
    "method=\"ward\"\n",
    "linkage = sch.linkage(data, method=method, metric=metric)\n",
    "plt.rcParams['figure.figsize'] = (6.97, 3)\n",
    "plt.rcParams.update({'font.size': 6})\n",
    "dendrogram = sch.dendrogram(linkage, truncate_mode=None, p=1, leaf_font_size = 0)\n",
    "axis = plt.gca()\n",
    "axis.set_xticks([])\n",
    "axis.set_xticklabels([])\n",
    "axis.set_ylabel(\"distance\", size=6, x=1)\n",
    "plt.show()\n",
    "\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.metrics import davies_bouldin_score\n",
    "plt.rcParams['figure.figsize'] = (3.32, 3.32*(1/1))\n",
    "y_top_limit = linkage[-1][2]\n",
    "y_bottom_limit = linkage[0][2]\n",
    "no_clusters = []\n",
    "silhouette_scores = []\n",
    "for distance in np.linspace(y_bottom_limit,y_top_limit,1000)[:-1]:\n",
    "    clusters = sch.fcluster(linkage, distance, criterion='distance')\n",
    "    silhouette_scores.append(silhouette_score(data, clusters, metric=metric, random_state=0))\n",
    "    no_clusters.append(len(np.unique(clusters)))\n",
    "\n",
    "fig, ax1 = plt.subplots()\n",
    "plt.rcParams['figure.figsize'] = [5,5]\n",
    "ax1.plot(np.linspace(y_bottom_limit,y_top_limit,1000)[:-1], no_clusters, c=\"magenta\")\n",
    "ax2 = ax1.twinx()\n",
    "ax2.plot(np.linspace(y_bottom_limit,y_top_limit,1000)[:-1], silhouette_scores, c=\"cyan\")\n",
    "ax1.set_ylabel(\"No. clusters\", c=\"magenta\")\n",
    "ax2.set_ylabel(\"Silhouette score\", c=\"cyan\")\n",
    "ax1.set_xlabel(\"Distance threshold\")\n",
    "print(\"Best 20 cluster numbers: \", np.array(no_clusters)[np.argsort(silhouette_scores)[-20:]])\n",
    "print(\"Best 20 distances: \", np.array(np.linspace(y_bottom_limit,y_top_limit,1000)[:-1])[np.argsort(silhouette_scores)[-20:]])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=0.95)\n",
    "pca.fit(pseudo_CLR_transformed)\n",
    "pca_pseudo_CLR_transformed = pca.transform(pseudo_CLR_transformed)\n",
    "print(\"PCA components: \",pca.explained_variance_ratio_.shape)\n",
    "UMAP_reducer = umap.UMAP(random_state=1, n_neighbors=1737, n_components=5, n_epochs=1000, transform_queue_size=10)\n",
    "UMAP_reducer.fit(pca_pseudo_CLR_transformed)\n",
    "UMAP_embedding = UMAP_reducer.transform(pca_pseudo_CLR_transformed)\n",
    "\n",
    "plt.rcParams['figure.figsize'] = [5,5]\n",
    "print(\"UMAP of CLR transformed data, PCA (65PCs, 0.95 variance), Euclidean distance\")\n",
    "plt.scatter(UMAP_embedding[:,0], UMAP_embedding[:,1], c=\"black\", s=s)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "data = UMAP_embedding\n",
    "metric = \"euclidean\"\n",
    "method=\"ward\"\n",
    "linkage = sch.linkage(data, method=method, metric=metric)\n",
    "plt.rcParams['figure.figsize'] = (6.97, 3)\n",
    "plt.rcParams.update({'font.size': 6})\n",
    "dendrogram = sch.dendrogram(linkage, truncate_mode=None, p=1, leaf_font_size = 0)\n",
    "axis = plt.gca()\n",
    "axis.set_xticks([])\n",
    "axis.set_xticklabels([])\n",
    "axis.set_ylabel(\"distance\", size=6, x=1)\n",
    "plt.show()\n",
    "\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.metrics import davies_bouldin_score\n",
    "plt.rcParams['figure.figsize'] = (3.32, 3.32*(1/1))\n",
    "y_top_limit = linkage[-1][2]\n",
    "y_bottom_limit = linkage[0][2]\n",
    "no_clusters = []\n",
    "silhouette_scores = []\n",
    "for distance in np.linspace(y_bottom_limit,y_top_limit,1000)[:-1]:\n",
    "    clusters = sch.fcluster(linkage, distance, criterion='distance')\n",
    "    silhouette_scores.append(silhouette_score(data, clusters, metric=metric, random_state=0))\n",
    "    no_clusters.append(len(np.unique(clusters)))\n",
    "\n",
    "fig, ax1 = plt.subplots()\n",
    "plt.rcParams['figure.figsize'] = [5,5]\n",
    "ax1.plot(np.linspace(y_bottom_limit,y_top_limit,1000)[:-1], no_clusters, c=\"magenta\")\n",
    "ax2 = ax1.twinx()\n",
    "ax2.plot(np.linspace(y_bottom_limit,y_top_limit,1000)[:-1], silhouette_scores, c=\"cyan\")\n",
    "ax1.set_ylabel(\"No. clusters\", c=\"magenta\")\n",
    "ax2.set_ylabel(\"Silhouette score\", c=\"cyan\")\n",
    "ax1.set_xlabel(\"Distance threshold\")\n",
    "print(\"Best 20 cluster numbers: \", np.array(no_clusters)[np.argsort(silhouette_scores)[-20:]])\n",
    "print(\"Best 20 distances: \", np.array(np.linspace(y_bottom_limit,y_top_limit,1000)[:-1])[np.argsort(silhouette_scores)[-20:]])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "7.254446096396663from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=0.95)\n",
    "pca.fit(pseudo_CLR_transformed)\n",
    "pca_pseudo_CLR_transformed = pca.transform(pseudo_CLR_transformed)\n",
    "print(\"PCA components: \",pca.explained_variance_ratio_.shape)\n",
    "UMAP_reducer = umap.UMAP(random_state=1, n_neighbors=1737)#, n_components=5, n_epochs=1000, transform_queue_size=10)\n",
    "UMAP_reducer.fit(pca_pseudo_CLR_transformed)\n",
    "UMAP_embedding = UMAP_reducer.transform(pca_pseudo_CLR_transformed)\n",
    "\n",
    "plt.rcParams['figure.figsize'] = [5,5]\n",
    "print(\"UMAP of CLR transformed data, PCA (65PCs, 0.95 variance), Euclidean distance\")\n",
    "plt.scatter(UMAP_embedding[:,0], UMAP_embedding[:,1], c=\"black\", s=s)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "data = UMAP_embedding\n",
    "metric = \"euclidean\"\n",
    "method=\"ward\"\n",
    "linkage = sch.linkage(data, method=method, metric=metric)\n",
    "plt.rcParams['figure.figsize'] = (6.97, 3)\n",
    "plt.rcParams.update({'font.size': 6})\n",
    "dendrogram = sch.dendrogram(linkage, truncate_mode=None, p=1, leaf_font_size = 0)\n",
    "axis = plt.gca()\n",
    "axis.set_xticks([])\n",
    "axis.set_xticklabels([])\n",
    "axis.set_ylabel(\"distance\", size=6, x=1)\n",
    "plt.show()\n",
    "\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.metrics import davies_bouldin_score\n",
    "plt.rcParams['figure.figsize'] = (3.32, 3.32*(1/1))\n",
    "y_top_limit = linkage[-1][2]\n",
    "y_bottom_limit = linkage[0][2]\n",
    "no_clusters = []\n",
    "silhouette_scores = []\n",
    "for distance in np.linspace(y_bottom_limit,y_top_limit,1000)[:-1]:\n",
    "    clusters = sch.fcluster(linkage, distance, criterion='distance')\n",
    "    silhouette_scores.append(silhouette_score(data, clusters, metric=metric, random_state=0))\n",
    "    no_clusters.append(len(np.unique(clusters)))\n",
    "\n",
    "fig, ax1 = plt.subplots()\n",
    "plt.rcParams['figure.figsize'] = [5,5]\n",
    "ax1.plot(np.linspace(y_bottom_limit,y_top_limit,1000)[:-1], no_clusters, c=\"magenta\")\n",
    "ax2 = ax1.twinx()\n",
    "ax2.plot(np.linspace(y_bottom_limit,y_top_limit,1000)[:-1], silhouette_scores, c=\"cyan\")\n",
    "ax1.set_ylabel(\"No. clusters\", c=\"magenta\")\n",
    "ax2.set_ylabel(\"Silhouette score\", c=\"cyan\")\n",
    "ax1.set_xlabel(\"Distance threshold\")\n",
    "print(\"Best 20 cluster numbers: \", np.array(no_clusters)[np.argsort(silhouette_scores)[-20:]])\n",
    "print(\"Best 20 distances: \", np.array(np.linspace(y_bottom_limit,y_top_limit,1000)[:-1])[np.argsort(silhouette_scores)[-20:]])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=0.95)\n",
    "pca.fit(pseudo_CLR_transformed)\n",
    "pca_pseudo_CLR_transformed = pca.transform(pseudo_CLR_transformed)\n",
    "print(\"PCA components: \",pca.explained_variance_ratio_.shape)\n",
    "UMAP_reducer = umap.UMAP(random_state=0)#, n_neighbors=1737)#, n_components=5, n_epochs=1000, transform_queue_size=10)\n",
    "UMAP_reducer.fit(pca_pseudo_CLR_transformed)\n",
    "UMAP_embedding = UMAP_reducer.transform(pca_pseudo_CLR_transformed)\n",
    "\n",
    "plt.rcParams['figure.figsize'] = [5,5]\n",
    "print(\"UMAP of CLR transformed data, PCA (65PCs, 0.95 variance), Euclidean distance\")\n",
    "plt.scatter(UMAP_embedding[:,0], UMAP_embedding[:,1], c=\"black\", s=s)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "data = UMAP_embedding\n",
    "metric = \"euclidean\"\n",
    "method=\"ward\"\n",
    "linkage = sch.linkage(data, method=method, metric=metric)\n",
    "plt.rcParams['figure.figsize'] = (6.97, 3)\n",
    "plt.rcParams.update({'font.size': 6})\n",
    "dendrogram = sch.dendrogram(linkage, truncate_mode=None, p=1, leaf_font_size = 0)\n",
    "axis = plt.gca()\n",
    "axis.set_xticks([])\n",
    "axis.set_xticklabels([])\n",
    "axis.set_ylabel(\"distance\", size=6, x=1)\n",
    "plt.show()\n",
    "\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.metrics import davies_bouldin_score\n",
    "plt.rcParams['figure.figsize'] = (3.32, 3.32*(1/1))\n",
    "y_top_limit = linkage[-1][2]\n",
    "y_bottom_limit = linkage[0][2]\n",
    "no_clusters = []\n",
    "silhouette_scores = []\n",
    "for distance in np.linspace(y_bottom_limit,y_top_limit,1000)[:-1]:\n",
    "    clusters = sch.fcluster(linkage, distance, criterion='distance')\n",
    "    silhouette_scores.append(silhouette_score(data, clusters, metric=metric, random_state=0))\n",
    "    no_clusters.append(len(np.unique(clusters)))\n",
    "\n",
    "fig, ax1 = plt.subplots()\n",
    "plt.rcParams['figure.figsize'] = [5,5]\n",
    "ax1.plot(np.linspace(y_bottom_limit,y_top_limit,1000)[:-1], no_clusters, c=\"magenta\")\n",
    "ax2 = ax1.twinx()\n",
    "ax2.plot(np.linspace(y_bottom_limit,y_top_limit,1000)[:-1], silhouette_scores, c=\"cyan\")\n",
    "ax1.set_ylabel(\"No. clusters\", c=\"magenta\")\n",
    "ax2.set_ylabel(\"Silhouette score\", c=\"cyan\")\n",
    "ax1.set_xlabel(\"Distance threshold\")\n",
    "print(\"Best 20 cluster numbers: \", np.array(no_clusters)[np.argsort(silhouette_scores)[-20:]])\n",
    "print(\"Best 20 distances: \", np.array(np.linspace(y_bottom_limit,y_top_limit,1000)[:-1])[np.argsort(silhouette_scores)[-20:]])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=0.95)\n",
    "pca.fit(pseudo_CLR_transformed)\n",
    "pca_pseudo_CLR_transformed = pca.transform(pseudo_CLR_transformed)\n",
    "print(\"PCA components: \",pca.explained_variance_ratio_.shape)\n",
    "UMAP_reducer = umap.UMAP(random_state=0,min_dist=0.1, n_neighbors=1737, n_components=10, n_epochs=10000, transform_queue_size=10)\n",
    "UMAP_reducer.fit(pca_pseudo_CLR_transformed)\n",
    "UMAP_embedding = UMAP_reducer.transform(pca_pseudo_CLR_transformed)\n",
    "\n",
    "plt.rcParams['figure.figsize'] = [5,5]\n",
    "print(\"UMAP of CLR transformed data, PCA (65PCs, 0.95 variance), Euclidean distance\")\n",
    "plt.scatter(UMAP_embedding[:,0], UMAP_embedding[:,1], c=\"black\", s=s)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "data = UMAP_embedding\n",
    "metric = \"euclidean\"\n",
    "method=\"ward\"\n",
    "linkage = sch.linkage(data, method=method, metric=metric)\n",
    "plt.rcParams['figure.figsize'] = (6.97, 3)\n",
    "plt.rcParams.update({'font.size': 6})\n",
    "dendrogram = sch.dendrogram(linkage, truncate_mode=None, p=1, leaf_font_size = 0)\n",
    "axis = plt.gca()\n",
    "axis.set_xticks([])\n",
    "axis.set_xticklabels([])\n",
    "axis.set_ylabel(\"distance\", size=6, x=1)\n",
    "plt.show()\n",
    "\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.metrics import calinski_harabasz_score\n",
    "plt.rcParams['figure.figsize'] = (3.32, 3.32*(1/1))\n",
    "y_top_limit = linkage[-1][2]\n",
    "y_bottom_limit = linkage[0][2]\n",
    "no_clusters = []\n",
    "# silhouette_scores0 = []\n",
    "for distance in np.linspace(y_bottom_limit,y_top_limit,1000)[:-1]:\n",
    "    clusters = sch.fcluster(linkage, distance, criterion='distance')\n",
    "#     silhouette_scores0.append(davies_bouldin_score(data, clusters))#, metric=metric, random_state=0))\n",
    "    no_clusters.append(len(np.unique(clusters)))\n",
    "\n",
    "fig, ax1 = plt.subplots()\n",
    "plt.rcParams['figure.figsize'] = [5,5]\n",
    "ax1.plot(np.linspace(y_bottom_limit,y_top_limit,1000)[:-1], no_clusters, c=\"magenta\")\n",
    "# ax2 = ax1.twinx()\n",
    "# ax2.plot(np.linspace(y_bottom_limit,y_top_limit,1000)[:-1], silhouette_scores0, c=\"cyan\")\n",
    "ax1.set_ylabel(\"No. clusters\", c=\"magenta\")\n",
    "# ax2.set_ylabel(\"Silhouette score\", c=\"cyan\")\n",
    "ax1.set_xlabel(\"Distance threshold\")\n",
    "# print(\"Best 20 cluster numbers: \", np.array(no_clusters0)[np.argsort(silhouette_scores0)[-20:]])\n",
    "# print(\"Best 20 distances: \", np.array(np.linspace(y_bottom_limit,y_top_limit,1000)[:-1])[np.argsort(silhouette_scores0)[-20:]])\n",
    "plt.show()\n",
    "\n",
    "from kneed import KneeLocator\n",
    "kn = KneeLocator(np.linspace(y_bottom_limit,y_top_limit,1000)[:-1], no_clusters0, curve='convex', direction='decreasing')\n",
    "print(kn.knee)\n",
    "np.unique(sch.fcluster(linkage, kn.knee, criterion='distance'), return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=0.95)\n",
    "pca.fit(pseudo_CLR_transformed)\n",
    "pca_pseudo_CLR_transformed = pca.transform(pseudo_CLR_transformed)\n",
    "print(\"PCA components: \",pca.explained_variance_ratio_.shape)\n",
    "UMAP_reducer = umap.UMAP(random_state=1,min_dist=0.1, n_neighbors=1737, n_components=10, n_epochs=10000, transform_queue_size=10)\n",
    "UMAP_reducer.fit(pca_pseudo_CLR_transformed)\n",
    "UMAP_embedding = UMAP_reducer.transform(pca_pseudo_CLR_transformed)\n",
    "\n",
    "plt.rcParams['figure.figsize'] = [5,5]\n",
    "print(\"UMAP of CLR transformed data, PCA (65PCs, 0.95 variance), Euclidean distance\")\n",
    "plt.scatter(UMAP_embedding[:,0], UMAP_embedding[:,1], c=\"black\", s=s)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "data = UMAP_embedding\n",
    "metric = \"euclidean\"\n",
    "method=\"ward\"\n",
    "linkage = sch.linkage(data, method=method, metric=metric)\n",
    "plt.rcParams['figure.figsize'] = (6.97, 3)\n",
    "plt.rcParams.update({'font.size': 6})\n",
    "dendrogram = sch.dendrogram(linkage, truncate_mode=None, p=1, leaf_font_size = 0)\n",
    "axis = plt.gca()\n",
    "axis.set_xticks([])\n",
    "axis.set_xticklabels([])\n",
    "axis.set_ylabel(\"distance\", size=6, x=1)\n",
    "plt.show()\n",
    "\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.metrics import calinski_harabasz_score\n",
    "plt.rcParams['figure.figsize'] = (3.32, 3.32*(1/1))\n",
    "y_top_limit = linkage[-1][2]\n",
    "y_bottom_limit = linkage[0][2]\n",
    "no_clusters = []\n",
    "# silhouette_scores0 = []\n",
    "for distance in np.linspace(y_bottom_limit,y_top_limit,1000)[:-1]:\n",
    "    clusters = sch.fcluster(linkage, distance, criterion='distance')\n",
    "#     silhouette_scores0.append(davies_bouldin_score(data, clusters))#, metric=metric, random_state=0))\n",
    "    no_clusters.append(len(np.unique(clusters)))\n",
    "\n",
    "fig, ax1 = plt.subplots()\n",
    "plt.rcParams['figure.figsize'] = [5,5]\n",
    "ax1.plot(np.linspace(y_bottom_limit,y_top_limit,1000)[:-1], no_clusters, c=\"magenta\")\n",
    "# ax2 = ax1.twinx()\n",
    "# ax2.plot(np.linspace(y_bottom_limit,y_top_limit,1000)[:-1], silhouette_scores0, c=\"cyan\")\n",
    "ax1.set_ylabel(\"No. clusters\", c=\"magenta\")\n",
    "# ax2.set_ylabel(\"Silhouette score\", c=\"cyan\")\n",
    "ax1.set_xlabel(\"Distance threshold\")\n",
    "# print(\"Best 20 cluster numbers: \", np.array(no_clusters0)[np.argsort(silhouette_scores0)[-20:]])\n",
    "# print(\"Best 20 distances: \", np.array(np.linspace(y_bottom_limit,y_top_limit,1000)[:-1])[np.argsort(silhouette_scores0)[-20:]])\n",
    "plt.show()\n",
    "\n",
    "from kneed import KneeLocator\n",
    "kn = KneeLocator(np.linspace(y_bottom_limit,y_top_limit,1000)[:-1], no_clusters0, curve='convex', direction='decreasing')\n",
    "print(kn.knee)\n",
    "np.unique(sch.fcluster(linkage, kn.knee, criterion='distance'), return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=0.95)\n",
    "pca.fit(pseudo_CLR_transformed)\n",
    "pca_pseudo_CLR_transformed = pca.transform(pseudo_CLR_transformed)\n",
    "print(\"PCA components: \",pca.explained_variance_ratio_.shape)\n",
    "UMAP_reducer = umap.UMAP(random_state=2,min_dist=0.1, n_neighbors=1737, n_components=10, n_epochs=10000, transform_queue_size=10)\n",
    "UMAP_reducer.fit(pca_pseudo_CLR_transformed)\n",
    "UMAP_embedding = UMAP_reducer.transform(pca_pseudo_CLR_transformed)\n",
    "\n",
    "plt.rcParams['figure.figsize'] = [5,5]\n",
    "print(\"UMAP of CLR transformed data, PCA (65PCs, 0.95 variance), Euclidean distance\")\n",
    "plt.scatter(UMAP_embedding[:,0], UMAP_embedding[:,1], c=\"black\", s=s)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "data = UMAP_embedding\n",
    "metric = \"euclidean\"\n",
    "method=\"ward\"\n",
    "linkage = sch.linkage(data, method=method, metric=metric)\n",
    "plt.rcParams['figure.figsize'] = (6.97, 3)\n",
    "plt.rcParams.update({'font.size': 6})\n",
    "dendrogram = sch.dendrogram(linkage, truncate_mode=None, p=1, leaf_font_size = 0)\n",
    "axis = plt.gca()\n",
    "axis.set_xticks([])\n",
    "axis.set_xticklabels([])\n",
    "axis.set_ylabel(\"distance\", size=6, x=1)\n",
    "plt.show()\n",
    "\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.metrics import calinski_harabasz_score\n",
    "plt.rcParams['figure.figsize'] = (3.32, 3.32*(1/1))\n",
    "y_top_limit = linkage[-1][2]\n",
    "y_bottom_limit = linkage[0][2]\n",
    "no_clusters = []\n",
    "# silhouette_scores0 = []\n",
    "for distance in np.linspace(y_bottom_limit,y_top_limit,1000)[:-1]:\n",
    "    clusters = sch.fcluster(linkage, distance, criterion='distance')\n",
    "#     silhouette_scores0.append(davies_bouldin_score(data, clusters))#, metric=metric, random_state=0))\n",
    "    no_clusters.append(len(np.unique(clusters)))\n",
    "\n",
    "fig, ax1 = plt.subplots()\n",
    "plt.rcParams['figure.figsize'] = [5,5]\n",
    "ax1.plot(np.linspace(y_bottom_limit,y_top_limit,1000)[:-1], no_clusters, c=\"magenta\")\n",
    "# ax2 = ax1.twinx()\n",
    "# ax2.plot(np.linspace(y_bottom_limit,y_top_limit,1000)[:-1], silhouette_scores0, c=\"cyan\")\n",
    "ax1.set_ylabel(\"No. clusters\", c=\"magenta\")\n",
    "# ax2.set_ylabel(\"Silhouette score\", c=\"cyan\")\n",
    "ax1.set_xlabel(\"Distance threshold\")\n",
    "# print(\"Best 20 cluster numbers: \", np.array(no_clusters0)[np.argsort(silhouette_scores0)[-20:]])\n",
    "# print(\"Best 20 distances: \", np.array(np.linspace(y_bottom_limit,y_top_limit,1000)[:-1])[np.argsort(silhouette_scores0)[-20:]])\n",
    "plt.show()\n",
    "\n",
    "from kneed import KneeLocator\n",
    "kn = KneeLocator(np.linspace(y_bottom_limit,y_top_limit,1000)[:-1], no_clusters0, curve='convex', direction='decreasing')\n",
    "print(kn.knee)\n",
    "np.unique(sch.fcluster(linkage, kn.knee, criterion='distance'), return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from kneed import KneeLocator\n",
    "kn = KneeLocator(np.linspace(y_bottom_limit,y_top_limit,1000)[:-1], no_clusters11, curve='convex', direction='decreasing')\n",
    "print(kn.knee)\n",
    "np.unique(sch.fcluster(linkage, kn.knee, criterion='distance'), return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(sch.fcluster(linkage, kn.knee, criterion='distance'), return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_this = sch.fcluster(linkage, kn.knee, criterion='distance')\n",
    "clusters52_split = np.split(np.argsort(split_this), indices_or_sections=np.cumsum(np.unique(split_this, return_counts=True)[1][:-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=0.95)\n",
    "pca.fit(pseudo_CLR_transformed)\n",
    "pca_pseudo_CLR_transformed = pca.transform(pseudo_CLR_transformed)\n",
    "print(\"PCA components: \",pca.explained_variance_ratio_.shape)\n",
    "UMAP_reducer = umap.UMAP(random_state=2,min_dist=0.1, n_neighbors=1737, n_components=30, n_epochs=1000, transform_queue_size=10)\n",
    "UMAP_reducer.fit(pca_pseudo_CLR_transformed)\n",
    "UMAP_embedding = UMAP_reducer.transform(pca_pseudo_CLR_transformed)\n",
    "\n",
    "plt.rcParams['figure.figsize'] = [5,5]\n",
    "print(\"UMAP of CLR transformed data, PCA (65PCs, 0.95 variance), Euclidean distance\")\n",
    "plt.scatter(UMAP_embedding[:,0], UMAP_embedding[:,1], c=\"black\", s=s)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "data = UMAP_embedding\n",
    "metric = \"euclidean\"\n",
    "method=\"ward\"\n",
    "linkage = sch.linkage(data, method=method, metric=metric)\n",
    "plt.rcParams['figure.figsize'] = (6.97, 3)\n",
    "plt.rcParams.update({'font.size': 6})\n",
    "dendrogram = sch.dendrogram(linkage, truncate_mode=None, p=1, leaf_font_size = 0)\n",
    "axis = plt.gca()\n",
    "axis.set_xticks([])\n",
    "axis.set_xticklabels([])\n",
    "axis.set_ylabel(\"distance\", size=6, x=1)\n",
    "plt.show()\n",
    "\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.metrics import calinski_harabasz_score\n",
    "plt.rcParams['figure.figsize'] = (3.32, 3.32*(1/1))\n",
    "y_top_limit = linkage[-1][2]\n",
    "y_bottom_limit = linkage[0][2]\n",
    "no_clusters = []\n",
    "# silhouette_scores0 = []\n",
    "for distance in np.linspace(y_bottom_limit,y_top_limit,1000)[:-1]:\n",
    "    clusters = sch.fcluster(linkage, distance, criterion='distance')\n",
    "#     silhouette_scores0.append(davies_bouldin_score(data, clusters))#, metric=metric, random_state=0))\n",
    "    no_clusters.append(len(np.unique(clusters)))\n",
    "\n",
    "fig, ax1 = plt.subplots()\n",
    "plt.rcParams['figure.figsize'] = [5,5]\n",
    "ax1.plot(np.linspace(y_bottom_limit,y_top_limit,1000)[:-1], no_clusters, c=\"magenta\")\n",
    "# ax2 = ax1.twinx()\n",
    "# ax2.plot(np.linspace(y_bottom_limit,y_top_limit,1000)[:-1], silhouette_scores0, c=\"cyan\")\n",
    "ax1.set_ylabel(\"No. clusters\", c=\"magenta\")\n",
    "# ax2.set_ylabel(\"Silhouette score\", c=\"cyan\")\n",
    "ax1.set_xlabel(\"Distance threshold\")\n",
    "# print(\"Best 20 cluster numbers: \", np.array(no_clusters0)[np.argsort(silhouette_scores0)[-20:]])\n",
    "# print(\"Best 20 distances: \", np.array(np.linspace(y_bottom_limit,y_top_limit,1000)[:-1])[np.argsort(silhouette_scores0)[-20:]])\n",
    "plt.show()\n",
    "\n",
    "from kneed import KneeLocator\n",
    "kn = KneeLocator(np.linspace(y_bottom_limit,y_top_limit,1000)[:-1], no_clusters0, curve='convex', direction='decreasing')\n",
    "print(kn.knee)\n",
    "np.unique(sch.fcluster(linkage, kn.knee, criterion='distance'), return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=0.95)\n",
    "pca.fit(pseudo_CLR_transformed)\n",
    "pca_pseudo_CLR_transformed = pca.transform(pseudo_CLR_transformed)\n",
    "print(\"PCA components: \",pca.explained_variance_ratio_.shape)\n",
    "UMAP_reducer = umap.UMAP(random_state=1)#2,min_dist=0.1, n_neighbors=1737, n_components=30, n_epochs=1000, transform_queue_size=10)\n",
    "UMAP_reducer.fit(pca_pseudo_CLR_transformed)\n",
    "UMAP_embedding = UMAP_reducer.transform(pca_pseudo_CLR_transformed)\n",
    "\n",
    "plt.rcParams['figure.figsize'] = [5,5]\n",
    "print(\"UMAP of CLR transformed data, PCA (65PCs, 0.95 variance), Euclidean distance\")\n",
    "plt.scatter(UMAP_embedding[:,0], UMAP_embedding[:,1], c=\"black\", s=s)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "data = UMAP_embedding\n",
    "metric = \"euclidean\"\n",
    "method=\"ward\"\n",
    "linkage = sch.linkage(data, method=method, metric=metric)\n",
    "plt.rcParams['figure.figsize'] = (6.97, 3)\n",
    "plt.rcParams.update({'font.size': 6})\n",
    "dendrogram = sch.dendrogram(linkage, truncate_mode=None, p=1, leaf_font_size = 0)\n",
    "axis = plt.gca()\n",
    "axis.set_xticks([])\n",
    "axis.set_xticklabels([])\n",
    "axis.set_ylabel(\"distance\", size=6, x=1)\n",
    "plt.show()\n",
    "\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.metrics import calinski_harabasz_score\n",
    "plt.rcParams['figure.figsize'] = (3.32, 3.32*(1/1))\n",
    "y_top_limit = linkage[-1][2]\n",
    "y_bottom_limit = linkage[0][2]\n",
    "no_clusters = []\n",
    "silhouette_scores0 = []\n",
    "for distance in np.linspace(y_bottom_limit,y_top_limit,1000)[:-1]:\n",
    "    clusters = sch.fcluster(linkage, distance, criterion='distance')\n",
    "    silhouette_scores0.append(silhouette_score(data, clusters))#, metric=metric, random_state=0))\n",
    "    no_clusters.append(len(np.unique(clusters)))\n",
    "\n",
    "fig, ax1 = plt.subplots()\n",
    "plt.rcParams['figure.figsize'] = [5,5]\n",
    "ax1.plot(np.linspace(y_bottom_limit,y_top_limit,1000)[:-1], no_clusters, c=\"magenta\")\n",
    "ax2 = ax1.twinx()\n",
    "ax2.plot(np.linspace(y_bottom_limit,y_top_limit,1000)[:-1], silhouette_scores0, c=\"cyan\")\n",
    "ax1.set_ylabel(\"No. clusters\", c=\"magenta\")\n",
    "ax2.set_ylabel(\"Silhouette score\", c=\"cyan\")\n",
    "ax1.set_xlabel(\"Distance threshold\")\n",
    "print(\"Best 20 cluster numbers: \", np.array(no_clusters0)[np.argsort(silhouette_scores0)[-20:]])\n",
    "print(\"Best 20 distances: \", np.array(np.linspace(y_bottom_limit,y_top_limit,1000)[:-1])[np.argsort(silhouette_scores0)[-20:]])\n",
    "plt.show()\n",
    "\n",
    "from kneed import KneeLocator\n",
    "kn = KneeLocator(np.linspace(y_bottom_limit,y_top_limit,1000)[:-1], no_clusters0, curve='convex', direction='decreasing')\n",
    "print(kn.knee)\n",
    "np.unique(sch.fcluster(linkage, kn.knee, criterion='distance'), return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.metrics import calinski_harabasz_score\n",
    "plt.rcParams['figure.figsize'] = (3.32, 3.32*(1/1))\n",
    "y_top_limit = linkage[-1][2]\n",
    "y_bottom_limit = linkage[0][2]\n",
    "# no_clusters = []\n",
    "# silhouette_scores0 = []\n",
    "# for distance in np.linspace(y_bottom_limit,y_top_limit,1000)[:-1]:\n",
    "#     clusters = sch.fcluster(linkage, distance, criterion='distance')\n",
    "#     silhouette_scores0.append(silhouette_score(data, clusters))#, metric=metric, random_state=0))\n",
    "#     no_clusters.append(len(np.unique(clusters)))\n",
    "\n",
    "fig, ax1 = plt.subplots()\n",
    "plt.rcParams['figure.figsize'] = [3.32,3.32]\n",
    "ax1.axvline(8.133220147326256, c=\"cyan\")\n",
    "ax1.axhline(29, c=\"cyan\")\n",
    "\n",
    "ax1.plot(np.linspace(y_bottom_limit,y_top_limit,1000)[:-1], no_clusters, c=\"magenta\")\n",
    "# ax2 = ax1.twinx()\n",
    "# ax2.plot(np.linspace(y_bottom_limit,y_top_limit,1000)[:-1], silhouette_scores0, c=\"cyan\")\n",
    "ax1.set_ylabel(\"No. clusters\")\n",
    "# ax2.set_ylabel(\"Silhouette score\", c=\"cyan\")\n",
    "ax1.set_xlabel(\"Distance threshold\")\n",
    "print(\"Best 20 cluster numbers: \", np.array(no_clusters0)[np.argsort(silhouette_scores0)[-20:]])\n",
    "print(\"Best 20 distances: \", np.array(np.linspace(y_bottom_limit,y_top_limit,1000)[:-1])[np.argsort(silhouette_scores0)[-20:]])\n",
    "plt.savefig('figures/agglomerative_knee.png', dpi=300, bbox_inches = 'tight',pad_inches = 0.01)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = UMAP_embedding\n",
    "metric = \"euclidean\"\n",
    "method=\"ward\"\n",
    "linkage = sch.linkage(data, method=method, metric=metric)\n",
    "plt.rcParams['figure.figsize'] = (6.97, 3)\n",
    "plt.rcParams.update({'font.size': 6})\n",
    "dendrogram = sch.dendrogram(linkage, truncate_mode=None, p=1, leaf_font_size = 0, color_threshold=8.133220147326256)\n",
    "axis = plt.gca()\n",
    "axis.set_xticks([])\n",
    "axis.set_xticklabels([])\n",
    "axis.set_ylabel(\"Distance threshold\", size=6, x=1)\n",
    "axis.set_xlabel(\"Leaves of the dendogram\", size=6)\n",
    "plt.savefig('figures/agglomerative_dendogram.png', dpi=300, bbox_inches = 'tight',pad_inches = 0.01)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(sch.fcluster(linkage, kn.knee, criterion='distance')).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(clusters29)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters29 = sch.fcluster(linkage, kn.knee, criterion='distance')-1\n",
    "clusters29_split = np.split(np.argsort(clusters31), indices_or_sections=np.cumsum(np.unique(clusters31, return_counts=True)[1][:-1]))\n",
    "\n",
    "plt.rcParams['figure.figsize'] = [3.32,3.32]\n",
    "\n",
    "print(\"UMAP of CLR transformed data, PCA (65PCs, 0.95 variance), Euclidean distance\")\n",
    "# plt.scatter(embedding[:,0], embedding[:,1], c=\"black\", s=s)\n",
    "for cluster in clusters29_split:\n",
    "    plt.scatter(UMAP_embedding[:,0][cluster], UMAP_embedding[:,1][cluster], s=1)\n",
    "plt.xlabel(\"UMAP axis 1\")\n",
    "plt.ylabel(\"UMAP axis 2\")\n",
    "\n",
    "plt.savefig('figures/UMAP_agglomerative.png', dpi=300, bbox_inches = 'tight',pad_inches = 0.01)\n",
    "\n",
    "plt.show()\n",
    "n+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pseudo_CLR_transformed.index.values[0] in ob_state.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ob_state[pseudo_CLR_transformed.index.values[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_classification = clusters29\n",
    "\n",
    "Belloni_classes = np.array([\"alpha\", \"beta\", \"gamma\", \"delta\", \"theta\", \"kappa\", \"lambda\", \"mu\", \"nu\", \"rho\", \"phi\", \"chi\", \"eta\", \"omega\", \"Unknown\"])\n",
    "\n",
    "comparison_matrix = np.zeros((len(np.unique(new_classification)), len(Belloni_classes)), dtype=int)\n",
    "\n",
    "comparison_matrix_df = pd.DataFrame(comparison_matrix, columns=Belloni_classes, index=np.unique(new_classification))\n",
    "\n",
    "for n_Bc, Belloni_class in enumerate(Belloni_classes):\n",
    "    if \n",
    "    Belloni_class_indices = np.where(np.array(segment_class) == Belloni_class)[0]\n",
    "    count_clusters_for_class = np.unique(np.take(new_classification, Belloni_class_indices), return_counts=True)\n",
    "    for cluster_ind, cluster in enumerate(count_clusters_for_class[0]):\n",
    "        comparison_matrix_df[Belloni_class][cluster] = count_clusters_for_class[1][cluster_ind]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_normalized_comparison_matrix_df=(comparison_matrix_df-comparison_matrix_df.min())/(comparison_matrix_df.max()-comparison_matrix_df.min())\n",
    "known_comparison_matrix_df = comparison_matrix_df.drop(columns=['Unknown']).T\n",
    "component_normalized_comparison_matrix_df = (known_comparison_matrix_df-known_comparison_matrix_df.min())/(known_comparison_matrix_df.max()-known_comparison_matrix_df.min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find particularly class-homogeneous components\n",
    "\n",
    "good_comps = {}\n",
    "for comp in range(122):\n",
    "    comp_counts = comparison_matrix_df.iloc[comp,:].sort_values(ascending=False)\n",
    "    comp_counts_nonzero = comp_counts.where(comp_counts>0).dropna().astype(int)\n",
    "    if comp_counts_nonzero.index[0] == \"Unknown\":\n",
    "        comp_class_proportion = comp_counts_nonzero[1:]/comp_counts_nonzero[1:].sum()\n",
    "        if comp_class_proportion[0]>0.999:\n",
    "            good_comps[comp] = comp_counts_nonzero\n",
    "    else:\n",
    "        good_comps[comp] = comp_counts_nonzero\n",
    "print(len(good_comps))\n",
    "# dominated_comps = []\n",
    "# for i,v in good_comps.items():\n",
    "#     if v.index[1] == \"chi\":\n",
    "#         dominated_comps.append(i)\n",
    "# print(dominated_comps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chi is >95% bar unknowns : (10), 3, 5, 7, 15, 24, 28, 37, 44, 53, 57, 60, 69, 76, 91, 103, 112\n",
    "# kappa : (79),  4, 42, 78, 104 (87 has 90.8% kappa)\n",
    "# theta : 25, 26, 71\n",
    "# lambda : 66\n",
    "# rho : 12, 18, 19, 20, 22, 23, 30, 38, 45, 49, 50, 54, 56, 63, 65, 74, 77, 86, 97, 107\n",
    "\n",
    "print(good_comps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comparison_matrix_df.iloc[98]#/(comparison_matrix_df.iloc[87].sum()-2628)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comp_list = []\n",
    "for i,v in good_comps.items():\n",
    "    if len(v)>1:\n",
    "        comp_list.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(comp_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comp_counts = comparison_matrix_df.iloc[7,:].sort_values(ascending=False)\n",
    "comp_counts_nonzero = comp_counts.where(comp_counts>0).dropna().astype(int)\n",
    "comp_counts_nonzero[1:]/comp_counts_nonzero[1:].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comparison_matrix_df.sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comparison_matrix_df.T.iloc[:,66]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(0.002524/1.002524)*7617"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(shape_moments_GM114_labels, return_counts=True)[1][10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.where(comparison_matrix_df.T.idxmax().values != \"Unknown\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = (6.97, 6.97*(1/3))\n",
    "import seaborn as sns\n",
    "\n",
    "from matplotlib.ticker import (MultipleLocator, FormatStrFormatter,\n",
    "                               AutoMinorLocator)\n",
    "\n",
    "\n",
    "\n",
    "ax = sns.heatmap(class_normalized_comparison_matrix_df.T, xticklabels=True, yticklabels=True, cmap='coolwarm')#, linewidth=0.5)\n",
    "# # ax.set_xticklabels(ax.get_xmajorticklabels(), fontsize = 6)\n",
    "# ax.tick_params(axis=\"x\", which=\"major\", length=2, width=0.75, labelsize=6, direction=\"out\")\n",
    "\n",
    "ax.xaxis.set_major_locator(MultipleLocator(4))\n",
    "ax.xaxis.set_major_formatter(FormatStrFormatter('%d'))\n",
    "\n",
    "# For the minor ticks, use no labels; default NullFormatter.\n",
    "# ax.xaxis.set_minor_locator(MultipleLocator(1.5))\n",
    "ax.xaxis.set_minor_locator(AutoMinorLocator())\n",
    "\n",
    "\n",
    "# for label in ax.xaxis.get_majorticklabels():\n",
    "#     label.set_transform(label.get_transform() + 0.5)\n",
    "    \n",
    "# for label in ax.xaxis.get_minorticklabels():\n",
    "#     label.set_transform(label.get_transform() + 0.5)\n",
    "\n",
    "# plt.title(\"Gaussian mixture components' populations in terms of classified data (component-wise min-maxed)\")\n",
    "plt.savefig(\"figures/GMM122vsBelloni_heatmap.png\", dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(sch.fcluster(linkage, kn.knee, criterion='distance'), return_counts=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(clusters53_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_1738 = [id_index for id_index, id_ in enumerate(ids) if id_ in pseudo_CLR_transformed.index.values]\n",
    "lcs_1738 = [lcs[index] for index in index_1738]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters53 = sch.fcluster(linkage, kn.knee, criterion='distance')\n",
    "clusters53_split = np.split(np.argsort(clusters53), indices_or_sections=np.cumsum(np.unique(clusters53, return_counts=True)[1][:-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lc_start_times = []\n",
    "lc_end_times = []\n",
    "for lc in lcs_1738:\n",
    "    lc_start_times.append(lc[0][0])\n",
    "    lc_end_times.append(lc[0][-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(np.sort(lc_end_times) - np.sort(lc_start_times))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pseudo_CLR_transformed.index.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gap_duration = (np.sort(lc_start_times)[1:] - np.sort(lc_end_times)[:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_labels_chronologically = np.take(clusters53, np.argsort(lc_start_times))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = (20, 3)\n",
    "\n",
    "plt.scatter(np.take(lc_start_times, np.argsort(lc_start_times)), cluster_labels_chronologically)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(transitions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gap_trans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "self_trans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transitions=[]\n",
    "self_trans = 0\n",
    "gap_trans = 0\n",
    "for gap_ind, gap in enumerate(gap_duration):\n",
    "    if gap>10e3:\n",
    "        gap_trans+=1\n",
    "    if cluster_labels_chronologically[gap_ind]== cluster_labels_chronologically[gap_ind+1]:\n",
    "        self_trans+=1\n",
    "    if gap<10e3:# and cluster_labels_chronologically[gap_ind]!= cluster_labels_chronologically[gap_ind+1]:\n",
    "        transitions.append((cluster_labels_chronologically[gap_ind], cluster_labels_chronologically[gap_ind+1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transitions_df = pd.DataFrame(np.zeros((53,53)))\n",
    "for trans in transitions:\n",
    "    transitions_df.iloc[trans[0]-1, trans[1]-1] +=1\n",
    "    \n",
    "# transitions_df.columns = np.array(range(53))+1\n",
    "# transitions_df.index = np.array(range(53))+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "transitions_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cmap_discretize(cmap, N):\n",
    "    \"\"\"Return a discrete colormap from the continuous colormap cmap.\n",
    "\n",
    "        cmap: colormap instance, eg. cm.jet. \n",
    "        N: number of colors.\n",
    "\n",
    "    Example\n",
    "        x = resize(arange(100), (5,100))\n",
    "        djet = cmap_discretize(cm.jet, 5)\n",
    "        imshow(x, cmap=djet)\n",
    "    \"\"\"\n",
    "\n",
    "    if type(cmap) == str:\n",
    "        cmap = get_cmap(cmap)\n",
    "    colors_i = concatenate((linspace(0, 1., N), (0.,0.,0.,0.)))\n",
    "    colors_rgba = cmap(colors_i)\n",
    "    indices = linspace(0, 1., N+1)\n",
    "    cdict = {}\n",
    "    for ki,key in enumerate(('red','green','blue')):\n",
    "        cdict[key] = [ (indices[i], colors_rgba[i-1,ki], colors_rgba[i,ki]) for i in xrange(N+1) ]\n",
    "    # Return colormap object.\n",
    "    return matplotlib.colors.LinearSegmentedColormap(cmap.name + \"_%d\"%N, cdict, 1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = [3.32,3.32]\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "import seaborn as sns\n",
    "# trans_heatmap= \n",
    "\n",
    "# cmap = LinearSegmentedColormap.from_list('Custom', myColors, len(myColors))\n",
    "cmap = sns.color_palette(\"rocket\", n_colors=5)# cmap_discretize(cmap, N)\n",
    "cmap_new = []\n",
    "cmap_new.append((cmap[0][0]/4,cmap[0][1]/4,cmap[0][2]/4))\n",
    "for c in cmap[1:]:\n",
    "    cmap_new.append(c)\n",
    "# cmap = [cmap[0],cmap[2], cmap[3], cmap[4],cmap[5]]\n",
    "sns.heatmap(transitions_df, cmap=cmap_new)\n",
    "axes=plt.gca()\n",
    "axes.set_xticks(np.array(range(0,53,3))+0.5)\n",
    "axes.set_xticklabels(list(range(1,54,3)))\n",
    "axes.set_yticks(np.array(range(0,53,3))+0.5)\n",
    "axes.set_yticklabels(list(range(1,54,3)))\n",
    "axes.set_ylabel(\"Initial cluster index\")\n",
    "axes.set_xlabel(\"Final cluster index\")\n",
    "\n",
    "colorbar = axes.collections[0].colorbar\n",
    "colorbar.set_ticks(np.array(range(5))*4/5+2/5)\n",
    "colorbar.set_ticklabels([0,1,2,3,4])\n",
    "\n",
    "# axes[1][1].set_xticks([500,1000,1500,2000])\n",
    "\n",
    "# trans_heatmap.pivot(\"Cluster index\", \"Cluster index\", \"No. transitions\")\n",
    "plt.savefig('figures/transitions_heatmap.png', dpi=300, bbox_inches = 'tight',pad_inches = 0.01)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "pd.Series(np.diag(transitions_df), index=[transitions_df.index, transitions_df.columns])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "##### transitions_df.iloc[np.diag_indices(53)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = [3.32,3.32]\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "import seaborn as sns\n",
    "# trans_heatmap= \n",
    "\n",
    "# cmap = LinearSegmentedColormap.from_list('Custom', myColors, len(myColors))\n",
    "cmap = sns.color_palette(\"rocket\", n_colors=5)# cmap_discretize(cmap, N)\n",
    "cmap_new = []\n",
    "cmap_new.append((cmap[0][0]/4,cmap[0][1]/4,cmap[0][2]/4))\n",
    "for c in cmap[1:]:\n",
    "    cmap_new.append(c)\n",
    "# cmap = [cmap[0],cmap[2], cmap[3], cmap[4],cmap[5]]\n",
    "sns.heatmap(transitions_df.T, cmap=cmap_new)\n",
    "axes=plt.gca()\n",
    "axes.set_xticks(np.array(range(0,53,3))+0.5)\n",
    "axes.set_xticklabels(list(range(0,53,3)))\n",
    "axes.set_yticks(np.array(range(0,53,3))+0.5)\n",
    "axes.set_yticklabels(list(range(0,53,3)))\n",
    "axes.set_ylabel(\"Initial cluster index\")\n",
    "axes.set_xlabel(\"Final cluster index\")\n",
    "\n",
    "colorbar = axes.collections[0].colorbar\n",
    "colorbar.set_ticks(np.array(range(5))*4/5+2/5)\n",
    "colorbar.set_ticklabels([0,1,2,3,4])\n",
    "\n",
    "# axes[1][1].set_xticks([500,1000,1500,2000])\n",
    "\n",
    "# trans_heatmap.pivot(\"Cluster index\", \"Cluster index\", \"No. transitions\")\n",
    "# plt.savefig('figures/transitions_heatmap.png', dpi=300, bbox_inches = 'tight',pad_inches = 0.01)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "transitions_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmap_new = []\n",
    "cmap_new.append((cmap[0][0]/2,cmap[0][1]/2,cmap[0][2]/2))\n",
    "for c in cmap[1:]:\n",
    "    cmap_new.append(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.triu_indices(53, k=1)[0].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rand_counts = np.zeros(1378*2)\n",
    "rand_counts[np.random.choice(np.array(range(1378*2)), size=682)]+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = np.zeros(100000)\n",
    "for i in range(100000):\n",
    "    rand_counts = np.zeros(1378*2)\n",
    "    rand_counts[np.random.choice(np.array(range(1378*2)), size=682)]+=1\n",
    "    res[i] = np.sqrt(((rand_counts[:1378] - rand_counts[1378:])**2).mean())\n",
    "    print(i)\n",
    "    clear_output(wait=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "triu_vals = np.array(transitions_df)[np.triu_indices(53, k=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tril_vals = np.array(transitions_df).T[np.triu_indices(53, k=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sqrt(((triu_vals-tril_vals)**2).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import ttest_1samp\n",
    "ttest_1samp(res, 0.58)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.mean(res), np.std(res))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(np.sqrt(((triu_vals-tril_vals)**2).mean())-np.mean(res))/np.std(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(res, bins=20)\n",
    "ax = plt.gca()\n",
    "ax.axvline(np.sqrt(((triu_vals-tril_vals)**2).mean()), c=\"magenta\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.triu_indices(53)\n",
    "np.tril_indices(53)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.random.shuffle(x)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=x*682/x.sum()\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=np.random.rand(53,53)\n",
    "x[np.diag_indices(53)]=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "((transitions_df-transitions_df.T)**2).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "\n",
    "G=nx.Graph()\n",
    "for node in range(53):\n",
    "    G.add_node(str(node+1))\n",
    "\n",
    "for trans in transitions:\n",
    "    G.add_edge(str(trans[0]), str(trans[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nx.draw(G)\n",
    "# plt.savefig('figures/transitions_graph.png', dpi=300, bbox_inches = 'tight',pad_inches = 0.01)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(lc_start_times, np.ones(len(lc_start_times)), s=0.1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lc_start_times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = (20, 3)\n",
    "\n",
    "# which_to_plot = np.flip(np.take(pseudo_CLR_transformed.index.values, np.argsort(embedding[:,0])[-271:]))\n",
    "which_to_plot = np.take(pseudo_CLR_transformed.index.values,clusters50_split[23])\n",
    "\n",
    "for ob_ind, ob in enumerate(which_to_plot):\n",
    "#     if np.sort(ob)[-1] > 0.9:\n",
    "#     if np.argsort(ob)[-1] == 22:\n",
    "        lc_index = np.where(np.array(ids) == ob)[0][0]\n",
    "        fig, ax = plt.subplots(nrows=1, ncols=2)\n",
    "        ax[0].plot(lcs[lc_index][0], lcs[lc_index][1])\n",
    "        ax[1].plot(lcs[lc_index][0]-lcs[lc_index][0][0], lcs[lc_index][1])\n",
    "        ax[1].set_ylim((0,10000))\n",
    "        ax[1].set_xlim((0,3000))\n",
    "\n",
    "        plt.show()\n",
    "#         print(list(ObID_GaussComps_dict_122_chrono.keys())[ob_ind], np.argsort(ob)[-5:], np.sort(ob)[-5:])\n",
    "#     print(np.argsort(ob), np.sort(ob)[-3:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=0.95)\n",
    "pca.fit(pseudo_CLR_transformed)\n",
    "\n",
    "# print(pca.explained_variance_ratio_.shape)\n",
    "\n",
    "reducer = umap.UMAP(random_state=1, n_neighbors=50, n_components=2)\n",
    "reducer.fit(pca.transform(pseudo_CLR_transformed))\n",
    "embedding = reducer.transform(pca.transform(pseudo_CLR_transformed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = [5,5]\n",
    "\n",
    "print(\"UMAP of CLR transformed data, PCA (65PCs, 0.95 variance), Euclidean distance\")\n",
    "plt.scatter(embedding[:,0], embedding[:,1], c=\"black\", s=s)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# row_ratio = obs_component_counts_df_122.div(obs_component_counts_df_122.sum(axis=1).values, axis=\"rows\")\n",
    "data = embedding#pca.transform(pseudo_CLR_transformed)\n",
    "metric = \"euclidean\"\n",
    "\n",
    "linkage = sch.linkage(data, method=\"complete\", metric=metric)\n",
    "plt.rcParams['figure.figsize'] = (6.97, 3)\n",
    "plt.rcParams.update({'font.size': 6})\n",
    "\n",
    "dendrogram = sch.dendrogram(linkage, truncate_mode=None, p=1, leaf_font_size = 0)\n",
    "axis = plt.gca()\n",
    "# axis.tick_params(axis=\"x\", which=\"major\", length=0, width=0, labelsize=0, direction=\"in\")\n",
    "axis.set_xticks([])\n",
    "axis.set_xticklabels([])\n",
    "axis.set_ylabel(\"euclidean distance\", size=6, x=1)Connectivity matrices\n",
    "\n",
    "# plt.savefig('figures/agglomerative_dendogram.png', dpi=300, bbox_inches = 'tight',pad_inches = 0)\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.metrics import davies_bouldin_score\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (3.32, 3.32*(1/1))\n",
    "\n",
    "\n",
    "y_top_limit = linkage[-1][2]\n",
    "y_bottom_limit = linkage[0][2]\n",
    "\n",
    "no_clusters = []\n",
    "silhouette_scores = []\n",
    "for distance in np.linspace(y_bottom_limit,y_top_limit,1000)[:-1]:\n",
    "    clusters = sch.fcluster(linkage, distance, criterion='distance')\n",
    "    silhouette_scores.append(silhouette_score(data, clusters, metric=metric, random_state=0))\n",
    "    no_clusters.append(len(np.unique(clusters)))\n",
    "    \n",
    "    \n",
    "fig, ax1 = plt.subplots()\n",
    "\n",
    "\n",
    "plt.rcParams['figure.figsize'] = [5,5]\n",
    "# plt.axhline(114, c=\"cyan\")\n",
    "# ax1.axvline(0.98298298, c=\"black\")\n",
    "# ax1.axhline(60, c=\"black\")\n",
    "ax1.plot(np.linspace(y_bottom_limit,y_top_limit,1000)[:-1], no_clusters, c=\"magenta\")\n",
    "\n",
    "ax2 = ax1.twinx()\n",
    "ax2.plot(np.linspace(y_bottom_limit,y_top_limit,1000)[:-1], silhouette_scores, c=\"cyan\")\n",
    "# plt.title(\"No. of clusters as the function of distance for hierarchical clustering\")\n",
    "# ax3 = ax1.twinx()\n",
    "# ax3.plot(np.linspace(0,y_limit,1000)[:-1], calinski_harabasz_scores, c=\"orange\")\n",
    "# ax4 = ax1.twinx()\n",
    "# ax4.plot(np.linspace(0,y_limit,1000)[:-2], np.diff(silhouette_scores, 1), c=\"green\")\n",
    "\n",
    "ax1.set_ylabel(\"No. clusters\", c=\"magenta\")\n",
    "ax2.set_ylabel(\"Silhouette score\", c=\"cyan\")\n",
    "ax1.set_xlabel(\"Distance threshold\")\n",
    "# plt.ylim((0,50))\n",
    "# plt.savefig('figures/agglomerative_silhouette.png', dpi=300, bbox_inches = 'tight',pad_inches = 0)\n",
    "print(\"Best 20 cluster numbers: \", np.array(no_clusters)[np.argsort(silhouette_scores)[-20:]])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skip_first=1\n",
    "skip_last=400\n",
    "\n",
    "np.array(np.linspace(y_bottom_limit,y_top_limit,1000)[skip_first:-1-skip_last])[np.argsort(silhouette_scores[skip_first:-skip_last])[-20:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique( sch.fcluster(linkage, 2.31247118, criterion='distance'), return_counts=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters13 = sch.fcluster(linkage, 2.31247118, criterion='distance')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_this = clusters13\n",
    "clusters13_split = np.split(np.argsort(split_this), indices_or_sections=np.cumsum(np.unique(split_this, return_counts=True)[1][:-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = [5,5]\n",
    "\n",
    "print(\"UMAP of CLR transformed data, PCA (65PCs, 0.95 variance), Euclidean distance\")\n",
    "plt.scatter(embedding[:,0], embedding[:,1], c=\"black\", s=s)\n",
    "for cluster in clusters40_split[n]:\n",
    "    plt.scatter(embedding[:,0][cluster], embedding[:,1][cluster], c=\"cyan\", s=s)\n",
    "plt.show()\n",
    "print(n)\n",
    "n+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "clusters9_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"UMAP of CLR transformed data, PCA (65PCs, 0.95 variance), Euclidean distance\")\n",
    "plt.scatter(embedding[:,0], embedding[:,1], c=\"black\", s=s)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import Isomap\n",
    "from sklearn.manifold import LocallyLinearEmbedding\n",
    "from sklearn.manifold import SpectralEmbedding\n",
    "from sklearn.manifold import MDS\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "isomap_mapper = TSNE(n_components=2)#, eigen_solver=\"dense\")\n",
    "X_isomap_transformed = isomap_mapper.fit_transform(pseudo_CLR_transformed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = (5, 5)\n",
    "\n",
    "plt.scatter(X_isomap_transformed[:,0], X_isomap_transformed[:,1], c=\"black\", s=s)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_lim = 7.5\n",
    "len(np.take(pseudo_CLR_transformed.index.values, np.where(embedding[:,0]>x_lim)[0]))\n",
    "np.flip(np.argsort(embedding[:,0])[-271:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('{}/1776_light_curves_1s_bin_errorfix.pkl'.format(data_dir), 'rb') as f:\n",
    "    lcs = pickle.load(f)\n",
    "with open('{}/1776_light_curves_1s_bin_ids_errorfix.pkl'.format(data_dir), 'rb') as f:\n",
    "    ids = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters13_split[3].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = (20, 3)\n",
    "\n",
    "# which_to_plot = np.flip(np.take(pseudo_CLR_transformed.index.values, np.argsort(embedding[:,0])[-271:]))\n",
    "which_to_plot = np.take(pseudo_CLR_transformed.index.values,clusters13_split[1])\n",
    "\n",
    "for ob_ind, ob in enumerate(which_to_plot):\n",
    "#     if np.sort(ob)[-1] > 0.9:\n",
    "#     if np.argsort(ob)[-1] == 22:\n",
    "        lc_index = np.where(np.array(ids) == ob)[0][0]\n",
    "        fig, ax = plt.subplots(nrows=1, ncols=2)\n",
    "        ax[0].plot(lcs[lc_index][0], lcs[lc_index][1])\n",
    "        ax[1].plot(lcs[lc_index][0]-lcs[lc_index][0][0], lcs[lc_index][1])\n",
    "        ax[1].set_ylim((0,10000))\n",
    "        ax[1].set_xlim((0,3000))\n",
    "\n",
    "        plt.show()\n",
    "#         print(list(ObID_GaussComps_dict_122_chrono.keys())[ob_ind], np.argsort(ob)[-5:], np.sort(ob)[-5:])\n",
    "#     print(np.argsort(ob), np.sort(ob)[-3:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lc_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=0.95)\n",
    "pca.fit(pseudo_CLR_transformed)\n",
    "\n",
    "print(pca.explained_variance_ratio_.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.max(normalized_obs_component_counts_df_122.iloc[:,120].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zscore(normalized_obs_component_counts_df_122, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reducer = umap.UMAP(random_state=42)\n",
    "reducer.fit(zscore(normalized_obs_component_counts_df_122, axis=0))\n",
    "\n",
    "\n",
    "embedding = reducer.transform(zscore(normalized_obs_component_counts_df_122, axis=0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"normalised row, zscored column\")\n",
    "plt.scatter(embedding[:,0], embedding[:,1], c=\"black\", s=s)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reducer = umap.UMAP(random_state=42)\n",
    "reducer.fit(normalized_obs_component_counts_df_122)\n",
    "\n",
    "\n",
    "embedding = reducer.transform(normalized_obs_component_counts_df_122)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"normalised rows\")\n",
    "plt.scatter(embedding[:,0], embedding[:,1], c=\"black\", s=s)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "row_ratio = obs_component_counts_df_122.div(obs_component_counts_df_122.sum(axis=1).values, axis=\"rows\")\n",
    "reducer = umap.UMAP(random_state=42)\n",
    "reducer.fit(row_ratio)\n",
    "\n",
    "\n",
    "embedding = reducer.transform(row_ratio)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"div row by sum\")\n",
    "plt.scatter(embedding[:,0], embedding[:,1], c=\"black\", s=s)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reducer = umap.UMAP(random_state=42)\n",
    "reducer.fit(obs_component_counts_df_122)\n",
    "\n",
    "\n",
    "embedding = reducer.transform(obs_component_counts_df_122)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"raw\")\n",
    "plt.scatter(embedding[:,0], embedding[:,1], c=\"black\", s=s)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "row_ratio = obs_component_counts_df_122.div(obs_component_counts_df_122.sum(axis=1).values, axis=\"rows\")\n",
    "reducer = umap.UMAP(random_state=42, metric=\"manhattan\")\n",
    "reducer.fit(row_ratio)\n",
    "\n",
    "\n",
    "embedding = reducer.transform(row_ratio)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"div row by sum, manhattan\")\n",
    "plt.scatter(embedding[:,0], embedding[:,1], c=\"black\", s=s)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "row_ratio = obs_component_counts_df_122.div(obs_component_counts_df_122.sum(axis=1).values, axis=\"rows\")\n",
    "reducer = umap.UMAP(random_state=42, metric=\"cosine\")\n",
    "reducer.fit(row_ratio)\n",
    "\n",
    "\n",
    "embedding = reducer.transform(row_ratio)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"div row by sum, cosine\")\n",
    "plt.scatter(embedding[:,0], embedding[:,1], c=\"black\", s=s)\n",
    "plt.show()\n",
    "chebyshev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "row_ratio = obs_component_counts_df_122.div(obs_component_counts_df_122.sum(axis=1).values, axis=\"rows\")\n",
    "reducer = umap.UMAP(random_state=42, metric=\"chebyshev\")\n",
    "reducer.fit(row_ratio)\n",
    "\n",
    "\n",
    "embedding = reducer.transform(row_ratio)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"div row by sum, chebyshev\")\n",
    "plt.scatter(embedding[:,0], embedding[:,1], c=\"black\", s=s)\n",
    "plt.show()\n",
    "chebyshev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "row_ratio = obs_component_counts_df_122.div(obs_component_counts_df_122.sum(axis=1).values, axis=\"rows\")\n",
    "reducer = umap.UMAP(random_state=42, metric=\"braycurtis\")\n",
    "reducer.fit(row_ratio)\n",
    "\n",
    "\n",
    "embedding = reducer.transform(row_ratio)\n",
    "#. Furthermore, principal-component analysis (PCA) has common assumptions of normally distributed and linearly related variables, often violated by biological data (47). As a result, classical distance metrics that take into account only the presence/absence of taxa, such as the Jaccard index, or metrics that explicitly account for relative abundances, such as Bray-Curtis symmetrized distance, are commonly used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = (3.32, 3.32)\n",
    "print(\"div row by sum, braycurtis\")\n",
    "plt.scatter(embedding[:,0], embedding[:,1], c=\"black\", s=s)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "row_ratio = obs_component_counts_df_122.div(obs_component_counts_df_122.sum(axis=1).values, axis=\"rows\")\n",
    "reducer = umap.UMAP(random_state=42, metric=\"jaccard\")\n",
    "reducer.fit(row_ratio)\n",
    "\n",
    "\n",
    "embedding = reducer.transform(row_ratio)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = (3.32, 3.32)\n",
    "print(\"div row by sum, jaccard\")\n",
    "plt.scatter(embedding[:,0], embedding[:,1], c=\"black\", s=s)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reducer = umap.UMAP(random_state=42, metric=\"euclidean\")\n",
    "reducer.fit(non_zero_row_ratio)\n",
    "\n",
    "\n",
    "embedding = reducer.transform(non_zero_row_ratio)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"x*1000+1 div row by sum, CLR, aitchison/ euclidean\")\n",
    "plt.scatter(embedding[:,0], embedding[:,1], c=\"black\", s=s)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "row_ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "row_ratio = obs_component_counts_df_122.div(obs_component_counts_df_122.sum(axis=1).values, axis=\"rows\")\n",
    "\n",
    "reducer = umap.UMAP(random_state=42, metric=\"ll_dirichlet\")\n",
    "reducer.fit(non_zero_row_ratio)\n",
    "\n",
    "\n",
    "embedding = reducer.transform(non_zero_row_ratio)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"div row by sum, mahalanobis\")\n",
    "plt.scatter(embedding[:,0], embedding[:,1], c=\"black\", s=s)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perform agglomerative clustering of observations in Gaussian cluster space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import cluster\n",
    "agglo = cluster.FeatureAgglomeration()\n",
    "agglo.fit(normalized_obs_component_counts_df_122)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_features = agglo.transform(normalized_obs_component_counts_df_122)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _validate_aitchison(w, u_v):\n",
    "    if np.any(w <= 0):\n",
    "        raise ValueError('Values in %s should be positive.' % u_v)\n",
    "    return(w)\n",
    "\n",
    "def _validate_vector(u, dtype=None):\n",
    "    # XXX Is order='c' really necessary?\n",
    "    u = np.asarray(u, dtype=dtype, order='c').squeeze()\n",
    "    # Ensure values such as u=1 and u=[1] still return 1-D arrays.\n",
    "    u = np.atleast_1d(u)\n",
    "    if u.ndim > 1:\n",
    "        raise ValueError(\"Input vector should be 1-D.\")\n",
    "    return u\n",
    "\n",
    "def aitchison(u, v):\n",
    "    \"\"\"\n",
    "    Computes the Aitchison distance between two 1-D arrays.\n",
    "    The Aitchison distance between 1-D arrays `u` and `v`, is defined as\n",
    "    .. math::\n",
    "       {||log \\left( \\frac{u}{v} \\right) - mean \\left(log \\left( \\frac{u}{v} \\right) \\right) ||}_2\n",
    "       \n",
    "    Parameters\n",
    "    ----------\n",
    "    u : (N,) array_like\n",
    "        Input array.\n",
    "    v : (N,) array_like\n",
    "        Input array.\n",
    "    Returns\n",
    "    -------\n",
    "    aitchison : double\n",
    "        The Aitchison distance between vectors `u` and `v`.\n",
    "        \n",
    "    Since Aitchison distance is just the Euclidean distance of the clr (center-log ratio) transformed data points, do you need a separate C-level function? Doing the transformations on a per-vector basis will be slower than precomputing them. Unless memory is a concern? This is somewhat similar to how cosine cdist is currently implemented.\n",
    "    \"\"\"\n",
    "    \n",
    "    u = _validate_vector(u)\n",
    "    v = _validate_vector(v)\n",
    "    _validate_aitchison(u, 'u')\n",
    "    _validate_aitchison(v, 'v')\n",
    "    log_u_v = np.log(u / v)\n",
    "    dist = np.linalg.norm(log_u_v - np.mean(log_u_v), ord=2)\n",
    "    return(dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "row_ratio.min()<0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# row_ratio = obs_component_counts_df_122.div(obs_component_counts_df_122.sum(axis=1).values, axis=\"rows\")\n",
    "\n",
    "linkage = sch.linkage(obs_component_counts_df_122, method=\"complete\", metric=\"cosine\")\n",
    "plt.rcParams['figure.figsize'] = (6.97, 3)\n",
    "plt.rcParams.update({'font.size': 6})\n",
    "\n",
    "dendrogram = sch.dendrogram(linkage, truncate_mode=None, p=1, leaf_font_size = 0, color_threshold=0.98298298)\n",
    "axis = plt.gca()\n",
    "# axis.tick_params(axis=\"x\", which=\"major\", length=0, width=0, labelsize=0, direction=\"in\")\n",
    "axis.set_xticks([])\n",
    "axis.set_xticklabels([])\n",
    "axis.set_ylabel(\"Cosine distance\", size=6, x=1)\n",
    "\n",
    "plt.savefig('figures/agglomerative_dendogram.png', dpi=300, bbox_inches = 'tight',pad_inches = 0)\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(no_clusters[-200:])[np.argsort(calinski_harabasz_scores[-200:])]#[-100:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(no_clusters)[np.argsort(silhouette_scores)][-100:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.vstack((np.array(silhouette_scores),np.array(no_clusters))).T[-50:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.metrics import davies_bouldin_score\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (3.32, 3.32*(1/1))\n",
    "\n",
    "\n",
    "# y_limit = linkage[-1][2]\n",
    "# no_clusters = []\n",
    "# silhouette_scores = []\n",
    "# # calinski_harabasz_scores = []\n",
    "# for distance in np.linspace(0,y_limit,1000)[:-1]:\n",
    "#     clusters = sch.fcluster(linkage, distance, criterion='distance')\n",
    "#     silhouette_scores.append(silhouette_score(obs_component_counts_df_122, clusters, metric=\"cosine\", random_state=0))\n",
    "#     no_clusters.append(len(np.unique(clusters)))\n",
    "# #     calinski_harabasz_scores.append(davies_bouldin_score(row_ratio, clusters))\n",
    "    \n",
    "    \n",
    "fig, ax1 = plt.subplots()\n",
    "\n",
    "\n",
    "plt.rcParams['figure.figsize'] = [5,5]\n",
    "# plt.axhline(114, c=\"cyan\")\n",
    "ax1.axvline(0.98298298, c=\"black\")\n",
    "ax1.axhline(60, c=\"black\")\n",
    "ax1.plot(np.linspace(0,y_limit,1000)[:-1], no_clusters, c=\"magenta\")\n",
    "\n",
    "ax2 = ax1.twinx()\n",
    "ax2.plot(np.linspace(0,y_limit,1000)[:-1], silhouette_scores, c=\"cyan\")\n",
    "# plt.title(\"No. of clusters as the function of distance for hierarchical clustering\")\n",
    "# ax3 = ax1.twinx()\n",
    "# ax3.plot(np.linspace(0,y_limit,1000)[:-1], calinski_harabasz_scores, c=\"orange\")\n",
    "# ax4 = ax1.twinx()\n",
    "# ax4.plot(np.linspace(0,y_limit,1000)[:-2], np.diff(silhouette_scores, 1), c=\"green\")\n",
    "\n",
    "ax1.set_ylabel(\"No. clusters\", c=\"magenta\")\n",
    "ax2.set_ylabel(\"Silhouette score\", c=\"cyan\")\n",
    "# ax3.set_ylabel(\"davies_bouldin_score\")\n",
    "ax1.set_xlabel(\"Distance threshold\")\n",
    "# plt.ylim((0,50))\n",
    "plt.savefig('figures/agglomerative_silhouette.png', dpi=300, bbox_inches = 'tight',pad_inches = 0)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# row_ratio = obs_component_counts_df_122.div(obs_component_counts_df_122.sum(axis=1).values, axis=\"rows\")\n",
    "\n",
    "linkage = sch.linkage(obs_component_counts_df_122, method=\"complete\", metric=\"braycurtis\")\n",
    "plt.rcParams['figure.figsize'] = (6.97, 3)\n",
    "plt.rcParams.update({'font.size': 6})\n",
    "\n",
    "dendrogram = sch.dendrogram(linkage, truncate_mode=None, p=1, leaf_font_size = 0, color_threshold=0.98298298)\n",
    "axis = plt.gca()\n",
    "# axis.tick_params(axis=\"x\", which=\"major\", length=0, width=0, labelsize=0, direction=\"in\")\n",
    "axis.set_xticks([])\n",
    "axis.set_xticklabels([])\n",
    "axis.set_ylabel(\"Cosine distance\", size=6, x=1)\n",
    "\n",
    "# plt.savefig('figures/agglomerative_dendogram.png', dpi=300, bbox_inches = 'tight',pad_inches = 0)\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(clusters).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.metrics import davies_bouldin_score\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (3.32, 3.32*(1/1))\n",
    "\n",
    "\n",
    "y_limit = linkage[-1][2]\n",
    "no_clusters = []\n",
    "silhouette_scores = []\n",
    "for distance in np.linspace(0.01,y_limit,1000)[:-1]:\n",
    "    clusters = sch.fcluster(linkage, distance, criterion='distance')\n",
    "    silhouette_scores.append(silhouette_score(obs_component_counts_df_122, clusters, metric=\"braycurtis\", random_state=0))\n",
    "    no_clusters.append(len(np.unique(clusters)))\n",
    "    \n",
    "    \n",
    "fig, ax1 = plt.subplots()\n",
    "\n",
    "\n",
    "plt.rcParams['figure.figsize'] = [5,5]\n",
    "# plt.axhline(114, c=\"cyan\")\n",
    "ax1.axvline(0.98298298, c=\"black\")\n",
    "ax1.axhline(60, c=\"black\")\n",
    "ax1.plot(np.linspace(0.01,y_limit,1000)[:-1], no_clusters, c=\"magenta\")\n",
    "\n",
    "ax2 = ax1.twinx()\n",
    "ax2.plot(np.linspace(0.01,y_limit,1000)[:-1], silhouette_scores, c=\"cyan\")\n",
    "# plt.title(\"No. of clusters as the function of distance for hierarchical clustering\")\n",
    "# ax3 = ax1.twinx()\n",
    "# ax3.plot(np.linspace(0,y_limit,1000)[:-1], calinski_harabasz_scores, c=\"orange\")\n",
    "# ax4 = ax1.twinx()\n",
    "# ax4.plot(np.linspace(0,y_limit,1000)[:-2], np.diff(silhouette_scores, 1), c=\"green\")\n",
    "\n",
    "ax1.set_ylabel(\"No. clusters\", c=\"magenta\")\n",
    "ax2.set_ylabel(\"Silhouette score\", c=\"cyan\")\n",
    "ax1.set_xlabel(\"Distance threshold\")\n",
    "# plt.ylim((0,50))\n",
    "# plt.savefig('figures/agglomerative_silhouette.png', dpi=300, bbox_inches = 'tight',pad_inches = 0)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats.mstats import gmean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.corrcoef(data.iloc[:,x],data.iloc[:,y])[0,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.close()\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (30,30)\n",
    "plt.rcParams.update({'font.size': 10})\n",
    "\n",
    "fig, axes = plt.subplots(nrows=5, ncols=5)\n",
    "\n",
    "tril = np.array(np.tril_indices(5)).T\n",
    "\n",
    "data = obs_component_counts_df_122.div(np.sum(obs_component_counts_df_122, axis=1), axis=\"rows\")\n",
    "\n",
    "for x,y in tril:\n",
    "    axes[x,y].scatter(data.iloc[:,x],data.iloc[:,y])\n",
    "    axes[x,y].set_ylim((-0.1,1))\n",
    "    axes[x,y].set_xlim((-0.1,1))\n",
    "    axes[x,y].text(0.99, 0.99, np.corrcoef(data.iloc[:,x],data.iloc[:,y])[0,1], ha='right', va='top', transform=axes[x,y].transAxes, size=10)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.close()\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (30,30)\n",
    "plt.rcParams.update({'font.size': 10})\n",
    "\n",
    "fig, axes = plt.subplots(nrows=10, ncols=10)\n",
    "\n",
    "tril = np.array(np.tril_indices(10)).T\n",
    "\n",
    "data = pseudo_CLR_transformed\n",
    "\n",
    "for x,y in tril:\n",
    "    axes[x,y].scatter(data.iloc[:,x],data.iloc[:,y])\n",
    "#     axes[x,y].set_ylim((-0.1,1))\n",
    "#     axes[x,y].set_xlim((-0.1,1))\n",
    "    axes[x,y].text(0.99, 0.99, np.corrcoef(data.iloc[:,x],data.iloc[:,y])[0,1], ha='right', va='top', transform=axes[x,y].transAxes, size=10)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pseudo_counts = (obs_component_counts_df_122*2).replace(to_replace=0, value=1)\n",
    "closed_pseudo_counts = pseudo_counts.div(np.sum(pseudo_counts, axis=1), axis=\"rows\")\n",
    "\n",
    "scaled_pseudo_counts = closed_pseudo_counts.div(gmean(closed_pseudo_counts, axis=1), axis=\"rows\")\n",
    "pseudo_CLR_transformed = np.log(scaled_pseudo_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_pseudo_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.close()\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (30,30)\n",
    "plt.rcParams.update({'font.size': 10})\n",
    "\n",
    "fig, axes = plt.subplots(nrows=10, ncols=10)\n",
    "\n",
    "tril = np.array(np.tril_indices(10)).T\n",
    "\n",
    "data = pseudo_CLR_transformed\n",
    "\n",
    "for x,y in tril:\n",
    "    axes[x,y].scatter(data.iloc[:,x],data.iloc[:,y])\n",
    "#     axes[x,y].set_ylim((-0.1,1))\n",
    "#     axes[x,y].set_xlim((-0.1,1))\n",
    "    axes[x,y].text(0.99, 0.99, np.corrcoef(data.iloc[:,x],data.iloc[:,y])[0,1], ha='right', va='top', transform=axes[x,y].transAxes, size=10)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pseudo_counts = (obs_component_counts_df_122*1e1).replace(to_replace=0, value=1)\n",
    "closed_pseudo_counts = pseudo_counts.div(np.sum(pseudo_counts, axis=1), axis=\"rows\")\n",
    "\n",
    "scaled_pseudo_counts = closed_pseudo_counts.div(gmean(closed_pseudo_counts, axis=1), axis=\"rows\")\n",
    "pseudo_CLR_transformed = np.log(scaled_pseudo_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pseudo_CLR_transformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_component_counts_df_122.div(np.mean(obs_component_counts_df_122, axis=1).values, axis=\"rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(obs_component_counts_df_122, axis=1).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pseudo_CLR_transformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# row_ratio = obs_component_counts_df_122.div(obs_component_counts_df_122.sum(axis=1).values, axis=\"rows\")\n",
    "\n",
    "linkage = sch.linkage(pseudo_CLR_transformed, method=\"complete\", metric=\"euclidean\")\n",
    "plt.rcParams['figure.figsize'] = (6.97, 3)\n",
    "plt.rcParams.update({'font.size': 6})\n",
    "\n",
    "dendrogram = sch.dendrogram(linkage, truncate_mode=None, p=1, leaf_font_size = 0)\n",
    "axis = plt.gca()\n",
    "# axis.tick_params(axis=\"x\", which=\"major\", length=0, width=0, labelsize=0, direction=\"in\")\n",
    "axis.set_xticks([])\n",
    "axis.set_xticklabels([])\n",
    "axis.set_ylabel(\"Cosine distance\", size=6, x=1)\n",
    "\n",
    "# plt.savefig('figures/agglomerative_dendogram.png', dpi=300, bbox_inches = 'tight',pad_inches = 0)\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.metrics import davies_bouldin_score\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (3.32, 3.32*(1/1))\n",
    "\n",
    "data = pseudo_CLR_transformed\n",
    "metric = \"euclidean\"\n",
    "\n",
    "y_top_limit = linkage[-1][2]\n",
    "y_bottom_limit = linkage[0][2]\n",
    "no_clusters = []\n",
    "silhouette_scores = []\n",
    "for distance in np.linspace(y_bottom_limit,y_top_limit,1000)[:-1]:\n",
    "    clusters = sch.fcluster(linkage, distance, criterion='distance')\n",
    "    silhouette_scores.append(silhouette_score(data, clusters, metric=metric, random_state=0))\n",
    "    no_clusters.append(len(np.unique(clusters)))\n",
    "    \n",
    "    \n",
    "fig, ax1 = plt.subplots()\n",
    "\n",
    "\n",
    "plt.rcParams['figure.figsize'] = [5,5]\n",
    "# plt.axhline(114, c=\"cyan\")\n",
    "# ax1.axvline(0.98298298, c=\"black\")\n",
    "# ax1.axhline(60, c=\"black\")\n",
    "ax1.plot(np.linspace(y_bottom_limit,y_top_limit,1000)[:-1], no_clusters, c=\"magenta\")\n",
    "\n",
    "ax2 = ax1.twinx()\n",
    "ax2.plot(np.linspace(y_bottom_limit, y_top_limit,1000)[:-1], silhouette_scores, c=\"cyan\")\n",
    "# plt.title(\"No. of clusters as the function of distance for hierarchical clustering\")\n",
    "# ax3 = ax1.twinx()\n",
    "# ax3.plot(np.linspace(0,y_limit,1000)[:-1], calinski_harabasz_scores, c=\"orange\")\n",
    "# ax4 = ax1.twinx()\n",
    "# ax4.plot(np.linspace(0,y_limit,1000)[:-2], np.diff(silhouette_scores, 1), c=\"green\")\n",
    "\n",
    "ax1.set_ylabel(\"No. clusters\", c=\"magenta\")\n",
    "ax2.set_ylabel(\"Silhouette score\", c=\"cyan\")\n",
    "ax1.set_xlabel(\"Distance threshold\")\n",
    "# plt.ylim((0,50))\n",
    "# plt.savefig('figures/agglomerative_silhouette.png', dpi=300, bbox_inches = 'tight',pad_inches = 0)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "print(\"Best 20 cluster numbers: \", np.array(no_clusters)[np.argsort(silhouette_scores)[-20:]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "2**122"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# row_ratio = obs_component_counts_df_122.div(obs_component_counts_df_122.sum(axis=1).values, axis=\"rows\")\n",
    "\n",
    "linkage = sch.linkage(pseudo_CLR_transformed, method=\"complete\", metric=\"cosine\")\n",
    "plt.rcParams['figure.figsize'] = (6.97, 3)\n",
    "plt.rcParams.update({'font.size': 6})\n",
    "\n",
    "dendrogram = sch.dendrogram(linkage, truncate_mode=None, p=1, leaf_font_size = 0)\n",
    "axis = plt.gca()\n",
    "# axis.tick_params(axis=\"x\", which=\"major\", length=0, width=0, labelsize=0, direction=\"in\")\n",
    "axis.set_xticks([])\n",
    "axis.set_xticklabels([])\n",
    "axis.set_ylabel(\"Cosine distance\", size=6, x=1)\n",
    "\n",
    "# plt.savefig('figures/agglomerative_dendogram.png', dpi=300, bbox_inches = 'tight',pad_inches = 0)\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.metrics import davies_bouldin_score\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (3.32, 3.32*(1/1))\n",
    "\n",
    "data = pseudo_CLR_transformed\n",
    "metric = \"cosine\"\n",
    "\n",
    "y_top_limit = linkage[-1][2]\n",
    "y_bottom_limit = linkage[0][2]\n",
    "no_clusters = []\n",
    "silhouette_scores = []\n",
    "for distance in np.linspace(y_bottom_limit,y_top_limit,1000)[:-1]:\n",
    "    clusters = sch.fcluster(linkage, distance, criterion='distance')\n",
    "    silhouette_scores.append(silhouette_score(data, clusters, metric=metric, random_state=0))\n",
    "    no_clusters.append(len(np.unique(clusters)))\n",
    "    \n",
    "    \n",
    "fig, ax1 = plt.subplots()\n",
    "\n",
    "\n",
    "plt.rcParams['figure.figsize'] = [5,5]\n",
    "# plt.axhline(114, c=\"cyan\")\n",
    "# ax1.axvline(0.98298298, c=\"black\")\n",
    "# ax1.axhline(60, c=\"black\")\n",
    "ax1.plot(np.linspace(y_bottom_limit,y_top_limit,1000)[:-1], no_clusters, c=\"magenta\")\n",
    "\n",
    "ax2 = ax1.twinx()\n",
    "ax2.plot(np.linspace(y_bottom_limit, y_top_limit,1000)[:-1], silhouette_scores, c=\"cyan\")\n",
    "# plt.title(\"No. of clusters as the function of distance for hierarchical clustering\")\n",
    "# ax3 = ax1.twinx()\n",
    "# ax3.plot(np.linspace(0,y_limit,1000)[:-1], calinski_harabasz_scores, c=\"orange\")\n",
    "# ax4 = ax1.twinx()\n",
    "# ax4.plot(np.linspace(0,y_limit,1000)[:-2], np.diff(silhouette_scores, 1), c=\"green\")\n",
    "\n",
    "ax1.set_ylabel(\"No. clusters\", c=\"magenta\")\n",
    "ax2.set_ylabel(\"Silhouette score\", c=\"cyan\")\n",
    "ax1.set_xlabel(\"Distance threshold\")\n",
    "# ax1.set_ylim((0,200))\n",
    "# plt.savefig('figures/agglomerative_silhouette.png', dpi=300, bbox_inches = 'tight',pad_inches = 0)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "print(\"Best 20 cluster numbers: \", np.array(no_clusters)[np.argsort(silhouette_scores)[-20:]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pseudo_counts = (obs_component_counts_df_122*1000).replace(to_replace=0, value=1)\n",
    "scaled_pseudo_counts = pseudo_counts.div(gmean(pseudo_counts, axis=1), axis=\"rows\")\n",
    "pseudo_CLR_transformed_1000 = np.log(scaled_pseudo_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_component_counts_df_122.div(np.mean(obs_component_counts_df_122, axis=1).values, axis=\"rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pseudo_CLR_transformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# row_ratio = obs_component_counts_df_122.div(obs_component_counts_df_122.sum(axis=1).values, axis=\"rows\")\n",
    "\n",
    "linkage = sch.linkage(pseudo_CLR_transformed_1000, method=\"complete\", metric=\"euclidean\")\n",
    "plt.rcParams['figure.figsize'] = (6.97, 3)\n",
    "plt.rcParams.update({'font.size': 6})\n",
    "\n",
    "dendrogram = sch.dendrogram(linkage, truncate_mode=None, p=1, leaf_font_size = 0)\n",
    "axis = plt.gca()\n",
    "# axis.tick_params(axis=\"x\", which=\"major\", length=0, width=0, labelsize=0, direction=\"in\")\n",
    "axis.set_xticks([])\n",
    "axis.set_xticklabels([])\n",
    "axis.set_ylabel(\"Cosine distance\", size=6, x=1)\n",
    "\n",
    "# plt.savefig('figures/agglomerative_dendogram.png', dpi=300, bbox_inches = 'tight',pad_inches = 0)\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.metrics import davies_bouldin_score\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (3.32, 3.32*(1/1))\n",
    "\n",
    "data = pseudo_CLR_transformed_1000\n",
    "metric = \"euclidean\"\n",
    "\n",
    "y_top_limit = linkage[-1][2]\n",
    "y_bottom_limit = linkage[0][2]\n",
    "no_clusters = []\n",
    "silhouette_scores = []\n",
    "for distance in np.linspace(y_bottom_limit,y_top_limit,1000)[:-1]:\n",
    "    clusters = sch.fcluster(linkage, distance, criterion='distance')\n",
    "    silhouette_scores.append(silhouette_score(data, clusters, metric=metric, random_state=0))\n",
    "    no_clusters.append(len(np.unique(clusters)))\n",
    "    \n",
    "    \n",
    "fig, ax1 = plt.subplots()\n",
    "\n",
    "\n",
    "plt.rcParams['figure.figsize'] = [5,5]\n",
    "# plt.axhline(114, c=\"cyan\")\n",
    "# ax1.axvline(0.98298298, c=\"black\")\n",
    "# ax1.axhline(60, c=\"black\")\n",
    "ax1.plot(np.linspace(y_bottom_limit,y_top_limit,1000)[:-1], no_clusters, c=\"magenta\")\n",
    "\n",
    "ax2 = ax1.twinx()\n",
    "ax2.plot(np.linspace(y_bottom_limit, y_top_limit,1000)[:-1], silhouette_scores, c=\"cyan\")\n",
    "# plt.title(\"No. of clusters as the function of distance for hierarchical clustering\")\n",
    "# ax3 = ax1.twinx()\n",
    "# ax3.plot(np.linspace(0,y_limit,1000)[:-1], calinski_harabasz_scores, c=\"orange\")\n",
    "# ax4 = ax1.twinx()\n",
    "# ax4.plot(np.linspace(0,y_limit,1000)[:-2], np.diff(silhouette_scores, 1), c=\"green\")\n",
    "\n",
    "ax1.set_ylabel(\"No. clusters\", c=\"magenta\")\n",
    "ax2.set_ylabel(\"Silhouette score\", c=\"cyan\")\n",
    "ax1.set_xlabel(\"Distance threshold\")\n",
    "# plt.ylim((0,50))\n",
    "# plt.savefig('figures/agglomerative_silhouette.png', dpi=300, bbox_inches = 'tight',pad_inches = 0)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "print(\"Best 20 cluster numbers: \", np.array(no_clusters)[np.argsort(silhouette_scores)[-20:]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# row_ratio = obs_component_counts_df_122.div(obs_component_counts_df_122.sum(axis=1).values, axis=\"rows\")\n",
    "\n",
    "linkage = sch.linkage(pseudo_CLR_transformed_1000, method=\"complete\", metric=\"cosine\")\n",
    "plt.rcParams['figure.figsize'] = (6.97, 3)\n",
    "plt.rcParams.update({'font.size': 6})\n",
    "\n",
    "dendrogram = sch.dendrogram(linkage, truncate_mode=None, p=1, leaf_font_size = 0)\n",
    "axis = plt.gca()\n",
    "# axis.tick_params(axis=\"x\", which=\"major\", length=0, width=0, labelsize=0, direction=\"in\")\n",
    "axis.set_xticks([])\n",
    "axis.set_xticklabels([])\n",
    "axis.set_ylabel(\"Cosine distance\", size=6, x=1)\n",
    "\n",
    "# plt.savefig('figures/agglomerative_dendogram.png', dpi=300, bbox_inches = 'tight',pad_inches = 0)\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.metrics import davies_bouldin_score\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (3.32, 3.32*(1/1))\n",
    "\n",
    "data = pseudo_CLR_transformed_1000\n",
    "metric = \"cosine\"\n",
    "\n",
    "y_top_limit = linkage[-1][2]\n",
    "y_bottom_limit = linkage[0][2]\n",
    "no_clusters = []\n",
    "silhouette_scores = []\n",
    "for distance in np.linspace(y_bottom_limit,y_top_limit,1000)[:-1]:\n",
    "    clusters = sch.fcluster(linkage, distance, criterion='distance')\n",
    "    silhouette_scores.append(silhouette_score(data, clusters, metric=metric, random_state=0))\n",
    "    no_clusters.append(len(np.unique(clusters)))\n",
    "    \n",
    "    \n",
    "fig, ax1 = plt.subplots()\n",
    "\n",
    "\n",
    "plt.rcParams['figure.figsize'] = [5,5]\n",
    "# plt.axhline(114, c=\"cyan\")\n",
    "# ax1.axvline(0.98298298, c=\"black\")\n",
    "# ax1.axhline(60, c=\"black\")\n",
    "ax1.plot(np.linspace(y_bottom_limit,y_top_limit,1000)[:-1], no_clusters, c=\"magenta\")\n",
    "\n",
    "ax2 = ax1.twinx()\n",
    "ax2.plot(np.linspace(y_bottom_limit, y_top_limit,1000)[:-1], silhouette_scores, c=\"cyan\")\n",
    "# plt.title(\"No. of clusters as the function of distance for hierarchical clustering\")\n",
    "# ax3 = ax1.twinx()\n",
    "# ax3.plot(np.linspace(0,y_limit,1000)[:-1], calinski_harabasz_scores, c=\"orange\")\n",
    "# ax4 = ax1.twinx()\n",
    "# ax4.plot(np.linspace(0,y_limit,1000)[:-2], np.diff(silhouette_scores, 1), c=\"green\")\n",
    "\n",
    "ax1.set_ylabel(\"No. clusters\", c=\"magenta\")\n",
    "ax2.set_ylabel(\"Silhouette score\", c=\"cyan\")\n",
    "ax1.set_xlabel(\"Distance threshold\")\n",
    "# ax1.set_ylim((0,200))\n",
    "# plt.savefig('figures/agglomerative_silhouette.png', dpi=300, bbox_inches = 'tight',pad_inches = 0)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "print(\"Best 20 cluster numbers: \", np.array(no_clusters)[np.argsort(silhouette_scores)[-20:]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reducer = umap.UMAP(random_state=42, metric=\"euclidean\")\n",
    "reducer.fit(pseudo_CLR_transformed_1000)\n",
    "\n",
    "\n",
    "embedding = reducer.transform(pseudo_CLR_transformed_1000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"CLR with pseudo counts/ euclidean\")\n",
    "plt.scatter(embedding[:,0], embedding[:,1], c=\"black\", s=s)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reducer = umap.UMAP(random_state=42, metric=\"euclidean\")\n",
    "reducer.fit(pseudo_CLR_transformed)\n",
    "\n",
    "\n",
    "embedding = reducer.transform(pseudo_CLR_transformed_1000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"CLR with pseudo counts/ euclidean\")\n",
    "plt.scatter(embedding[:,0], embedding[:,1], c=\"black\", s=s)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pseudo_counts = (obs_component_counts_df_122*1e5).replace(to_replace=0, value=1)\n",
    "scaled_pseudo_counts = pseudo_counts.div(gmean(pseudo_counts, axis=1), axis=\"rows\")\n",
    "pseudo_CLR_transformed_1e5 = np.log(scaled_pseudo_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reducer = umap.UMAP(random_state=42, metric=\"euclidean\")\n",
    "reducer.fit(pseudo_CLR_transformed)\n",
    "\n",
    "\n",
    "embedding = reducer.transform(pseudo_CLR_transformed_1e5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"CLR with pseudo counts/ euclidean\")\n",
    "plt.scatter(embedding[:,0], embedding[:,1], c=\"black\", s=s)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pseudo_CLR_transformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=0.99)\n",
    "pca.fit(pseudo_CLR_transformed)\n",
    "\n",
    "print(pca.explained_variance_ratio_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "# from scipy.sparse import random as sparse_random\n",
    "# from sklearn.random_projection import sparse_random_matrix\n",
    "# X = sparse_random(100, 100, density=0.01, format='csr',\n",
    "#                   random_state=42)\n",
    "svd = TruncatedSVD(n_components=100,random_state=42)\n",
    "svd.fit(pseudo_CLR_transformed)\n",
    "\n",
    "print(svd.explained_variance_ratio_)\n",
    "\n",
    "print(svd.explained_variance_ratio_.sum())\n",
    "\n",
    "print(svd.singular_values_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pseudo_counts = pd.DataFrame(lda.transform(obs_component_counts_df_122))\n",
    "scaled_pseudo_counts = pseudo_counts.div(gmean(pseudo_counts, axis=1), axis=\"rows\")\n",
    "pseudo_CLR_transformed_1e5 = np.log(scaled_pseudo_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reducer = umap.UMAP(random_state=42, metric=\"euclidean\")\n",
    "reducer.fit(pseudo_CLR_transformed_1e5)\n",
    "\n",
    "\n",
    "embedding = reducer.transform(pseudo_CLR_transformed_1e5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = (5,5)\n",
    "\n",
    "print(\"counds with dirchlet decomposition\")\n",
    "plt.scatter(embedding[:,0], embedding[:,1], c=\"black\", s=s)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pseudo_CLR_transformed_1e5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "lda = LatentDirichletAllocation(n_components=122,random_state=0)\n",
    "lda.fit(obs_component_counts_df_122)\n",
    "\n",
    "# get topics for some given samples:\n",
    "lda.transform(obs_component_counts_df_122)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(pca.explained_variance_ratio_)\n",
    "plt.plot(np.diff(pca.explained_variance_ratio_, n=2))\n",
    "# plt.ylim((0,0.25))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.diff(pca.explained_variance_ratio_, n=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(pca.explained_variance_ratio_[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.where((np.cumsum(pca.explained_variance_ratio_)>0.99) == 1)[0]+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import OPTICS\n",
    "\n",
    "clustering = OPTICS(min_samples=5).fit(log_lda_components_probs)\n",
    "clustering.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(clustering.labels_, return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Reachability plot\n",
    "# clust = optics_shape_moments\n",
    "\n",
    "# plt.rcParams['figure.figsize'] = (100,10)\n",
    "# plt.rcParams.update({'font.size': 42})\n",
    "\n",
    "\n",
    "\n",
    "# # labels_050 = cluster_optics_dbscan(reachability=clust.reachability_,\n",
    "# #                                    core_distances=clust.core_distances_,\n",
    "# #                                    ordering=clust.ordering_, eps=0.5)\n",
    "# # labels_200 = cluster_optics_dbscan(reachability=clust.reachability_,\n",
    "# #                                    core_distances=clust.core_distances_,\n",
    "# #                                    ordering=clust.ordering_, eps=2)\n",
    "\n",
    "space = np.arange(len(log_lda_components_probs))\n",
    "reachability = clustering.reachability_[clustering.ordering_]\n",
    "labels = clustering.labels_[clustering.ordering_]\n",
    "\n",
    "# # breaks = [0,21430,199990,242305,303065,309196,371590,383575,391430,395315, 414530, 468201]#371590\n",
    "# # for nb, brejk in enumerate(breaks[:-1]):\n",
    "# #     Xk = space[labels == 0][brejk:breaks[nb+1]]\n",
    "# #     Rk = reachability[labels == 0][brejk:breaks[nb+1]]\n",
    "# #     plt.scatter(Xk, Rk, alpha=1)\n",
    "    \n",
    "\n",
    "# plt.plot(space[labels == -1], reachability[labels == -1], 'white', alpha=0.3)\n",
    "plt.scatter(space, reachability, s=0.1)#, c=np.array(shape_moments_GM114_labels_volume_fake)[clust.ordering_], cmap=\"cool\")\n",
    "# plt.plot(space, np.full_like(space, 0.5, dtype=float), 'k-.', alpha=0.5)\n",
    "plt.ylabel('Reachability (epsilon distance)')\n",
    "# plt.set_title('Reachability Plot')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(no_clusters)[np.argsort(silhouette_scores)[-20:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.argmin(silhouette_scores)#[-10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "silhouette_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Try Latent Dirichlet Alocation on the observation x pattern matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GaussComps_str = [[u'{}'.format(val) for val in ob] for ob in ObID_GaussComps_dict_122_chrono.values()]# if len(ob)>20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "GaussComps_str[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the bigram and trigram models\n",
    "bigram = gensim.models.Phrases(GaussComps_str, min_count=100, threshold=0.25, scoring=\"npmi\")#, min_count=2, threshold=1)#, threshold=100) # higher threshold fewer phrases.\n",
    "trigram = gensim.models.Phrases(bigram[GaussComps_str])#, min_count=2, threshold=1)#, threshold=100) \n",
    "quadgrams = gensim.models.Phrases(trigram[GaussComps_str])#, min_count=2, threshold=1)\n",
    "\n",
    "bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "trigram_mod = gensim.models.phrases.Phraser(trigram)\n",
    "quadgrams_mod = gensim.models.phrases.Phraser(quadgrams)\n",
    "\n",
    "def make_bigrams(texts):\n",
    "    return [bigram[doc] for doc in texts]\n",
    "\n",
    "def make_trigrams(texts):\n",
    "    return [trigram_mod[bigram_mod[doc]] for doc in texts]\n",
    "\n",
    "def make_quadgrams(texts):\n",
    "    return [quadgrams_mod[trigram_mod[bigram_mod[doc]]] for doc in texts]\n",
    "\n",
    "# Form Bigrams\n",
    "data_words_bigrams = make_bigrams(GaussComps_str)\n",
    "data_words_trigrams = make_trigrams(GaussComps_str)\n",
    "data_words_quadgrams = make_quadgrams(GaussComps_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Dictionary\n",
    "id2word = gensim.corpora.Dictionary(GaussComps_str)\n",
    "\n",
    "# Term Document Frequency\n",
    "corpus = [id2word.doc2bow(text) for text in GaussComps_str]\n",
    "\n",
    "# View\n",
    "print(corpus[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Dictionary\n",
    "id2word_quad = gensim.corpora.Dictionary(data_words_quadgrams)\n",
    "\n",
    "# Term Document Frequency\n",
    "corpus_quad = [id2word_quad.doc2bow(text, return_missing=True) for text in data_words_quadgrams]\n",
    "\n",
    "# View\n",
    "print(corpus_quad[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# corpus_matrix = gensim.matutils.corpus2csc(corpus_quad[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "print(len(np.unique([item for sublist in data_words_quadgrams for item in sublist], return_counts=True)[0]))\n",
    "np.unique([item for sublist in data_words_quadgrams for item in sublist], return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "gensim_lda_model = gensim.models.LdaMulticore(corpus=corpus_quad,\n",
    "                                       id2word=id2word_quad,\n",
    "                                       num_topics=1000, \n",
    "                                       random_state=0,\n",
    "                                       chunksize=2000,\n",
    "                                       passes=10,\n",
    "                                       per_word_topics=True, \n",
    "                                       minimum_probability=0.01,\n",
    "                                      workers=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "X = obs_component_counts_df_122\n",
    "scoring = None#('perplexity', 'score')\n",
    "\n",
    "results = []\n",
    "\n",
    "for n_components in range(20):\n",
    "    n_components+=1\n",
    "    lda = LatentDirichletAllocation(n_components=n_components, random_state=0)\n",
    "    cv_results = cross_validate(lda, X, cv=5, scoring=scoring, return_train_score=True)\n",
    "    results.append((n_components, cv_results))\n",
    "    print(n_components)\n",
    "    clear_output(wait=True)\n",
    "\n",
    "    \n",
    "    n_components=10, *, doc_topic_prior=None, topic_word_prior=None, \n",
    "    learning_method='batch', learning_decay=0.7, learning_offset=10.0, max_iter=10, \n",
    "    batch_size=128, evaluate_every=-1, total_samples=1000000.0, perp_tol=0.1, mean_change_tol=0.001,\n",
    "    max_doc_update_iter=100, n_jobs=None, verbose=0, random_state=None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GaussComps_str_per_ob = [\" \".join(np.array(ob, dtype=str)) for ob in ObID_GaussComps_dict_122_chrono.values()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "GaussComps_str_per_ob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "GaussComps_str_per_ob = [\" \".join(np.array(ob, dtype=str)) for ob in ObID_GaussComps_dict_122_chrono.values()]\n",
    "vectorizer = CountVectorizer(analyzer='word', ngram_range=(1, 10), min_df=50)\n",
    "X = vectorizer.fit_transform(GaussComps_str_per_ob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "vectorizer.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "lda = LatentDirichletAllocation(n_components=100, random_state=0, verbose=1, batch_size=2000, n_jobs=20, max_iter=1000)\n",
    "lda.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "lda.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats.mstats import gmean\n",
    "lda_components_probs = pd.DataFrame(lda.transform(X))\n",
    "scaled_lda_components_probs = lda_components_probs.div(gmean(lda_components_probs, axis=1), axis=\"rows\")\n",
    "log_lda_components_probs = np.log(scaled_lda_components_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda.transform(X).sum(axis=1).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(lda.transform(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=0.99)\n",
    "pca.fit(log_lda_components_probs)\n",
    "\n",
    "print(pca.explained_variance_ratio_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(pca.explained_variance_ratio_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca.transform(log_lda_components_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = (5, 5)\n",
    "\n",
    "plt.plot(pca.explained_variance_ratio_)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reducer = umap.UMAP(random_state=42)\n",
    "reducer.fit(log_lda_components_probs)\n",
    "\n",
    "\n",
    "embedding = reducer.transform(log_lda_components_probs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "list(ObID_GaussComps_dict_122_chrono.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('{}/1776_light_curves_1s_bin_errorfix.pkl'.format(data_dir), 'rb') as f:\n",
    "    lcs = pickle.load(f)\n",
    "with open('{}/1776_light_curves_1s_bin_ids_errorfix.pkl'.format(data_dir), 'rb') as f:\n",
    "    ids = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.where(np.array(ids) == \"10408-01-03-00\")[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[2, 4, 5, 6, 7, 8, 9, 10, 12, 13, 14, 15, 16, 17, 19, 21, 22, 23, 25, 26, 27, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 41, 42, 43, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 59, 60, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 73, 75, 76, 78, 79, 80, 81, 83, 85, 86, 87, 88, 89, 90, 92, 93, 94, 95, 96, 97, 99]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = (20, 3)\n",
    "\n",
    "for ob_ind, ob in enumerate(lda.transform(X)):\n",
    "#     if np.sort(ob)[-1] > 0.9:\n",
    "    if np.argsort(ob)[-1] == 22:\n",
    "        lc_index = np.where(np.array(ids) == list(ObID_GaussComps_dict_122_chrono.keys())[ob_ind])[0][0]\n",
    "        fig, ax = plt.subplots(nrows=1, ncols=2)\n",
    "        ax[0].plot(lcs[lc_index][0], lcs[lc_index][1])\n",
    "        ax[1].plot(lcs[lc_index][0]-lcs[lc_index][0][0], lcs[lc_index][1])\n",
    "        ax[1].set_ylim((0,10000))\n",
    "        ax[1].set_xlim((0,3000))\n",
    "\n",
    "        plt.show()\n",
    "        print(list(ObID_GaussComps_dict_122_chrono.keys())[ob_ind], np.argsort(ob)[-5:], np.sort(ob)[-5:])\n",
    "#     print(np.argsort(ob), np.sort(ob)[-3:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(lda.transform(X)[2])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "good_comps = 0\n",
    "good_comps_list = []\n",
    "for component in range(100):\n",
    "    if np.mean(lda.components_[0]) != np.mean(lda.components_[component]):\n",
    "        good_comps+=1\n",
    "        good_comps_list.append(component)\n",
    "print(good_comps_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(lda.components_[1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca.transform(log_lda_components_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.cluster.hierarchy as sch\n",
    "# row_ratio = obs_component_counts_df_122.div(obs_component_counts_df_122.sum(axis=1).values, axis=\"rows\")\n",
    "\n",
    "linkage = sch.linkage(pca.transform(log_lda_components_probs), method=\"complete\", metric=\"euclidean\")\n",
    "plt.rcParams['figure.figsize'] = (6.97, 3)\n",
    "plt.rcParams.update({'font.size': 6})\n",
    "\n",
    "dendrogram = sch.dendrogram(linkage, truncate_mode=None, p=1, leaf_font_size = 0)\n",
    "axis = plt.gca()\n",
    "# axis.tick_params(axis=\"x\", which=\"major\", length=0, width=0, labelsize=0, direction=\"in\")\n",
    "axis.set_xticks([])\n",
    "axis.set_xticklabels([])\n",
    "axis.set_ylabel(\"Euclidean distance\", size=6, x=1)\n",
    "\n",
    "# plt.savefig('figures/agglomerative_dendogram.png', dpi=300, bbox_inches = 'tight',pad_inches = 0)\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.metrics import davies_bouldin_score\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (3.32, 3.32*(1/1))\n",
    "\n",
    "data = log_lda_components_probs\n",
    "metric = \"euclidean\"\n",
    "\n",
    "y_top_limit = linkage[-1][2]\n",
    "y_bottom_limit = linkage[0][2]\n",
    "no_clusters = []\n",
    "silhouette_scores = []\n",
    "for distance in np.linspace(y_bottom_limit,y_top_limit,1000)[:-1]:\n",
    "    clusters = sch.fcluster(linkage, distance, criterion='distance')\n",
    "    silhouette_scores.append(silhouette_score(data, clusters, metric=metric, random_state=0))\n",
    "    no_clusters.append(len(np.unique(clusters)))\n",
    "    \n",
    "    \n",
    "fig, ax1 = plt.subplots()\n",
    "\n",
    "\n",
    "plt.rcParams['figure.figsize'] = [5,5]\n",
    "# plt.axhline(114, c=\"cyan\")\n",
    "# ax1.axvline(0.98298298, c=\"black\")\n",
    "# ax1.axhline(60, c=\"black\")\n",
    "ax1.plot(np.linspace(y_bottom_limit,y_top_limit,1000)[:-1], no_clusters, c=\"magenta\")\n",
    "\n",
    "ax2 = ax1.twinx()\n",
    "ax2.plot(np.linspace(y_bottom_limit, y_top_limit,1000)[:-1], silhouette_scores, c=\"cyan\")\n",
    "# plt.title(\"No. of clusters as the function of distance for hierarchical clustering\")\n",
    "# ax3 = ax1.twinx()\n",
    "# ax3.plot(np.linspace(0,y_limit,1000)[:-1], calinski_harabasz_scores, c=\"orange\")\n",
    "# ax4 = ax1.twinx()\n",
    "# ax4.plot(np.linspace(0,y_limit,1000)[:-2], np.diff(silhouette_scores, 1), c=\"green\")\n",
    "\n",
    "ax1.set_ylabel(\"No. clusters\", c=\"magenta\")\n",
    "ax2.set_ylabel(\"Silhouette score\", c=\"cyan\")\n",
    "ax1.set_xlabel(\"Distance threshold\")\n",
    "# ax1.set_ylim((0,200))\n",
    "# plt.savefig('figures/agglomerative_silhouette.png', dpi=300, bbox_inches = 'tight',pad_inches = 0)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "print(\"Best 20 cluster numbers: \", np.array(no_clusters)[np.argsort(silhouette_scores)[-20:]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.cluster.hierarchy as sch\n",
    "# row_ratio = obs_component_counts_df_122.div(obs_component_counts_df_122.sum(axis=1).values, axis=\"rows\")\n",
    "\n",
    "linkage = sch.linkage(log_lda_components_probs, method=\"complete\", metric=\"euclidean\")\n",
    "plt.rcParams['figure.figsize'] = (6.97, 3)\n",
    "plt.rcParams.update({'font.size': 6})\n",
    "\n",
    "dendrogram = sch.dendrogram(linkage, truncate_mode=None, p=1, leaf_font_size = 0)\n",
    "axis = plt.gca()\n",
    "# axis.tick_params(axis=\"x\", which=\"major\", length=0, width=0, labelsize=0, direction=\"in\")\n",
    "axis.set_xticks([])\n",
    "axis.set_xticklabels([])\n",
    "axis.set_ylabel(\"Euclidean distance\", size=6, x=1)\n",
    "\n",
    "# plt.savefig('figures/agglomerative_dendogram.png', dpi=300, bbox_inches = 'tight',pad_inches = 0)\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.metrics import davies_bouldin_score\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (3.32, 3.32*(1/1))\n",
    "\n",
    "data = log_lda_components_probs\n",
    "metric = \"euclidean\"\n",
    "\n",
    "y_top_limit = linkage[-1][2]\n",
    "y_bottom_limit = linkage[0][2]\n",
    "no_clusters = []\n",
    "silhouette_scores = []\n",
    "for distance in np.linspace(y_bottom_limit,y_top_limit,1000)[:-1]:\n",
    "    clusters = sch.fcluster(linkage, distance, criterion='distance')\n",
    "    silhouette_scores.append(silhouette_score(data, clusters, metric=metric, random_state=0))\n",
    "    no_clusters.append(len(np.unique(clusters)))\n",
    "    \n",
    "    \n",
    "fig, ax1 = plt.subplots()\n",
    "\n",
    "\n",
    "plt.rcParams['figure.figsize'] = [5,5]\n",
    "# plt.axhline(114, c=\"cyan\")\n",
    "# ax1.axvline(0.98298298, c=\"black\")\n",
    "# ax1.axhline(60, c=\"black\")\n",
    "ax1.plot(np.linspace(y_bottom_limit,y_top_limit,1000)[:-1], no_clusters, c=\"magenta\")\n",
    "\n",
    "ax2 = ax1.twinx()\n",
    "ax2.plot(np.linspace(y_bottom_limit, y_top_limit,1000)[:-1], silhouette_scores, c=\"cyan\")\n",
    "# plt.title(\"No. of clusters as the function of distance for hierarchical clustering\")\n",
    "# ax3 = ax1.twinx()\n",
    "# ax3.plot(np.linspace(0,y_limit,1000)[:-1], calinski_harabasz_scores, c=\"orange\")\n",
    "# ax4 = ax1.twinx()\n",
    "# ax4.plot(np.linspace(0,y_limit,1000)[:-2], np.diff(silhouette_scores, 1), c=\"green\")\n",
    "\n",
    "ax1.set_ylabel(\"No. clusters\", c=\"magenta\")\n",
    "ax2.set_ylabel(\"Silhouette score\", c=\"cyan\")\n",
    "ax1.set_xlabel(\"Distance threshold\")\n",
    "# ax1.set_ylim((0,200))\n",
    "# plt.savefig('figures/agglomerative_silhouette.png', dpi=300, bbox_inches = 'tight',pad_inches = 0)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "print(\"Best 20 cluster numbers: \", np.array(no_clusters)[np.argsort(silhouette_scores)[-20:]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.linspace(y_bottom_limit,y_top_limit,1000)[:-1], no_clusters, c=\"magenta\")\n",
    "plt.ylim((0,250))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = (5,5)\n",
    "\n",
    "print(\"lda 2451 vocab log\")\n",
    "s=0.5\n",
    "plt.scatter(embedding[:,0], embedding[:,1], c=\"black\", s=s)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = (5,5)\n",
    "\n",
    "print(\"lda 893 vocab log\")\n",
    "s=0.5\n",
    "plt.scatter(embedding[:,0], embedding[:,1], c=\"black\", s=s)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "vectorizer.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tmtoolkit.topicmod.evaluate import metric_coherence_gensim\n",
    "metric_coherence_gensim(measure='c_v', \n",
    "                        top_n=100, \n",
    "                        topic_word_distrib=lda.components_, \n",
    "                        dtm=X, \n",
    "                        vocab=np.array([x for x in vectorizer.vocabulary_.keys()]), \n",
    "                        texts=GaussComps_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Signature:\n",
    "metric_coherence_gensim(\n",
    "    measure,\n",
    "    topic_word_distrib=None,\n",
    "    gensim_model=None,\n",
    "    vocab=None,\n",
    "    dtm=None,\n",
    "    gensim_corpus=None,\n",
    "    texts=None,\n",
    "    top_n=20,\n",
    "    return_coh_model=False,\n",
    "    return_mean=False,\n",
    "    **kwargs,\n",
    ")\n",
    "Docstring:\n",
    "Calculate model coherence using Gensim's\n",
    "`CoherenceModel <https://radimrehurek.com/gensim/models/coherencemodel.html>`_. See also this `tutorial\n",
    "<https://rare-technologies.com/what-is-topic-coherence/>`_.\n",
    "\n",
    "Define which measure to use with parameter `measure`:\n",
    "\n",
    "- ``'u_mass'``\n",
    "- ``'c_v'``\n",
    "- ``'c_uci'``\n",
    "- ``'c_npmi'``\n",
    "\n",
    "Provide a topic word distribution `topic_word_distrib` OR a Gensim model `gensim_model`\n",
    "and the corpus' vocabulary as `vocab` OR pass a gensim corpus as `gensim_corpus`. `top_n` controls how many most\n",
    "probable words per topic are selected.\n",
    "\n",
    "If measure is ``'u_mass'``, a document-term-matrix `dtm` or `gensim_corpus` must be provided and `texts` can be\n",
    "None. If any other measure than ``'u_mass'`` is used, tokenized input as `texts` must be provided as 2D list::\n",
    "\n",
    "    [['some', 'text', ...],          # doc. 1\n",
    "     ['some', 'more', ...],          # doc. 2\n",
    "     ['another', 'document', ...]]   # doc. 3\n",
    "\n",
    "If `return_coh_model` is True, the whole :class:`gensim.models.CoherenceModel` instance will be returned, otherwise:\n",
    "\n",
    "- if `return_mean` is True, the mean coherence value will be returned\n",
    "- if `return_mean` is False, a list of coherence values (for each topic) will be returned\n",
    "\n",
    "Provided `kwargs` will be passed to :class:`gensim.models.CoherenceModel` or\n",
    ":meth:`gensim.models.CoherenceModel.get_coherence_per_topic`.\n",
    "\n",
    ".. note:: This function also supports models from `lda` and `sklearn` (by passing `topic_word_distrib`, `dtm` and\n",
    "          `vocab`)!\n",
    "\n",
    ":param measure: the coherence calculation type; one of the values listed above\n",
    ":param topic_word_distrib: topic-word distribution; shape KxM, where K is number of topics, M is vocabulary size if\n",
    "                           `gensim_model` is not given\n",
    ":param gensim_model: a topic model from Gensim if `topic_word_distrib` is not given\n",
    ":param vocab: vocabulary list/array if `gensim_corpus` is not given\n",
    ":param dtm: document-term matrix of shape NxM with N documents and vocabulary size M  if `gensim_corpus` is not\n",
    "            given\n",
    ":param gensim_corpus: a Gensim corpus if `vocab` is not given\n",
    ":param texts: list of tokenized documents; necessary if using a `measure` other than ``'u_mass'``\n",
    ":param top_n: number of most probable words selected per topic\n",
    ":param return_coh_model: if True, return :class:`gensim.models.CoherenceModel` as result\n",
    ":param return_mean: if `return_coh_model` is False and `return_mean` is True, return mean coherence\n",
    ":param kwargs: parameters passed to :class:`gensim.models.CoherenceModel` or\n",
    "               :meth:`gensim.models.CoherenceModel.get_coherence_per_topic`\n",
    ":return: if `return_coh_model` is True, return :class:`gensim.models.CoherenceModel` as result; otherwise if\n",
    "         `return_mean` is True, mean of all coherence values, otherwise array of length K with coherence per\n",
    "         topic\n",
    "File:      /export/data/jakubok/software/anaconda3/envs/py37/lib/python3.7/site-packages/tmtoolkit/topicmod/evaluate.py\n",
    "Type:      function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_lda_components_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.close()\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (30,30)\n",
    "plt.rcParams.update({'font.size': 10})\n",
    "\n",
    "fig, axes = plt.subplots(nrows=10, ncols=10)\n",
    "\n",
    "tril = np.array(np.tril_indices(10)).T\n",
    "\n",
    "data = log_lda_components_probs\n",
    "\n",
    "for x,y in tril:\n",
    "    axes[x,y].scatter(data.iloc[:,x],data.iloc[:,y])\n",
    "    axes[x,y].set_ylim((-3,13))\n",
    "    axes[x,y].set_xlim((-3,13))\n",
    "    axes[x,y].text(0.99, 0.99, np.corrcoef(data.iloc[:,x],data.iloc[:,y])[0,1], ha='right', va='top', transform=axes[x,y].transAxes, size=10)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.close()\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (30,30)\n",
    "plt.rcParams.update({'font.size': 10})\n",
    "\n",
    "fig, axes = plt.subplots(nrows=10, ncols=10)\n",
    "\n",
    "tril = np.array(np.tril_indices(10)).T\n",
    "\n",
    "data = log_lda_components_probs\n",
    "\n",
    "for x,y in tril:\n",
    "    axes[x,y].scatter(data.iloc[:,x],data.iloc[:,y])\n",
    "#     axes[x,y].set_ylim((-0.1,1))\n",
    "#     axes[x,y].set_xlim((-0.1,1))\n",
    "    axes[x,y].text(0.99, 0.99, np.corrcoef(data.iloc[:,x],data.iloc[:,y])[0,1], ha='right', va='top', transform=axes[x,y].transAxes, size=10)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(lda.transform(X).flatten())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build LDA model\n",
    "# lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
    "#                                            id2word=id2word,\n",
    "#                                            num_topics=20, \n",
    "#                                            random_state=100,\n",
    "#                                            update_every=1,\n",
    "#                                            chunksize=100,\n",
    "#                                            passes=10,\n",
    "#                                            alpha='auto',\n",
    "#                                            per_word_topics=True)\n",
    "\n",
    "# Build LDA model\n",
    "lda_model = gensim.models.LdaMulticore(corpus=corpus,\n",
    "                                       id2word=id2word,\n",
    "                                       num_topics=100, \n",
    "                                       random_state=0,\n",
    "                                       chunksize=2000,\n",
    "                                       passes=10,\n",
    "                                       per_word_topics=True, \n",
    "                                       minimum_probability=0.01,\n",
    "                                      workers=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "lda_model = gensim.models.LdaMulticore(corpus=corpus_quad,\n",
    "                                       id2word=id2word_quad,\n",
    "                                       num_topics=1000, \n",
    "                                       random_state=0,\n",
    "                                       chunksize=2000,\n",
    "                                       passes=10,\n",
    "                                       per_word_topics=True, \n",
    "                                       minimum_probability=0.01,\n",
    "                                      workers=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Print the Keyword in the 10 topics\n",
    "print(lda_model.print_topics(num_topics=-1, num_words=122))\n",
    "doc_lda = lda_model[corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(lda_model.print_topics(num_topics=-1, num_words=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_lda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute Perplexity\n",
    "print('\\nPerplexity: ', lda_model.log_perplexity(corpus))  # a measure of how good the model is. lower the better.\n",
    "\n",
    "# Compute Coherence Score\n",
    "coherence_model_lda = gensim.models.CoherenceModel(model=lda_model, texts=GaussComps_str, dictionary=id2word, coherence='c_v')\n",
    "coherence_lda = coherence_model_lda.get_coherence()\n",
    "print('\\nCoherence Score: ', coherence_lda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute Perplexity\n",
    "print('\\nPerplexity: ', lda_model.log_perplexity(corpus_quad))  # a measure of how good the model is. lower the better.\n",
    "\n",
    "# Compute Coherence Score\n",
    "coherence_model_lda = gensim.models.CoherenceModel(model=lda_model, texts=data_words_quadgrams, dictionary=id2word_quad, coherence='c_v')\n",
    "coherence_lda = coherence_model_lda.get_coherence()\n",
    "print('\\nCoherence Score: ', coherence_lda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import re\n",
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# from pprint import pprint\n",
    "\n",
    "# # Gensim\n",
    "# import gensim\n",
    "# import gensim.corpora as corpora\n",
    "# from gensim.utils import simple_preprocess\n",
    "# from gensim.models import CoherenceModel\n",
    "\n",
    "# # spacy for lemmatization\n",
    "# import spacy\n",
    "\n",
    "# Plotting tools\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim  # don't skip this\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# # Enable logging for gensim - optional\n",
    "# import logging\n",
    "# logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.ERROR)\n",
    "\n",
    "# import warnings\n",
    "# warnings.filterwarnings(\"ignore\",category=DeprecationWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyLDAvis.enable_notebook()\n",
    "vis = pyLDAvis.gensim.prepare(lda_model, corpus, id2word)\n",
    "vis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_coherence_values(id2word, corpus, texts, limit, start=2, step=3):\n",
    "    \"\"\"\n",
    "    Compute c_v coherence for various number of topics\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    dictionary : Gensim dictionary\n",
    "    corpus : Gensim corpus\n",
    "    texts : List of input texts\n",
    "    limit : Max num of topics\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "    model_list : List of LDA topic models\n",
    "    coherence_values : Coherence values corresponding to the LDA model with respective number of topics\n",
    "    \"\"\"\n",
    "    coherence_values = []\n",
    "    model_list = []\n",
    "    perpexities=[]\n",
    "    for num_topics in range(start, limit, step):\n",
    "        print(num_topics)\n",
    "        model = gensim.models.LdaMulticore(corpus=corpus, num_topics=num_topics, iterations=100, minimum_probability=0.01,\n",
    "                                                    id2word=id2word,chunksize=2000, random_state=1,passes=50,workers=35)\n",
    "        model_list.append(model)\n",
    "        coherencemodel = gensim.models.CoherenceModel(model=model, texts=texts, dictionary=dictionary, coherence='c_v')\n",
    "        coherence_values.append(coherencemodel.get_coherence())\n",
    "        perpexities.append(model.log_perplexity(corpus))\n",
    "#         np.savetxt(\"lda_gridsearch15-26_rng1.csv\", np.array([np.array(range(start, limit, step))[:len(perpexities)],coherence_values,perpexities]))\n",
    "\n",
    "    return model_list, coherence_values, perpexities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "model_list, coherence_values, perpexities = compute_coherence_values(id2word=id2word_quad, corpus=corpus_quad, texts=GaussComps_str, start=10, limit=100, step=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model = gensim.models.LdaMulticore(corpus=corpus_quad,\n",
    "                                       id2word=id2word_quad,\n",
    "                                       num_topics=1000, \n",
    "                                       random_state=0,\n",
    "                                       chunksize=2000,\n",
    "                                       passes=10,\n",
    "                                       per_word_topics=True, \n",
    "                                       minimum_probability=0.01,\n",
    "                                      workers=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_list2, coherence_values2, perpexities2 = compute_coherence_values(dictionary=id2word, corpus=corpus, texts=GaussComps_str, start=15, limit=26, step=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda23 = model_list[13]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_topics_sentences(ldamodel=lda_model, corpus=corpus, texts=GaussComps_str):\n",
    "    # Init output\n",
    "    sent_topics_df = pd.DataFrame()\n",
    "\n",
    "    # Get main topic in each document\n",
    "    for i, row in enumerate(ldamodel[corpus]):\n",
    "        row = sorted(row, key=lambda x: (x[1]), reverse=True)\n",
    "        # Get the Dominant topic, Perc Contribution and Keywords for each document\n",
    "        for j, (topic_num, prop_topic) in enumerate(row):\n",
    "            if j == 0:  # => dominant topic\n",
    "                wp = ldamodel.show_topic(topic_num)\n",
    "                topic_keywords = \", \".join([word for word, prop in wp])\n",
    "                sent_topics_df = sent_topics_df.append(pd.Series([int(topic_num), round(prop_topic,4), topic_keywords]), ignore_index=True)\n",
    "            else:\n",
    "                break\n",
    "    sent_topics_df.columns = ['Dominant_Topic', 'Perc_Contribution', 'Topic_Keywords']\n",
    "\n",
    "    # Add original text to the end of the output\n",
    "    contents = pd.Series(texts)\n",
    "    sent_topics_df = pd.concat([sent_topics_df, contents], axis=1)\n",
    "    return(sent_topics_df)\n",
    "\n",
    "\n",
    "df_topic_sents_keywords = format_topics_sentences(ldamodel=lda23, corpus=corpus, texts=GaussComps_str)\n",
    "\n",
    "# Format\n",
    "df_dominant_topic = df_topic_sents_keywords.reset_index()\n",
    "df_dominant_topic.columns = ['Document_No', 'Dominant_Topic', 'Topic_Perc_Contrib', 'Keywords', 'Text']\n",
    "\n",
    "df_dominant_topic[\"Indices\"] = df_dominant_topic.index.values\n",
    "\n",
    "# Show\n",
    "df_dominant_topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group top 5 sentences under each topic\n",
    "sent_topics_sorted_df = pd.DataFrame()\n",
    "\n",
    "sent_topics_outdf_grpd = df_topic_sents_keywords.groupby('Dominant_Topic')\n",
    "\n",
    "for i, grp in sent_topics_outdf_grpd:\n",
    "    sent_topics_sorted_df = pd.concat([sent_topics_sorted_df, \n",
    "                                             grp.sort_values(['Perc_Contribution'], ascending=[0]).head(5)], \n",
    "                                            axis=0)\n",
    "\n",
    "# Reset Index    \n",
    "# sent_topics_sorted_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Format\n",
    "sent_topics_sorted_df.columns = ['Topic_Num', \"Topic_Perc_Contrib\", \"Keywords\", \"Text\"]\n",
    "\n",
    "# Show\n",
    "sent_topics_sorted_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_topics_sorted_df.index.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('{}/1776_light_curves_1s_bin_errorfix.pkl'.format(data_dir), 'rb') as f:\n",
    "    lcs = pickle.load(f)\n",
    "with open('{}/1776_light_curves_1s_bin_ids_errorfix.pkl'.format(data_dir), 'rb') as f:\n",
    "    ids = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "plt.close()\n",
    "\n",
    "# counts_todo = [i for i,v in class_normalized_comparison_matrix_df[\"omega\"].sort_values(ascending=False)[:10].items() if v>=0.5]\n",
    "\n",
    "# if len(counts_todo) > 3:\n",
    "#     how_many_to_plot = 10\n",
    "# else:\n",
    "how_many_to_plot = 5\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (3.32*3,3.32*(2)*3)\n",
    "plt.rcParams.update({'font.size': 6})\n",
    "\n",
    "# component_counts = np.unique(shape_moments_GMM122_labels, return_counts=1)\n",
    "\n",
    "# max_stats = np.max(shape_moments[:,-4:],axis=0)\n",
    "# min_stats = np.min(shape_moments[:,-4:],axis=0)\n",
    "\n",
    "for plot_topic in range(23):\n",
    "    fig, axes = plt.subplots(nrows=how_many_to_plot, ncols=2)\n",
    "#     component_segment_indices = np.where(shape_moments_GMM122_labels == plot_component)[0]\n",
    "    for plot_ind in range(how_many_to_plot):\n",
    "        lc_index = sent_topics_sorted_df.index.values[plot_topic*5 + plot_ind]\n",
    "        data = lcs[lc_index]\n",
    "#         reconstruction = segment_reconstructions[component_segment_indices[plot_ind]]\n",
    "        axes[plot_ind,0].plot(data[0]-data[0][0],data[1]/1000)#, c=\"green\", linewidth=0.5, zorder=-5)\n",
    "        axes[plot_ind,0].text(0.99,0.99,\"{}\".format(lc_index), ha='right', va='top', transform=axes[plot_ind, 0].transAxes, size=8)\n",
    "        \n",
    "#         axes[plot_ind,0].plot(np.array(list(range(128)))*4, reconstruction/1000)\n",
    "        axes[plot_ind,1].plot(data[0]-data[0][0],data[1]/1000)#, c=\"green\", linewidth=0.5, zorder=-5)\n",
    "#         axes[plot_ind,1].plot(np.array(list(range(128)))*4, reconstruction/1000)\n",
    "    #     axes[subplot, 1].plot(np.array(list(range(128)))*4,segments_counts[class_segments[subplot]])\n",
    "    #     axes[subplot, 1].plot(np.array(list(range(128)))*4, reconstruction*np.std(data)+np.mean(data))\n",
    "        axes[plot_ind, 1].set_ylim([0, 12])\n",
    "        axes[plot_ind, 1].set_xlim([0, 3000])\n",
    "        axes[plot_ind, 1].yaxis.tick_right()\n",
    "        \n",
    "#         bar_heights=np.zeros(4)\n",
    "#         bar_heights[0] = np.mean(data)\n",
    "#         bar_heights[1] = np.std(data)\n",
    "#         bar_heights[2] = stats.skew(data)\n",
    "#         bar_heights[3] = stats.kurtosis(data)\n",
    "\n",
    "#         color = [\"red\" if x else \"blue\" for x in shape_moments[component_segment_indices[plot_ind],-4:]<0]\n",
    "#         bar_heights = (shape_moments[component_segment_indices[plot_ind],-4:]-min_stats)/ (max_stats-min_stats)\n",
    "#         axes[plot_ind, 2].bar(x=range(4), height=bar_heights)#, color=color)\n",
    "#         axes[plot_ind, 2].set_ylim([0,1])#[np.min(shape_moments[:,-4:]), np.max(shape_moments[:,-4:])])\n",
    "\n",
    "    #     if class_name == \"Unknown\": \n",
    "    #         class_name = \"??\"\n",
    "    #     else:\n",
    "    #         class_name = r\"$\\{}$\".format(class_name)\n",
    "    #     axes[subplot, 1].text(x=510, y=9500, s=class_name)\n",
    "\n",
    "        if plot_ind == 2:\n",
    "            axes[plot_ind,0].set_ylabel(\"Rate (kcounts/s)\", size=6)\n",
    "        if plot_ind == 4:#plot_ind == 12 or plot_ind == 13\n",
    "            axes[plot_ind,0].tick_params(axis=\"x\", which=\"major\", length=2, width=0.75, labelsize=6, direction=\"in\")\n",
    "            axes[plot_ind,1].tick_params(axis=\"x\", which=\"major\", length=2, width=0.75, labelsize=6, direction=\"in\")\n",
    "            axes[plot_ind,0].set_xlabel(\"Time (s)\", size=6, x=1)\n",
    "\n",
    "        else:\n",
    "            axes[plot_ind,0].tick_params(axis=\"x\", which=\"major\", length=2, width=0.75, labelsize=0, direction=\"in\")\n",
    "            axes[plot_ind,1].tick_params(axis=\"x\", which=\"major\", length=2, width=0.75, labelsize=0, direction=\"in\")\n",
    "\n",
    "    plt.suptitle(\"component {}\".format(plot_topic), y=1.01)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda23.get_document_topics(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import pyLDAvis\n",
    "import pyLDAvis.gensim  # don't skip this\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "pyLDAvis.enable_notebook()\n",
    "vis = pyLDAvis.gensim.prepare(lda23, corpus, id2word)\n",
    "vis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "limit=10; start=30; step=1;\n",
    "x = range(10, 30, 1)\n",
    "plt.plot(x, coherence_values, label=\"coherence_values\")\n",
    "x = range(15, 26, 1)\n",
    "plt.plot(x, coherence_values2, label=\"coherence_values\")\n",
    "# plt.plot(x, perpexities, label=\"perpexities\")\n",
    "plt.xlabel(\"Num Topics\")\n",
    "plt.ylabel(\"score\")\n",
    "plt.xlim((0,50))\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(range(10, 30, 1))[np.argsort(coherence_values)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.argsort(coherence_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(list(range(start, limit, step)))[np.argsort(perpexities)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(list(range(start, limit, step)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_quad = [] # list of lists of tuples with word id and count within observation\n",
    "for observation in GaussComps_str:\n",
    "    ind_counts = [(ind, count)for ind, count in enumerate(np.array(np.unique(observation, return_counts=True ))[1])]\n",
    "    corpus_quad.append(ind_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[(ind, count)for ind, count in enumerate(np.array(np.unique(GaussComps_str[0], return_counts=True ))[1])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(( np.unique(data_words_quadgrams[0], return_counts=True)[0]).shape)\n",
    "np.unique(data_words_quadgrams[0], return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique([item for sublist in data_words_quadgrams for item in sublist])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data_words_quadgrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.utils import simple_preprocess\n",
    "\n",
    "def sent_to_words(sentences):\n",
    "    for sentence in sentences:\n",
    "        yield(gensim.utils.simple_preprocess(str(sentence), deacc=False))  # deacc=True removes punctuationsdata = papers.paper_text_processed.values.tolist()\n",
    "data_words = list(sent_to_words(GaussComps_str))\n",
    "print(data_words[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = GaussComps_str\n",
    "bigram = gensim.models.Phrases(texts)\n",
    "texts = [bigram[line] for line in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "texts[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "[trigram_mod[bigram_mod[doc]] for doc in GaussComps_str[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in bigram_mod[ObID_GaussComps_dict_122.values()]:\n",
    "#     print(i)\n",
    "    \n",
    "print(bigram_mod[GaussComps_mt2[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram.save_corpus(\"corpus\", ObID_GaussComps_dict_122.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "# from gensim.utils import simple_preprocess\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.corpora as corpora# Create Dictionary\n",
    "id2word = corpora.Dictionary(data_lemmatized)# Create Corpus\n",
    "texts = data_lemmatized# Term Document Frequency\n",
    "corpus = [id2word.doc2bow(text) for text in texts]# View\n",
    "print(corpus[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Build the bigram and trigram models\n",
    "bigram = gensim.models.Phrases(data_words, min_count=5, threshold=100) # higher threshold fewer phrases.\n",
    "trigram = gensim.models.Phrases(bigram[data_words], threshold=100)  \n",
    "\n",
    "# Faster way to get a sentence clubbed as a trigram/bigram\n",
    "bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "trigram_mod = gensim.models.phrases.Phraser(trigram)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import CoherenceModel# Compute Coherence Score\n",
    "lda_model = gensim.models.LdaMulticore(corpus=corpus, #(word_id, word_frequency)\n",
    "                                       id2word=id2word,\n",
    "                                       num_topics=10, \n",
    "                                       random_state=100,\n",
    "                                       chunksize=100,\n",
    "                                       passes=10,\n",
    "                                       per_word_topics=True)\n",
    "coherence_model_lda = CoherenceModel(model=lda_model, texts=data_lemmatized, dictionary=id2word, coherence='c_v')\n",
    "coherence_lda = coherence_model_lda.get_coherence()print('\\nCoherence Score: ', coherence_lda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_scores = []\n",
    "val_scores = []\n",
    "for comps, scores in results:\n",
    "    train_scores.append(scores['train_score'])\n",
    "    val_scores.append(scores['test_score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(train_scores).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for iteration in range(5):\n",
    "    plt.plot(np.array(range(20))+1, np.array(train_scores)[:,iteration])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "sorted(sklearn.metrics.SCORERS.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualise agglomerative cluster populations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters = sch.fcluster(linkage, 0.98298298, criterion='distance')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "plt.close()\n",
    "\n",
    "# counts_todo = [i for i,v in class_normalized_comparison_matrix_df[\"omega\"].sort_values(ascending=False)[:10].items() if v>=0.5]\n",
    "\n",
    "# if len(counts_todo) > 3:\n",
    "#     how_many_to_plot = 10\n",
    "# else:\n",
    "\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (3.32*3,3.32*(2)*2)\n",
    "plt.rcParams.update({'font.size': 6})\n",
    "\n",
    "lcs_agglo_clusters = clusters-1 # output of fcluster is 1 indexed # np.unique(shape_moments_GMM122_labels, return_counts=1)\n",
    "\n",
    "max_stats = np.max(shape_moments[:,-4:],axis=0)\n",
    "min_stats = np.min(shape_moments[:,-4:],axis=0)\n",
    "\n",
    "for plot_component in range(60):\n",
    "    \n",
    "    cluster_lc_indices = np.where(lcs_agglo_clusters == plot_component)[0]\n",
    "    cluster_lc_ids = obs_component_counts_df_122.index.values[cluster_lc_indices]\n",
    "    how_many_to_plot = len(cluster_lc_indices)\n",
    "    fig, axes = plt.subplots(nrows=how_many_to_plot, ncols=2)\n",
    "    plt.rcParams['figure.figsize'] = (3.32*2,3.32*4*(2/3)*(how_many_to_plot/10))\n",
    "\n",
    "    for plot_ind in range(how_many_to_plot):\n",
    "        data = lcs[np.where(np.array(ids) == cluster_lc_ids[plot_ind])[0][0]]\n",
    "#         reconstruction = segment_reconstructions[component_segment_indices[plot_ind]]\n",
    "        axes[plot_ind,0].plot(data[0]-data[0][0],data[1]/1000)#, c=\"green\", linewidth=0.5, zorder=-5)\n",
    "#         axes[plot_ind,0].plot(np.array(list(range(128)))*4, reconstruction/1000)\n",
    "        axes[plot_ind,1].plot(data[0][:3000]-data[0][0],data[1][:3000]/1000)#, c=\"green\", linewidth=0.5, zorder=-5)\n",
    "#         axes[plot_ind,1].plot(np.array(list(range(128)))*4, reconstruction/1000)\n",
    "    #     axes[subplot, 1].plot(np.array(list(range(128)))*4,segments_counts[class_segments[subplot]])\n",
    "    #     axes[subplot, 1].plot(np.array(list(range(128)))*4, reconstruction*np.std(data)+np.mean(data))\n",
    "        axes[plot_ind, 1].set_ylim([0, 12])\n",
    "#         axes[plot_ind, 1].set_xlim([0, 3000])\n",
    "        axes[plot_ind, 1].yaxis.tick_right()\n",
    "        \n",
    "#         bar_heights=np.zeros(4)\n",
    "#         bar_heights[0] = np.mean(data)\n",
    "#         bar_heights[1] = np.std(data)\n",
    "#         bar_heights[2] = stats.skew(data)\n",
    "#         bar_heights[3] = stats.kurtosis(data)\n",
    "\n",
    "#         color = [\"red\" if x else \"blue\" for x in shape_moments[component_segment_indices[plot_ind],-4:]<0]\n",
    "#         bar_heights = (shape_moments[component_segment_indices[plot_ind],-4:]-min_stats)/ (max_stats-min_stats)\n",
    "#         axes[plot_ind, 2].bar(x=range(4), height=bar_heights)#, color=color)\n",
    "#         axes[plot_ind, 2].set_ylim([0,1])#[np.min(shape_moments[:,-4:]), np.max(shape_moments[:,-4:])])\n",
    "\n",
    "    #     if class_name == \"Unknown\": \n",
    "    #         class_name = \"??\"\n",
    "    #     else:\n",
    "    #         class_name = r\"$\\{}$\".format(class_name)\n",
    "    #     axes[subplot, 1].text(x=510, y=9500, s=class_name)\n",
    "\n",
    "#         if plot_ind == 4:\n",
    "#             axes[plot_ind,0].set_ylabel(\"Rate (kcounts/s)\", size=6)\n",
    "        if plot_ind == len(cluster_lc_indices)-1:#plot_ind == 12 or plot_ind == 13\n",
    "            axes[plot_ind,0].tick_params(axis=\"x\", which=\"major\", length=2, width=0.75, labelsize=6, direction=\"in\")\n",
    "            axes[plot_ind,1].tick_params(axis=\"x\", which=\"major\", length=2, width=0.75, labelsize=6, direction=\"in\")\n",
    "# #             axes[plot_ind,0].set_xlabel(\"Time (s)\", size=6, x=1)\n",
    "\n",
    "        else:\n",
    "#             axes[plot_ind,0].tick_params(axis=\"x\", which=\"major\", length=2, width=0.75, labelsize=0, direction=\"in\")\n",
    "#             axes[plot_ind,1].tick_params(axis=\"x\", which=\"major\", length=2, width=0.75, labelsize=0, direction=\"in\")\n",
    "            axes[plot_ind,0].set_xticklabels([])\n",
    "            axes[plot_ind,1].set_xticklabels([])\n",
    "            \n",
    "        if plot_ind == len(cluster_lc_indices)-1:\n",
    "            break\n",
    "\n",
    "    plt.suptitle(\"component {}, population {}\".format(plot_component, len(cluster_lc_indices)))\n",
    "    plt.savefig('figures/agglo_clusters/component{}population{}.png'.format(plot_component, len(cluster_lc_indices)), dpi=100, bbox_inches = 'tight',pad_inches = 0)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lcs[np.where(np.array(ids) == cluster_lc_ids[plot_ind])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.where(np.array(ids) == cluster_lc_ids[plot_ind])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(lcs_agglo_clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_limit = linkage[-1][2]\n",
    "no_clusters = []\n",
    "for distance in np.linspace(0,y_limit,1000):\n",
    "    clusters = sch.fcluster(linkage, distance, criterion='distance')\n",
    "    no_clusters.append(len(np.unique(clusters)))\n",
    "\n",
    "plt.rcParams['figure.figsize'] = [5,5]\n",
    "# plt.axhline(114, c=\"cyan\")\n",
    "plt.plot(np.linspace(0,y_limit,1000), no_clusters, c=\"magenta\")\n",
    "plt.title(\"No. of clusters as the function of distance for hierarchical clustering\")\n",
    "plt.ylabel(\"No. clusters\")\n",
    "plt.xlabel(\"Distance threshold\")\n",
    "# plt.ylim((0,50))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linkage = sch.linkage(non_zero_row_ratio, method=\"complete\", metric=aitchison)\n",
    "dendrogram = sch.dendrogram(linkage, truncate_mode=None, p=1, leaf_font_size = 0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_limit = linkage[-1][2]\n",
    "no_clusters = []\n",
    "for distance in np.linspace(0,y_limit,1000):\n",
    "    clusters = sch.fcluster(linkage, distance, criterion='distance')\n",
    "    no_clusters.append(len(np.unique(clusters)))\n",
    "\n",
    "plt.rcParams['figure.figsize'] = [5,5]\n",
    "# plt.axhline(114, c=\"cyan\")\n",
    "plt.plot(np.linspace(0,y_limit,1000), no_clusters, c=\"magenta\")\n",
    "plt.title(\"No. of clusters as the function of distance for hierarchical clustering\")\n",
    "plt.ylabel(\"No. clusters\")\n",
    "plt.xlabel(\"Distance threshold\")\n",
    "# plt.ylim((0,50))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_clusters[-2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(obs_component_counts_df_122.iloc[:,0], obs_component_counts_df_122.iloc[:,4])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(normalized_obs_component_counts_df_122.iloc[:,0], normalized_obs_component_counts_df_122.iloc[:,4])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(transformed_features[:,0], transformed_features[:,1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.cluster.hierarchy import dendrogram\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "\n",
    "\n",
    "def plot_dendrogram(model, **kwargs):\n",
    "    # Create linkage matrix and then plot the dendrogram\n",
    "\n",
    "    # create the counts of samples under each node\n",
    "    counts = np.zeros(model.children_.shape[0])\n",
    "    n_samples = len(model.labels_)\n",
    "    for i, merge in enumerate(model.children_):\n",
    "        current_count = 0\n",
    "        for child_idx in merge:\n",
    "            if child_idx < n_samples:\n",
    "                current_count += 1  # leaf node\n",
    "            else:\n",
    "                current_count += counts[child_idx - n_samples]\n",
    "        counts[i] = current_count\n",
    "\n",
    "    linkage_matrix = np.column_stack([model.children_, model.distances_,\n",
    "                                      counts]).astype(float)\n",
    "\n",
    "    # Plot the corresponding dendrogram\n",
    "    dendrogram(linkage_matrix, **kwargs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.cluster.hierarchy as sch\n",
    "\n",
    "def plot_denogram_and_no_clusters_curve(method, metric, data_set, data_set_name):\n",
    "    \"\"\"\n",
    "    method : how are the clusters agglomerated, eg. merge the two nearest clusters w.r.t the furthest data point(\"complete\"), \n",
    "                cluster centroid (\"average\"), minimum variance (\"ward\")\n",
    "    metric : euclidean, cosine (angular metric ignores the vector magnitude, so can be helpful for our counts)\n",
    "    data_set : one of the data frames with shape [1738, 500]\n",
    "    data_set_name : row_standardised, column_standardised, row_normalised, raw_counts\n",
    "    \"\"\"\n",
    "    \n",
    "    if method == \"ward\": # must be euclidean\n",
    "        metric = \"euclidean\"\n",
    "    \n",
    "    print(\"Method : {}, Metric : {}, Data : {}\".format(method, metric, data_set_name))\n",
    "    plt.rcParams['figure.figsize'] = [9,3]\n",
    "    linkage = sch.linkage(data_set, method=method, metric=metric)\n",
    "    dendrogram = sch.dendrogram(linkage, truncate_mode=None, p=2, leaf_font_size = 0)\n",
    "    y_limit = linkage[-1][2] # distance value of the last merger\n",
    "    ax=plt.gca()\n",
    "    ax.set_ylabel(\"Euclidean distance\")\n",
    "    ax.set_xlabel(\"Observations of the source (1738)\")\n",
    "    ax.set_xticklabels(())\n",
    "    plt.title(\"Method : {}, Metric : {}, Data : {}\".format(method, metric, data_set_name))\n",
    "    plt.show()\n",
    "\n",
    "    # find no. clusters as a function of distance\n",
    "    no_clusters = []\n",
    "    for distance in np.linspace(0,y_limit,1000):\n",
    "        clusters = sch.fcluster(linkage, distance, criterion='distance')\n",
    "        no_clusters.append(len(np.unique(clusters)))\n",
    "\n",
    "    plt.rcParams['figure.figsize'] = [5,5]\n",
    "    plt.axhline(114, c=\"cyan\")\n",
    "    plt.plot(np.linspace(0,y_limit,1000), no_clusters, c=\"magenta\")\n",
    "    plt.title(\"No. of clusters as the function of distance for hierarchical clustering\")\n",
    "    plt.ylabel(\"No. clusters\")\n",
    "    plt.xlabel(\"Distance threshold\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_denogram_and_no_clusters_curve(\"complete\", \"cosine\", obs_component_counts_df_122, \"raw_counts\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "method = \"complete\"\n",
    "metric = \"cosine\"\n",
    "data_set = obs_component_counts_df_122\n",
    "data_set_name = \"raw_counts\"\n",
    "\n",
    "print(\"Method : {}, Metric : {}, Data : {}\".format(method, metric, data_set_name))\n",
    "plt.rcParams['figure.figsize'] = [9,3]\n",
    "linkage = sch.linkage(data_set, method=method, metric=metric)\n",
    "dendrogram = sch.dendrogram(linkage, truncate_mode=None, p=2, leaf_font_size = 0)\n",
    "y_limit = linkage[-1][2] # distance value of the last merger\n",
    "ax=plt.gca()\n",
    "ax.set_ylabel(\"Euclidean distance\")\n",
    "ax.set_xlabel(\"Observations of the source (1738)\")\n",
    "ax.set_xticklabels(())\n",
    "plt.title(\"Method : {}, Metric : {}, Data : {}\".format(method, metric, data_set_name))\n",
    "plt.show()\n",
    "\n",
    "# find no. clusters as a function of distance\n",
    "no_clusters = []\n",
    "for distance in np.linspace(0,y_limit,1000):\n",
    "    clusters = sch.fcluster(linkage, distance, criterion='distance')\n",
    "    no_clusters.append(len(np.unique(clusters)))\n",
    "\n",
    "plt.rcParams['figure.figsize'] = [5,5]\n",
    "plt.axhline(73, c=\"cyan\")\n",
    "plt.plot(np.linspace(0,y_limit,1000), no_clusters, c=\"magenta\")\n",
    "plt.title(\"No. of clusters as the function of distance for hierarchical clustering\")\n",
    "plt.ylabel(\"No. clusters\")\n",
    "plt.xlabel(\"Distance threshold\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = [5,5]\n",
    "plt.axhline(114, c=\"cyan\")\n",
    "plt.plot(np.linspace(0,y_limit,1000), no_clusters, c=\"magenta\")\n",
    "plt.xlim([0.65,1])\n",
    "plt.ylim((0,120))\n",
    "plt.title(\"No. of clusters as the function of distance for hierarchical clustering\")\n",
    "plt.ylabel(\"No. clusters\")\n",
    "plt.xlabel(\"Distance threshold\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "for i in range(100):\n",
    "    print(np.linspace(0,y_limit,1000)[-i], no_clusters[-i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import AgglomerativeClustering\n",
    "\n",
    "clustering = AgglomerativeClustering(n_clusters=55, linkage=method, affinity=metric).fit(data_set )\n",
    "clustering\n",
    "\n",
    "ob_clusters_55_labels  = clustering.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load observation classifications from Huppenkothen 2017\n",
    "clean_belloni = open('{}/1915Belloniclass_updated.dat'.format(data_dir))\n",
    "lines = clean_belloni.readlines()\n",
    "states = lines[0].split()\n",
    "belloni_clean = {}\n",
    "for h,l in zip(states, lines[1:]):\n",
    "    belloni_clean[h] = l.split()\n",
    "    #state: obsID1, obsID2...\n",
    "ob_state = {}\n",
    "for state, obs in belloni_clean.items():\n",
    "    if state == \"chi1\" or state == \"chi2\" or state == \"chi3\" or state == \"chi4\": state = \"chi\"\n",
    "    for ob in obs:\n",
    "        ob_state[ob] = state\n",
    "        \n",
    "# make a dict of Tomaso's classifications  (Daniela's set) against observation ids,\n",
    "# i.e. ob_state == {'20187-02-01-00': 'alpha', '20187-02-01-01': 'alpha', '20402-01-22-00': 'alpha', ...}\n",
    "# lines = clean_belloni.readlines()\n",
    "# states = lines[0].split()\n",
    "# belloni_clean = {}\n",
    "# for h,l in zip(states, lines[1:]):\n",
    "#     belloni_clean[h] = l.split()\n",
    "#     #state: obsID1, obsID2...\n",
    "# ob_state = {}\n",
    "# for state, obs in belloni_clean.items():\n",
    "#     if state == \"chi1\" or state == \"chi2\" or state == \"chi3\" or state == \"chi4\": state = \"chi\"\n",
    "#     for ob in obs:\n",
    "#         ob_state[ob] = state\n",
    "        \n",
    "        \n",
    "# load IDs of segmented light curves: observationsID_segmentIndex\n",
    "with open('{}/468202_len128_s2_4cad_ids_errorfix.pkl'.format(data_dir), 'rb') as f:\n",
    "    seg_ids = pickle.load(f)\n",
    "\n",
    "        \n",
    "seg_ObIDs = [seg.split(\"_\")[0] for seg in seg_ids] # get rid of the within-observation segment indices and create a degenerate list of observation IDs\n",
    "\n",
    "classes = np.array([\"alpha\", \"beta\", \"gamma\", \"delta\", \"theta\", \"kappa\", \"lambda\", \"mu\", \"nu\", \"rho\", \"phi\", \"chi\", \"eta\", \"omega\"])\n",
    "scales = []\n",
    "segment_class = []\n",
    "for ob in seg_ObIDs:\n",
    "    if ob in ob_state:\n",
    "        segment_class.append(ob_state[ob])\n",
    "    else:\n",
    "        segment_class.append(\"Unknown\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('{}/1776_light_curves_1s_bin_errorfix.pkl'.format(data_dir), 'rb') as f:\n",
    "    lcs = pickle.load(f)\n",
    "with open('{}/1776_light_curves_1s_bin_ids_errorfix.pkl'.format(data_dir), 'rb') as f:\n",
    "    ids = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nu 38 60\n",
    "#eta 78, 85\n",
    "cluster_no = 0\n",
    "nrows = 10\n",
    "cluster_labels = ob_clusters_53_labels\n",
    "data_set = pseudo_CLR_transformed\n",
    "plt.close()\n",
    "plt.tight_layout()\n",
    "plt.rcParams['figure.figsize'] = (6.97, 8.4)\n",
    "plt.rcParams.update({'font.size': 6})\n",
    "\n",
    "for cluster_no in np.unique(cluster_labels):\n",
    "    fig, axes = plt.subplots(nrows=nrows, ncols=2)\n",
    "    plt.subplots_adjust(left=None, bottom=None, right=None, top=None, wspace=None, hspace=0.6)\n",
    "#     plt.suptitle(\"cluster {}\".format(cluster_no), y=0.9)\n",
    "    lcs_of_interest = data_set.index.values[np.where(cluster_labels == cluster_no)[0]]#\n",
    "    # lcs_of_interest = inv_ob_state[\"beta\"]#\n",
    "    for n_plot, lc_oi in enumerate(lcs_of_interest):\n",
    "        index_oi = np.where(np.array(ids) == lc_oi)[0][0]\n",
    "        axes[n_plot%nrows,0].plot(lcs[index_oi][0]-lcs[index_oi][0][0], lcs[index_oi][1], c= \"black\", linewidth=0.5)\n",
    "        axes[n_plot%nrows, 1].plot(lcs[index_oi][0]-lcs[index_oi][0][0], lcs[index_oi][1]/1000, c= \"black\", linewidth=0.5)\n",
    "        axes[n_plot%nrows,0].set_xlim([0,2000])\n",
    "        axes[n_plot%nrows,0].set_ylim([500,13000])\n",
    "        axes[n_plot%nrows,0].set_yticks([0,2000,4000,6000 , 8000])\n",
    "        axes[n_plot%nrows,0].set_yticklabels([0, \"\", \"\", \"\", 8])\n",
    "        axes[n_plot%nrows, 1].yaxis.tick_right()\n",
    "        axes[n_plot%nrows, 0].tick_params(axis=\"x\", which=\"major\", length=2, width=0.75, labelsize=6, direction=\"in\")\n",
    "        axes[n_plot%nrows, 1].tick_params(axis=\"x\", which=\"major\", length=2, width=0.75, labelsize=6, direction=\"in\")\n",
    "        plt.subplots_adjust(left=None, bottom=None, right=None, top=None, wspace=None, hspace=None)\n",
    "\n",
    "        if n_plot == 4:\n",
    "            axes[n_plot,0].set_ylabel(\"X-ray rate (kcounts/s)\")\n",
    "        elif n_plot == 9:\n",
    "            axes[n_plot,0].set_xlabel(\"Time (s)\", x=1.1)\n",
    "        if (n_plot%(nrows-1) == 0) and n_plot>0:\n",
    "            plt.savefig('figures/agglo_clusters/set53_c{}.png'.format(cluster_no), dpi=300, bbox_inches = 'tight',pad_inches = 0)\n",
    "            plt.show()\n",
    "            break\n",
    "            fig, axes = plt.subplots(nrows=nrows, ncols=1)\n",
    "            plt.subplots_adjust(left=None, bottom=None, right=None, top=None, wspace=None, hspace=None)\n",
    "    #         plt.suptitle(\"cluster {}\".format(cluster_no), y=0.9)\n",
    "        elif (n_plot == len(lcs_of_interest)-1):\n",
    "            plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_cluster = 0\n",
    "np.where(ob_clusters_55_labels==plot_cluster)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clustering = AgglomerativeClustering(n_clusters=73, linkage=method, affinity=metric).fit(data_set )\n",
    "# clustering\n",
    "\n",
    "# ob_clusters_55_labels  = clustering.labels_\n",
    "ob_clusters_53_labels = sch.fcluster(linkage, kn.knee, criterion='distance')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load observation classifications from Huppenkothen 2017\n",
    "clean_belloni = open('{}/1915Belloniclass_updated.dat'.format(data_dir))\n",
    "lines = clean_belloni.readlines()\n",
    "states = lines[0].split()\n",
    "belloni_clean = {}\n",
    "for h,l in zip(states, lines[1:]):\n",
    "    belloni_clean[h] = l.split()\n",
    "    #state: obsID1, obsID2...\n",
    "ob_state = {}\n",
    "for state, obs in belloni_clean.items():\n",
    "    if state == \"chi1\" or state == \"chi2\" or state == \"chi3\" or state == \"chi4\": state = \"chi\"\n",
    "    for ob in obs:\n",
    "        ob_state[ob] = state\n",
    "        \n",
    "# load IDs of segmented light curves: observationsID_segmentIndex\n",
    "with open('{}/468202_len128_s2_4cad_ids_errorfix.pkl'.format(data_dir), 'rb') as f:\n",
    "    seg_ids = pickle.load(f)\n",
    "\n",
    "        \n",
    "seg_ObIDs = [seg.split(\"_\")[0] for seg in seg_ids] # get rid of the within-observation segment indices and create a degenerate list of observation IDs\n",
    "\n",
    "classes = np.array([\"alpha\", \"beta\", \"gamma\", \"delta\", \"theta\", \"kappa\", \"lambda\", \"mu\", \"nu\", \"rho\", \"phi\", \"chi\", \"eta\", \"omega\"])\n",
    "scales = []\n",
    "segment_class = []\n",
    "for ob in seg_ObIDs:\n",
    "    if ob in ob_state:\n",
    "        segment_class.append(ob_state[ob])\n",
    "    else:\n",
    "        segment_class.append(\"Unknown\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_label_df = pd.DataFrame(ob_clusters_53_labels, index=pseudo_CLR_transformed.index.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(cluster_label_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "ob_clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_classification = cluster_label_df # they're in the order of pseudo_CLR_transformed.index.values\n",
    "\n",
    "ob_classes = []\n",
    "ob_clusters = []\n",
    "for ob in np.unique(seg_ObIDs):\n",
    "    ob_clusters.append(cluster_label_df.loc[ob][0])\n",
    "    if ob in ob_state:\n",
    "        ob_classes.append(ob_state[ob])\n",
    "    else:\n",
    "        ob_classes.append(\"Unknown\")\n",
    "\n",
    "Belloni_classes = np.array([\"alpha\", \"beta\", \"gamma\", \"delta\", \"theta\", \"kappa\", \"lambda\", \"mu\", \"nu\", \"rho\", \"phi\", \"chi\", \"eta\", \"omega\", \"Unknown\"])\n",
    "\n",
    "comparison_matrix = np.zeros((len(np.unique(new_classification)), len(Belloni_classes)), dtype=int)\n",
    "\n",
    "comparison_matrix_df = pd.DataFrame(comparison_matrix, columns=Belloni_classes, index=np.unique(new_classification))\n",
    "\n",
    "for n_Bc, Belloni_class in enumerate(Belloni_classes):\n",
    "    Belloni_class_indices = np.where(np.array(ob_classes) == Belloni_class)[0]\n",
    "    count_clusters_for_class = np.unique(np.take(ob_clusters, Belloni_class_indices), return_counts=True)\n",
    "    for cluster_ind, cluster in enumerate(count_clusters_for_class[0]):\n",
    "        comparison_matrix_df[Belloni_class][cluster] = count_clusters_for_class[1][cluster_ind]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_normalized_comparison_matrix_df=(comparison_matrix_df-comparison_matrix_df.min())/(comparison_matrix_df.max()-comparison_matrix_df.min())\n",
    "known_comparison_matrix_df = comparison_matrix_df.drop(columns=['Unknown']).T\n",
    "component_normalized_comparison_matrix_df = (known_comparison_matrix_df-known_comparison_matrix_df.min())/(known_comparison_matrix_df.max()-known_comparison_matrix_df.min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array([(str(x), \"\") for x in list(range(1,54,2))]).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = (6.97, 3.3)\n",
    "plt.rcParams.update({'font.size': 6})\n",
    "\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "ax = sns.heatmap(class_normalized_comparison_matrix_df.T, xticklabels=True, yticklabels=True, cmap='coolwarm')#, linewidth=0.5)\n",
    "# ax.set_xticklabels(ax.get_xmajorticklabels(), fontsize = 7)\n",
    "# plt.title(\"Agglomerative clusters in terms of classified data (class-wise normalisation)\")\n",
    "ax.set_xlabel(\"Agglomerative cluster index\")\n",
    "ax.set_ylabel(\"Class of the 14 class system\")\n",
    "ax.set_xticklabels(np.array([(str(x), \"\") for x in list(range(1,54,2))]).flatten()[:-1])\n",
    "\n",
    "plt.savefig('figures/Agglo53vsBelloni.png', dpi=300, bbox_inches = 'tight',pad_inches = 0, transparent=True)\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids_ar = np.array(segment_class)\n",
    "\n",
    "# class_names = list(inv_ob_state.keys())\n",
    "\n",
    "\n",
    "alpha = np.where(ids_ar == \"alpha\")[0][1]#1\n",
    "beta= np.where(ids_ar == \"beta\")[0][7]#7\n",
    "gamma=np.where(ids_ar == \"gamma\")[0][4]\n",
    "delta=np.where(ids_ar == \"delta\")[0][5]\n",
    "theta=np.where(ids_ar == \"theta\")[0][4]#4\n",
    "kappa=np.where(ids_ar == \"kappa\")[0][3]\n",
    "lambda1=np.where(ids_ar == \"lambda\")[0][2]\n",
    "mu=np.where(ids_ar == \"mu\")[0][1]\n",
    "nu=np.where(ids_ar == \"nu\")[0][1]\n",
    "rho=np.where(ids_ar == \"rho\")[0][1]\n",
    "phi=np.where(ids_ar == \"phi\")[0][1]\n",
    "chi=np.where(ids_ar == \"chi\")[0][1]\n",
    "eta=np.where(ids_ar == \"eta\")[0][4]\n",
    "omega=np.where(ids_ar == \"omega\")[0][3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "desc_stats[delta,2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(nrows=2, ncols=2)\n",
    "axes = axes.flatten()\n",
    "\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (3.32, 3.32*(1/1))\n",
    "plt.rcParams.update({'font.size': 6})\n",
    "\n",
    "from matplotlib.ticker import (MultipleLocator, FormatStrFormatter,\n",
    "                               AutoMinorLocator)\n",
    "\n",
    "stat_names = np.array([\"mean\", \"st.d.\", \"skew\", \"kurtosis\"])\n",
    "\n",
    "x_tick_space = [2500, 1000, 1, 2]\n",
    "x_limits = [[0,8000],[0,2500], [-2,3], [-2,8]]\n",
    "\n",
    "for plot_ind in range(4):\n",
    "#     light_c = selected_lcs[plot_ind]\n",
    "    axes[plot_ind].hist(desc_stats[:,plot_ind], bins=50, range = x_limits[plot_ind],zorder=-4)\n",
    "    y_vals = axes[plot_ind].get_yticks()\n",
    "    axes[plot_ind].set_yticklabels(['{:3.0f}'.format(x / 10000) for x in y_vals])\n",
    "    axes[plot_ind].xaxis.set_major_locator(MultipleLocator(x_tick_space[plot_ind]))\n",
    "    axes[plot_ind].xaxis.set_major_formatter(FormatStrFormatter('%d'))\n",
    "    axes[plot_ind].xaxis.set_minor_locator(AutoMinorLocator())\n",
    "    axes[plot_ind].text(0.99,0.99,\"{}\".format(stat_names[plot_ind]), ha='right', va='top', transform=axes[plot_ind].transAxes, size=6)\n",
    "    axes[plot_ind].axvline(desc_stats[chi,plot_ind], c=\"cyan\", zorder=-3, alpha=0.5)\n",
    "    axes[plot_ind].axvline(desc_stats[phi,plot_ind], c=\"magenta\", zorder=-3, alpha=0.5)\n",
    "    axes[plot_ind].axvline(desc_stats[gamma,plot_ind], c=\"black\", zorder=-3, alpha=0.5)\n",
    "    if plot_ind == 2:\n",
    "        axes[plot_ind].set_ylabel(\"No. segments ($10^4$)\", y=1, size=6)\n",
    "        axes[plot_ind].set_xlabel(\"Value of the statistic\", x=1, size=6)\n",
    "\n",
    "axes.reshape((2,2))\n",
    "plt.savefig('figures/stat_distributions.png', dpi=300, bbox_inches = 'tight',pad_inches = 0)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(desc_stats[:,3], bins=20)#, bins=50, range=[-2,48])\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('{}/1776_light_curves_1s_bin_errorfix.pkl'.format(data_dir), 'rb') as f:\n",
    "    lcs = pickle.load(f)\n",
    "with open('{}/1776_light_curves_1s_bin_ids_errorfix.pkl'.format(data_dir), 'rb') as f:\n",
    "    ids = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group top 5 sentences under each topic\n",
    "sent_topics_sorteddf_mallet = pd.DataFrame()\n",
    "\n",
    "sent_topics_outdf_grpd = df_topic_sents_keywords.groupby('Dominant_Topic')\n",
    "\n",
    "for i, grp in sent_topics_outdf_grpd:\n",
    "    sent_topics_sorteddf_mallet = pd.concat([sent_topics_sorteddf_mallet, \n",
    "                                             grp.sort_values(['Perc_Contribution'], ascending=[0]).head(1)], \n",
    "                                            axis=0)\n",
    "\n",
    "# Reset Index    \n",
    "sent_topics_sorteddf_mallet.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Format\n",
    "sent_topics_sorteddf_mallet.columns = ['Topic_Num', \"Topic_Perc_Contrib\", \"Keywords\", \"Text\"]\n",
    "\n",
    "# Show\n",
    "sent_topics_sorteddf_mallet.head()fig, axes = plt.subplots(nrows=5, ncols=2)\n",
    "axes = axes.flatten()\n",
    "\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (20,10)\n",
    "plt.rcParams.update({'font.size': 16})\n",
    "\n",
    "from matplotlib.ticker import (MultipleLocator, FormatStrFormatter,\n",
    "                               AutoMinorLocator)\n",
    "\n",
    "stat_names = np.array([\"mean\", \"st.d.\", \"skew\", \"kurtosis\"])\n",
    "\n",
    "x_tick_space = [2500, 1000, 1, 2]\n",
    "x_limits = [[0,8000],[0,2500], [-2,3], [-2,8]]\n",
    "\n",
    "for plot_ind in range(10):\n",
    "#     light_c = selected_lcs[plot_ind]\n",
    "    axes[plot_ind].plot(lcs[plot_ind][0]-lcs[plot_ind][0][0], lcs[plot_ind][1]/1000, c=\"black\")\n",
    "    axes[plot_ind].set_ylim([1.2,10])\n",
    "    axes[plot_ind].set_xlim([0,4000])\n",
    "    axes[plot_ind].set_yticks([0,4,8])\n",
    "    axes[plot_ind].set_yticklabels([0,4,8])\n",
    "#     .errorbar(lcs[plot_ind][0]-lcs[plot_ind][0][0], lcs[plot_ind][1], yerr=lcs[plot_ind][2], ecolor=\"black\")\n",
    "#     .hist(desc_stats[:,plot_ind], bins=50, range = x_limits[plot_ind],zorder=-4)\n",
    "#     y_vals = axes[plot_ind].get_yticks()\n",
    "#     axes[plot_ind].set_yticklabels(['{:3.0f}'.format(x / 10000) for x in y_vals])\n",
    "#     axes[plot_ind].xaxis.set_major_locator(MultipleLocator(x_tick_space[plot_ind]))\n",
    "#     axes[plot_ind].xaxis.set_major_formatter(FormatStrFormatter('%d'))\n",
    "#     axes[plot_ind].xaxis.set_minor_locator(AutoMinorLocator())\n",
    "#     axes[plot_ind].text(0.99,0.99,\"{}\".format(stat_names[plot_ind]), ha='right', va='top', transform=axes[plot_ind].transAxes, size=6)\n",
    "#     axes[plot_ind].axvline(desc_stats[chi,plot_ind], c=\"cyan\", zorder=-3, alpha=0.5)\n",
    "#     axes[plot_ind].axvline(desc_stats[phi,plot_ind], c=\"magenta\", zorder=-3, alpha=0.5)\n",
    "#     axes[plot_ind].axvline(desc_stats[gamma,plot_ind], c=\"black\", zorder=-3, alpha=0.5)\n",
    "#     if plot_ind == 2:\n",
    "#         axes[plot_ind].set_ylabel(\"No. segments ($10^4$)\", y=1, size=6)\n",
    "#         axes[plot_ind].set_xlabel(\"Value of the statistic\", x=1, size=6)\n",
    "    if plot_ind == 8:\n",
    "        axes[plot_ind].set_ylabel(\"X-ray rate (kcounts/s)\", y=3.5, fontsize=20)\n",
    "        axes[plot_ind].set_xlabel(\"Time (s)\", x=1, fontsize=20)\n",
    "\n",
    "axes.reshape((5,2))\n",
    "plt.subplots_adjust(left=None, bottom=None, right=None, top=None, wspace=None, hspace=0.5)\n",
    "plt.savefig('figures/example_light_curves.png', dpi=300, bbox_inches = 'tight',pad_inches = 0, transparent=True)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "\n",
    "print(__doc__)\n",
    "\n",
    "np.random.seed(42)\n",
    "plt.figure(figsize=(8, 8))\n",
    "\n",
    "\n",
    "# Generate train data\n",
    "X_inliers = 0.3 * np.random.randn(100, 2)\n",
    "X_inliers = np.r_[X_inliers + 2, X_inliers - 2]\n",
    "\n",
    "# Generate some outliers\n",
    "X_outliers = np.random.uniform(low=-4, high=4, size=(20, 2))\n",
    "X = np.r_[X_inliers, X_outliers]\n",
    "\n",
    "n_outliers = len(X_outliers)\n",
    "ground_truth = np.ones(len(X), dtype=int)\n",
    "ground_truth[-n_outliers:] = -1\n",
    "\n",
    "# fit the model for outlier detection (default)\n",
    "clf = LocalOutlierFactor(n_neighbors=20, contamination=0.1)\n",
    "# use fit_predict to compute the predicted labels of the training samples\n",
    "# (when LOF is used for outlier detection, the estimator has no predict,\n",
    "# decision_function and score_samples methods).\n",
    "y_pred = clf.fit_predict(X)\n",
    "n_errors = (y_pred != ground_truth).sum()\n",
    "X_scores = clf.negative_outlier_factor_\n",
    "\n",
    "# plt.title(\"Local Outlier Factor (LOF)\")\n",
    "plt.scatter(X[:, 0], X[:, 1], color='k', s=10., label='Data points')\n",
    "# plot circles with radius proportional to the outlier scores\n",
    "radius = (X_scores.max() - X_scores) / (X_scores.max() - X_scores.min())\n",
    "# plt.scatter(X[:, 0], X[:, 1], s=1000 * radius, edgecolors='r',\n",
    "#             facecolors='none', label='Outlier scores')\n",
    "\n",
    "gmm = mixture.GaussianMixture(n_components=2,\n",
    "                              covariance_type=\"full\")\n",
    "gmm.fit(X)\n",
    "\n",
    "\n",
    "splot = plt.subplot(1, 1, 1)\n",
    "Y_ = gmm.predict(X)\n",
    "for i, (mean, cov, color) in enumerate(zip(gmm.means_, gmm.covariances_,\n",
    "                                           color_iter)):\n",
    "    v, w = linalg.eigh(cov)\n",
    "    if not np.any(Y_ == i):\n",
    "        continue\n",
    "    plt.scatter(X[Y_ == i, 0], X[Y_ == i, 1], 2.8, color=color)\n",
    "\n",
    "    # Plot an ellipse to show the Gaussian component\n",
    "    angle = np.arctan2(w[0][1], w[0][0])\n",
    "    angle = 180. * angle / np.pi  # convert to degrees\n",
    "    v = 2. * np.sqrt(2.) * np.sqrt(v)\n",
    "    ell = mpl.patches.Ellipse(mean, v[0], v[1], 180. + angle, color=color)\n",
    "    ell.set_clip_box(splot.bbox)\n",
    "    ell.set_alpha(.5)\n",
    "    splot.add_artist(ell)\n",
    "\n",
    "\n",
    "\n",
    "# plt.axis('tight')\n",
    "# plt.xlim((-5, 5))\n",
    "# plt.ylim((-5, 5))\n",
    "# plt.xlabel(\"prediction errors: %d\" % (n_errors))\n",
    "# legend = plt.legend(loc='upper left')\n",
    "# legend.legendHandles[0]._sizes = [10]\n",
    "# legend.legendHandles[1]._sizes = [20]\n",
    "plt.savefig('figures/example_clusters.png', dpi=300, bbox_inches = 'tight',pad_inches = 0, transparent=True)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import itertools\n",
    "\n",
    "from scipy import linalg\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "\n",
    "from sklearn import mixture\n",
    "\n",
    "print(__doc__)\n",
    "\n",
    "# Number of samples per component\n",
    "n_samples = 500\n",
    "\n",
    "# Generate random sample, two components\n",
    "np.random.seed(0)\n",
    "# C = np.array([[0., -0.1], [1.7, .4]])\n",
    "# X = np.r_[np.dot(np.random.randn(n_samples, 2), C),\n",
    "#           .7 * np.random.randn(n_samples, 2) + np.array([-6, 3])]\n",
    "\n",
    "lowest_bic = np.infty\n",
    "bic = []\n",
    "n_components_range = range(1, 7)\n",
    "cv_types = ['spherical', 'tied', 'diag', 'full']\n",
    "for cv_type in cv_types:\n",
    "    for n_components in n_components_range:\n",
    "        # Fit a Gaussian mixture with EM\n",
    "        gmm = mixture.GaussianMixture(n_components=n_components,\n",
    "                                      covariance_type=cv_type)\n",
    "        gmm.fit(X)\n",
    "        bic.append(gmm.bic(X))\n",
    "        if bic[-1] < lowest_bic:\n",
    "            lowest_bic = bic[-1]\n",
    "            best_gmm = gmm\n",
    "\n",
    "bic = np.array(bic)\n",
    "color_iter = itertools.cycle(['navy', 'turquoise', 'cornflowerblue',\n",
    "                              'darkorange'])\n",
    "clf = best_gmm\n",
    "bars = []\n",
    "\n",
    "# Plot the BIC scores\n",
    "plt.figure(figsize=(8, 6))\n",
    "spl = plt.subplot(2, 1, 1)\n",
    "for i, (cv_type, color) in enumerate(zip(cv_types, color_iter)):\n",
    "    xpos = np.array(n_components_range) + .2 * (i - 2)\n",
    "    bars.append(plt.bar(xpos, bic[i * len(n_components_range):\n",
    "                                  (i + 1) * len(n_components_range)],\n",
    "                        width=.2, color=color))\n",
    "plt.xticks(n_components_range)\n",
    "plt.ylim([bic.min() * 1.01 - .01 * bic.max(), bic.max()])\n",
    "plt.title('BIC score per model')\n",
    "xpos = np.mod(bic.argmin(), len(n_components_range)) + .65 +\\\n",
    "    .2 * np.floor(bic.argmin() / len(n_components_range))\n",
    "plt.text(xpos, bic.min() * 0.97 + .03 * bic.max(), '*', fontsize=14)\n",
    "spl.set_xlabel('Number of components')\n",
    "spl.legend([b[0] for b in bars], cv_types)\n",
    "\n",
    "# Plot the winner\n",
    "splot = plt.subplot(2, 1, 2)\n",
    "Y_ = clf.predict(X)\n",
    "for i, (mean, cov, color) in enumerate(zip(clf.means_, clf.covariances_,\n",
    "                                           color_iter)):\n",
    "    v, w = linalg.eigh(cov)\n",
    "    if not np.any(Y_ == i):\n",
    "        continue\n",
    "    plt.scatter(X[Y_ == i, 0], X[Y_ == i, 1], .8, color=color)\n",
    "\n",
    "    # Plot an ellipse to show the Gaussian component\n",
    "    angle = np.arctan2(w[0][1], w[0][0])\n",
    "    angle = 180. * angle / np.pi  # convert to degrees\n",
    "    v = 2. * np.sqrt(2.) * np.sqrt(v)\n",
    "    ell = mpl.patches.Ellipse(mean, v[0], v[1], 180. + angle, color=color)\n",
    "    ell.set_clip_box(splot.bbox)\n",
    "    ell.set_alpha(.5)\n",
    "    splot.add_artist(ell)\n",
    "\n",
    "plt.xticks(())\n",
    "plt.yticks(())\n",
    "plt.title('Selected GMM: full model, 2 components')\n",
    "plt.subplots_adjust(hspace=.35, bottom=.02)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot\n",
    "from math import cos, sin, atan\n",
    "\n",
    "\n",
    "class Neuron():\n",
    "    def __init__(self, x, y):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "\n",
    "    def draw(self, neuron_radius):\n",
    "        circle = pyplot.Circle((self.x, self.y), radius=neuron_radius, fill=False)\n",
    "        pyplot.gca().add_patch(circle)\n",
    "\n",
    "\n",
    "class Layer():\n",
    "    def __init__(self, network, number_of_neurons, number_of_neurons_in_widest_layer):\n",
    "        self.vertical_distance_between_layers = 6\n",
    "        self.horizontal_distance_between_neurons = 2\n",
    "        self.neuron_radius = 0.5\n",
    "        self.number_of_neurons_in_widest_layer = number_of_neurons_in_widest_layer\n",
    "        self.previous_layer = self.__get_previous_layer(network)\n",
    "        self.y = self.__calculate_layer_y_position()\n",
    "        self.neurons = self.__intialise_neurons(number_of_neurons)\n",
    "\n",
    "    def __intialise_neurons(self, number_of_neurons):\n",
    "        neurons = []\n",
    "        x = self.__calculate_left_margin_so_layer_is_centered(number_of_neurons)\n",
    "        for iteration in range(number_of_neurons):\n",
    "            neuron = Neuron(x, self.y)\n",
    "            neurons.append(neuron)\n",
    "            x += self.horizontal_distance_between_neurons\n",
    "        return neurons\n",
    "\n",
    "    def __calculate_left_margin_so_layer_is_centered(self, number_of_neurons):\n",
    "        return self.horizontal_distance_between_neurons * (self.number_of_neurons_in_widest_layer - number_of_neurons) / 2\n",
    "\n",
    "    def __calculate_layer_y_position(self):\n",
    "        if self.previous_layer:\n",
    "            return self.previous_layer.y + self.vertical_distance_between_layers\n",
    "        else:\n",
    "            return 0\n",
    "\n",
    "    def __get_previous_layer(self, network):\n",
    "        if len(network.layers) > 0:\n",
    "            return network.layers[-1]\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    def __line_between_two_neurons(self, neuron1, neuron2):\n",
    "        angle = atan((neuron2.x - neuron1.x) / float(neuron2.y - neuron1.y))\n",
    "        x_adjustment = self.neuron_radius * sin(angle)\n",
    "        y_adjustment = self.neuron_radius * cos(angle)\n",
    "        line = pyplot.Line2D((neuron1.x - x_adjustment, neuron2.x + x_adjustment), (neuron1.y - y_adjustment, neuron2.y + y_adjustment))\n",
    "        pyplot.gca().add_line(line)\n",
    "\n",
    "    def draw(self, layerType=0):\n",
    "        for neuron in self.neurons:\n",
    "            neuron.draw( self.neuron_radius )\n",
    "            if self.previous_layer:\n",
    "                for previous_layer_neuron in self.previous_layer.neurons:\n",
    "                    self.__line_between_two_neurons(neuron, previous_layer_neuron)\n",
    "        # write Text\n",
    "#         x_text = self.number_of_neurons_in_widest_layer * self.horizontal_distance_between_neurons\n",
    "#         if layerType == 0:\n",
    "#             pyplot.text(x_text, self.y, 'Input Layer', fontsize = 12)\n",
    "#         elif layerType == -1:\n",
    "#             pyplot.text(x_text, self.y, 'Output Layer', fontsize = 12)\n",
    "#         else:\n",
    "#             pyplot.text(x_text, self.y, 'Hidden Layer '+str(layerType), fontsize = 12)\n",
    "\n",
    "class NeuralNetwork():\n",
    "    def __init__(self, number_of_neurons_in_widest_layer):\n",
    "        self.number_of_neurons_in_widest_layer = number_of_neurons_in_widest_layer\n",
    "        self.layers = []\n",
    "        self.layertype = 0\n",
    "\n",
    "    def add_layer(self, number_of_neurons ):\n",
    "        layer = Layer(self, number_of_neurons, self.number_of_neurons_in_widest_layer)\n",
    "        self.layers.append(layer)\n",
    "\n",
    "    def draw(self):\n",
    "        pyplot.figure()\n",
    "        for i in range( len(self.layers) ):\n",
    "            layer = self.layers[i]\n",
    "            if i == len(self.layers)-1:\n",
    "                i = -1\n",
    "            layer.draw( i )\n",
    "        pyplot.axis('scaled')\n",
    "        pyplot.axis('off')\n",
    "#         pyplot.title( 'Neural Network architecture', fontsize=15 )\n",
    "        pyplot.savefig('figures/neural_net.png', dpi=300, bbox_inches = 'tight',pad_inches = 0, transparent=True)\n",
    "        pyplot.show()\n",
    "\n",
    "class DrawNN():\n",
    "    def __init__( self, neural_network ):\n",
    "        self.neural_network = neural_network\n",
    "\n",
    "    def draw( self ):\n",
    "        widest_layer = max( self.neural_network )\n",
    "        network = NeuralNetwork( widest_layer )\n",
    "        for l in self.neural_network:\n",
    "            network.add_layer(l)\n",
    "        network.draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network = DrawNN( [12,8,4] )\n",
    "network.draw()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('../../../data_GRS1915/1776_light_curves_1s_bin_errorfix.pkl', 'rb') as f:\n",
    "#     lcs = pickle.load(f)\n",
    "# with open('../../../data_GRS1915/1776_light_curves_1s_bin_ids_errorfix.pkl', 'rb') as f:\n",
    "#     ids = pickle.load(f)\n",
    "    \n",
    "    \n",
    "clean_belloni = open('{}/1915Belloniclass_updated.dat'.format(data_dir))\n",
    "lines = clean_belloni.readlines()\n",
    "states = lines[0].split()\n",
    "belloni_clean = {}\n",
    "for h,l in zip(states, lines[1:]):\n",
    "    belloni_clean[h] = l.split()\n",
    "    #state: obsID1, obsID2...\n",
    "ob_state = {}\n",
    "for state, obs in belloni_clean.items():\n",
    "    if state == \"chi1\" or state == \"chi2\" or state == \"chi3\" or state == \"chi4\": state = \"chi\"\n",
    "    for ob in obs:\n",
    "        ob_state[ob] = state\n",
    "\n",
    "        \n",
    "\n",
    "inv_ob_state = {}\n",
    "for k, v in ob_state.items():\n",
    "    inv_ob_state[v] = inv_ob_state.get(v, [])\n",
    "    inv_ob_state[v].append(k)\n",
    "\n",
    "\n",
    "import matplotlib.pylab as pylab\n",
    "import matplotlib.ticker as ticker\n",
    "#https://stackoverflow.com/questions/8389636/creating-over-20-unique-legend-colors-using-matplotlib\n",
    "\n",
    "\n",
    "pylab.rcParams['figure.figsize'] = (29.7, 21.0) # A4 size 210mm x 297mm\n",
    "import matplotlib\n",
    "colors = matplotlib.colors.CSS4_COLORS.keys()\n",
    "colors = np.array(list(colors))\n",
    "\n",
    "\n",
    "NUM_COLORS = 14\n",
    "cm = plt.get_cmap(\"jet\")#('gist_rainbow')\n",
    "colors = [cm(1.*i/NUM_COLORS) for i in range(NUM_COLORS)]\n",
    "\n",
    "ids_ar = np.array(ids)\n",
    "\n",
    "class_names = list(inv_ob_state.keys())\n",
    "\n",
    "\n",
    "alpha = lcs[np.where(ids_ar == inv_ob_state[\"alpha\"][0])[0][0]]\n",
    "beta= lcs[np.where(ids_ar == inv_ob_state[\"beta\"][5])[0][0]] #3\n",
    "gamma=lcs[np.where(ids_ar == inv_ob_state[\"gamma\"][0])[0][0]]\n",
    "delta=lcs[np.where(ids_ar == inv_ob_state[\"delta\"][9])[0][0]]\n",
    "theta=lcs[np.where(ids_ar == inv_ob_state[\"theta\"][13])[0][0]]#11\n",
    "kappa=lcs[np.where(ids_ar == inv_ob_state[\"kappa\"][6])[0][0]]#6\n",
    "lambda1=lcs[np.where(ids_ar == inv_ob_state[\"lambda\"][3])[0][0]] #3\n",
    "mu=lcs[np.where(ids_ar == inv_ob_state[\"mu\"][6])[0][0]]#6\n",
    "nu=lcs[np.where(ids_ar == inv_ob_state[\"nu\"][0])[0][0]]\n",
    "rho=lcs[np.where(ids_ar == inv_ob_state[\"rho\"][9])[0][0]]#9\n",
    "phi=lcs[np.where(ids_ar == inv_ob_state[\"phi\"][3])[0][0]]# 3,6\n",
    "chi=lcs[np.where(ids_ar == inv_ob_state[\"chi\"][27])[0][0]]# 1,17,27\n",
    "eta=lcs[np.where(ids_ar == inv_ob_state[\"eta\"][2])[0][0]]# 1\n",
    "# omega=lcs[np.where(ids_ar == inv_ob_state[\"kappa\"][-3])[0][0]]\n",
    "omega=lcs[np.where(ids_ar == inv_ob_state[\"omega\"][1])[0][0]]\n",
    "\n",
    "\n",
    "selected_lcs = [alpha,beta,gamma,delta,theta,kappa,lambda1,mu,nu,rho,phi,chi,eta,omega]\n",
    "\n",
    "\n",
    "fig, axes = plt.subplots(nrows=7, ncols=2)\n",
    "axes = axes.flatten()\n",
    "\n",
    "plt.subplots_adjust(hspace=0.05)\n",
    "plt.subplots_adjust(wspace=0.01)\n",
    "\n",
    "good_classes = [\"delta\", \"mu\", \"rho\", \"phi\"]\n",
    "intervals = {}\n",
    "\n",
    "colors = [\"black\"]*14\n",
    "\n",
    "for plot_ind in range(14):\n",
    "    light_c = np.copy(selected_lcs[plot_ind])\n",
    "    class_name = class_names[plot_ind]\n",
    "    offset = light_c[0][0]\n",
    "    axes[plot_ind].set_ylim([-0.1, 1.1])\n",
    "    \n",
    "    if class_name == \"alpha\":\n",
    "        breaks = np.where((light_c[0][1:]-light_c[0][:-1]) != 1.)[0]+1\n",
    "        start=0\n",
    "        end =breaks[0]\n",
    "        light_c[1] /= np.max(light_c[1][start:end])*1.1\n",
    "        axes[plot_ind].plot(light_c[0][:breaks[0]]-offset, light_c[1][:breaks[0]], c=colors[plot_ind], linewidth=1, zorder=-5)\n",
    "        axes[plot_ind].set_xlim([0, 3500])\n",
    "        axes[plot_ind].text(1,1,r\"$\\{}$\".format(class_name), ha='right', va='top', transform=axes[plot_ind].transAxes, size=30)\n",
    "\n",
    "    elif class_name == \"beta\":\n",
    "        breaks = np.where((light_c[0][1:]-light_c[0][:-1]) != 1.)[0]+1 # [ 279 3584 6652]\n",
    "        start=breaks[0]\n",
    "        end =breaks[1]\n",
    "        offset = light_c[0][start]\n",
    "        light_c[1] /= np.max(light_c[1][start:end])*1.1\n",
    "        axes[plot_ind].plot(light_c[0][start:end]-offset, light_c[1][start:end], c=colors[plot_ind], linewidth=1, zorder=-5)\n",
    "        axes[plot_ind].text(1,1,r\"$\\{}$\".format(class_name), ha='right', va='top', transform=axes[plot_ind].transAxes, size=30)\n",
    "        axes[plot_ind].set_xlim([0, 3500])\n",
    "        \n",
    "    elif class_name == \"gamma\":\n",
    "        breaks = np.where((light_c[0][1:]-light_c[0][:-1]) != 1.)[0]+1 # [ 279 3584 6652]\n",
    "        start=breaks[0]\n",
    "        end =breaks[1]\n",
    "        offset = light_c[0][start]\n",
    "        light_c[1] /= np.max(light_c[1][start:end])*1.1\n",
    "        axes[plot_ind].plot(light_c[0][start:end]-offset, light_c[1][start:end], c=colors[plot_ind], linewidth=1, zorder=-5)\n",
    "        axes[plot_ind].text(1,1,r\"$\\{}$\".format(class_name), ha='right', va='top', transform=axes[plot_ind].transAxes, size=30)\n",
    "        axes[plot_ind].set_xlim([0, 3500])\n",
    "        \n",
    "    elif class_name == \"theta\":\n",
    "        breaks = np.where((light_c[0][1:]-light_c[0][:-1]) != 1.)[0]+1 # [ 279 3584 6652]\n",
    "        start=breaks[1]\n",
    "        end =breaks[2]\n",
    "        offset = light_c[0][start]\n",
    "        light_c[1] /= np.max(light_c[1][start:end])*1.1\n",
    "        axes[plot_ind].plot(light_c[0][start:end]-offset, light_c[1][start:end], c=colors[plot_ind], linewidth=1, zorder=-5)\n",
    "        axes[plot_ind].text(1,1,r\"$\\{}$\".format(class_name), ha='right', va='top', transform=axes[plot_ind].transAxes, size=30)\n",
    "        axes[plot_ind].set_xlim([0, 3500])\n",
    "        \n",
    "    elif class_name == \"kappa\":\n",
    "        breaks = np.where((light_c[0][1:]-light_c[0][:-1]) != 1.)[0]+1 # [ 279 3584 6652]\n",
    "        start=breaks[-1]\n",
    "        end =-1\n",
    "        offset = light_c[0][start]\n",
    "        light_c[1] /= np.max(light_c[1][start:end])*1.1\n",
    "        axes[plot_ind].plot(light_c[0][start:end]-offset, light_c[1][start:end], c=colors[plot_ind], linewidth=1, zorder=-5)\n",
    "        axes[plot_ind].text(1,1,r\"$\\{}$\".format(class_name), ha='right', va='top', transform=axes[plot_ind].transAxes, size=30)\n",
    "        axes[plot_ind].set_xlim([0, 3500])\n",
    "        \n",
    "    elif class_name == \"lambda\":\n",
    "        breaks = np.where((light_c[0][1:]-light_c[0][:-1]) != 1.)[0]+1 # [ 279 3584 6652]\n",
    "        start=breaks[-1]\n",
    "        end =-1\n",
    "        offset = light_c[0][start]\n",
    "        light_c[1] /= np.max(light_c[1][start:end])*1.1\n",
    "        axes[plot_ind].plot(light_c[0][start:end]-offset, light_c[1][start:end], c=colors[plot_ind], linewidth=1, zorder=-5)\n",
    "        axes[plot_ind].text(1,1,r\"$\\{}$\".format(class_name), ha='right', va='top', transform=axes[plot_ind].transAxes, size=30)\n",
    "        axes[plot_ind].set_xlim([0, 3500])\n",
    "        \n",
    "    elif class_name == \"nu\":\n",
    "        breaks = np.where((light_c[0][1:]-light_c[0][:-1]) != 1.)[0]+1 # [ 279 3584 6652]\n",
    "        start= breaks[0]\n",
    "        end =breaks[1]\n",
    "        offset = light_c[0][start]\n",
    "        light_c[1] /= np.max(light_c[1][start:end])*1.1\n",
    "        axes[plot_ind].plot(light_c[0][start:end]-offset, light_c[1][start:end], c=colors[plot_ind], linewidth=1, zorder=-5)\n",
    "        axes[plot_ind].text(1,1,r\"$\\{}$\".format(class_name), ha='right', va='top', transform=axes[plot_ind].transAxes, size=30)\n",
    "        axes[plot_ind].set_xlim([0, 3500])\n",
    "        \n",
    "    elif class_name == \"chi\":\n",
    "        breaks = np.where((light_c[0][1:]-light_c[0][:-1]) != 1.)[0]+1 # [ 279 3584 6652]\n",
    "        start= breaks[0]\n",
    "        end =-1\n",
    "        offset = light_c[0][start]\n",
    "        light_c[1] /= np.max(light_c[1][start:end])*1.1\n",
    "        axes[plot_ind].plot(light_c[0][start:end]-offset, light_c[1][start:end], c=colors[plot_ind], linewidth=1, zorder=-5)\n",
    "        axes[plot_ind].text(1,1,r\"$\\{}$\".format(class_name), ha='right', va='top', transform=axes[plot_ind].transAxes, size=30)\n",
    "        axes[plot_ind].set_xlim([0, 3500])\n",
    "        \n",
    "    elif class_name == \"eta\":\n",
    "        breaks = np.where((light_c[0][1:]-light_c[0][:-1]) != 1.)[0]+1 # [ 279 3584 6652]\n",
    "        start= breaks[2]\n",
    "        end =breaks[3]\n",
    "        offset = light_c[0][start]\n",
    "        light_c[1] /= np.max(light_c[1][start:end])*1.1\n",
    "        axes[plot_ind].plot(light_c[0][start:end]-offset, light_c[1][start:end], c=colors[plot_ind], linewidth=1, zorder=-5)\n",
    "        axes[plot_ind].text(1,1,r\"$\\{}$\".format(class_name), ha='right', va='top', transform=axes[plot_ind].transAxes, size=30)\n",
    "        axes[plot_ind].set_xlim([0, 3500])\n",
    "        \n",
    "    elif class_name == \"omega\":\n",
    "        breaks = np.where((light_c[0][1:]-light_c[0][:-1]) != 1.)[0]+1 # [ 279 3584 6652]\n",
    "        start= breaks[0]\n",
    "        end =-1\n",
    "        offset = light_c[0][start]\n",
    "        light_c[1] /= np.max(light_c[1][start:end])*1.1\n",
    "        axes[plot_ind].plot(light_c[0][start:end]-offset, light_c[1][start:end], c=colors[plot_ind], linewidth=1, zorder=-5)\n",
    "        axes[plot_ind].plot(light_c[0][start:end-48]-offset+1607, light_c[1][start+48:end], c=colors[plot_ind], linewidth=1, zorder=-5)\n",
    "        axes[plot_ind].text(1,1,r\"$\\{}$\".format(class_name), ha='right', va='top', transform=axes[plot_ind].transAxes, size=30)\n",
    "        axes[plot_ind].set_xlim([0, 3500])\n",
    "        \n",
    "    elif class_name == \"rho\":\n",
    "        light_c[1] /= np.max(light_c[1])*1.1\n",
    "        axes[plot_ind].plot(light_c[0]-offset, light_c[1], c=colors[plot_ind], linewidth=1, zorder=-5)\n",
    "        axes[plot_ind].text(1,1,r\"$\\{}$\".format(class_name), ha='right', va='top', transform=axes[plot_ind].transAxes, size=30)\n",
    "        axes[plot_ind].set_xlim([0, 3500])\n",
    "        \n",
    "        \n",
    "    elif class_name in good_classes:\n",
    "        \n",
    "        light_c[1] /= np.max(light_c[1])*1.1\n",
    "        axes[plot_ind].plot(light_c[0]-offset, light_c[1], c=colors[plot_ind], linewidth=1, zorder=-5)\n",
    "        axes[plot_ind].text(1,1,r\"$\\{}$\".format(class_name), ha='right', va='top', transform=axes[plot_ind].transAxes, size=30)\n",
    "        axes[plot_ind].set_xlim([0, 3500])\n",
    "    else:\n",
    "        axes[plot_ind].plot(light_c[0]-offset, light_c[1])\n",
    "        axes[plot_ind].plot(light_c[0][:3500]-offset, light_c[1][:3500])\n",
    "    \n",
    "    axes[plot_ind].set_xlim([0, 2500])\n",
    "#     axes[plot_ind].tick_params(axis=\"x\", which=\"major\", length=5, width=1, labelsize=20, direction=\"in\")\n",
    "    \n",
    "    if plot_ind%2 == 0:\n",
    "        axes[plot_ind].tick_params(axis=\"y\", which=\"major\", length=5, width=1, labelsize=20, direction=\"in\")\n",
    "    else:\n",
    "        axes[plot_ind].tick_params(axis=\"y\", which=\"major\", length=5, width=1, labelsize=0, direction=\"in\")\n",
    "        plt.setp(axes[plot_ind].get_yticklabels(), visible=False)\n",
    "    if plot_ind == 6:\n",
    "        axes[plot_ind].set_ylabel(\"Normalised count rate\", size=30)\n",
    "    if plot_ind == 12:\n",
    "        axes[plot_ind].tick_params(axis=\"x\", which=\"major\", length=5, width=1, labelsize=20, direction=\"in\")\n",
    "        axes[plot_ind].set_xlabel(\"Time (s)\", x=1, size=30)\n",
    "\n",
    "    else:\n",
    "        axes[plot_ind].tick_params(axis=\"x\", which=\"major\", length=5, width=1, labelsize=0, direction=\"in\")\n",
    "        plt.setp(axes[plot_ind].get_xticklabels(), visible=False)\n",
    "    \n",
    "    axes[plot_ind].set_yticks([0, 0.25, 0.5, 0.75, 1.0])\n",
    "    axes[plot_ind].set_yticklabels([\"0\",\"\",\"\",\"\", \"1\"])\n",
    "    axes[plot_ind].set_xticks([0, 500, 1000,1500, 2000])\n",
    "    axes[plot_ind].set_xticklabels([0,\"\",1000,\"\", 2000])\n",
    "\n",
    "    \n",
    "axes.reshape((7,2))\n",
    "\n",
    "\n",
    "# axes[0][0].tick_params(axis=\"x\", which=\"major\", length=5, width=1, labelsize=5, direction=\"in\")\n",
    "# axes[0][1].tick_params(axis=\"x\", which=\"major\", length=5, width=1, labelsize=5, direction=\"in\")\n",
    "# axes[1][0].tick_params(axis=\"x\", which=\"major\", length=5, width=1, labelsize=25, direction=\"in\")\n",
    "# axes[1][1].tick_params(axis=\"x\", which=\"major\", length=5, width=1, labelsize=25, direction=\"in\")\n",
    "\n",
    "# axes[0][0].tick_params(axis=\"y\", which=\"major\", length=5, width=1, labelsize=25, direction=\"in\")\n",
    "# axes[1][0].tick_params(axis=\"y\", which=\"major\", length=5, width=1, labelsize=25, direction=\"in\")\n",
    "# axes[0][1].tick_params(axis=\"y\", which=\"major\", length=5, width=1, labelsize=5, direction=\"in\")\n",
    "# axes[1][1].tick_params(axis=\"y\", which=\"major\", length=5, width=1, labelsize=5, direction=\"in\")\n",
    "\n",
    "# plt.setp(axes[0][1].get_yticklabels(), visible=False)\n",
    "# plt.setp(axes[1][1].get_yticklabels(), visible=False)\n",
    "# plt.setp(axes[0][1].get_xticklabels(), visible=False)\n",
    "# plt.setp(axes[0][0].get_xticklabels(), visible=False)\n",
    "\n",
    "# axes[1][0].set_xticks([500,1000,1500,2000])\n",
    "# axes[1][1].set_xticks([500,1000,1500,2000])\n",
    "\n",
    "\n",
    "# axes[0][0].yaxis.set_label_coords(-0.2, 0)\n",
    "# axes[1][0].xaxis.set_label_coords(1, -0.1)\n",
    "\n",
    "# plt.suptitle(\"14 classes of activity of x-ray black hole binary GRS1915+105\", fontsize=40, y=0.92)\n",
    "\n",
    "\n",
    "plt.savefig('figures/all_classes_of_GRS1915.png', dpi=300, bbox_inches = 'tight',pad_inches = 0, transparent=True)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification based on Gaussian component proportions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "inv_ob_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compar_results = np.zeros((2,1))\n",
    "\n",
    "# inverse the ob_state dictionary, so that inv_ob_state contains {\"state name\" : [list of observation IDs], ...}\n",
    "inv_ob_state = {} \n",
    "for k, v in ob_state.items():\n",
    "    inv_ob_state[v] = inv_ob_state.get(v, [])\n",
    "    inv_ob_state[v].append(k)\n",
    "\n",
    "test_sets = []\n",
    "train_sets = []\n",
    "# split the observations into test and training sets\n",
    "for test_iter in range(1000):\n",
    "    test_set = []\n",
    "    train_set = []\n",
    "    \n",
    "    # split is stratified, so done separately for each class\n",
    "    for class_name in [\"alpha\", \"beta\", \"gamma\", \"delta\", \"theta\", \"kappa\", \"lambda\", \"mu\", \"nu\", \"rho\", \"phi\", \"chi\", \"eta\", \"omega\"]:\n",
    "\n",
    "        class_obs_all = inv_ob_state[class_name] # all labeled observation IDs of this class\n",
    "        # exclude observations which did not produce any light curve segments\n",
    "        class_obs = []\n",
    "        for ob in class_obs_all:\n",
    "            if ob in seg_ObIDs: # seg_ObIDs contains observation ID for each of the 468202 light curve segments\n",
    "                class_obs.append(ob)\n",
    "        \n",
    "        # pick 1/3 of observations for the test set (previously replace=True was set!!!!)\n",
    "        test_obs = np.random.choice(class_obs, size=int(np.ceil(len(class_obs)/3)), replace=False) \n",
    "\n",
    "        if len(test_obs) == 0:\n",
    "            print(class_name)\n",
    "        \n",
    "        # use the remaining observations as training set\n",
    "        train_obs = []\n",
    "        for ob in class_obs:\n",
    "            if ob not in test_obs:\n",
    "                train_obs.append(ob)\n",
    "        test_set.append(test_obs)\n",
    "        train_set.append(train_obs)\n",
    "\n",
    "    test_set=np.hstack(test_set)\n",
    "    train_set=np.hstack(train_set)\n",
    "    test_sets.append(test_set)\n",
    "    train_sets.append(train_set)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(seg_ObIDs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "test_sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pseudo_CLR_transformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "obs_component_counts_df_122"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(obs_component_counts_df_122[\"Class\"].values[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pseudo_CLR_transformed[\"Class\"] = \"Unknown\"\n",
    "for k,v in ob_state.items():\n",
    "    if str(k) in obs_component_counts_df_122.index.values:\n",
    "        pseudo_CLR_transformed.loc[str(k), \"Class\"] = v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_component_counts_df_122.iloc[:,:122].div(np.sum(obs_component_counts_df_122.iloc[:,:122], axis=1), axis=\"rows\")\n",
    "obs_component_counts_df_122[\"Class\"] = \"Unknown\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k,v in ob_state.items():\n",
    "    if str(k) in obs_component_counts_df_122.index.values:\n",
    "        obs_component_counts_df_122.loc[str(k), \"Class\"] = v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "obs_component_counts_df_122"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "pseudo_CLR_transformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(list(ob_state.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "148+72"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_component_counts_df_122.loc[test_set].Class.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "inv_ob_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "for t in test_set:\n",
    "    print(ob_state[t])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_component_counts_df_122.iloc[:,:122] = obs_component_counts_df_122.iloc[:,:122].div(np.sum(obs_component_counts_df_122.iloc[:,:122], axis=1), axis=\"rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from  sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import plot_confusion_matrix\n",
    "\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.metrics import accuracy_score\n",
    "#(y_true, y_pred, *, beta=1.0, labels=None, pos_label=1, average=None, warn_for=('precision', 'recall', 'f-score'), sample_weight=None, zero_division='warn')\n",
    "\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (6.97, 6.97)\n",
    "plt.rcParams.update({'font.size': 6})\n",
    "\n",
    "experiment_list = [\n",
    "\n",
    "    [obs_component_counts_df_122, \"Classification with proportional make up of observations in terms of 114 Gaussian mixture components\"],\n",
    "\n",
    "]\n",
    "\n",
    "reports = []\n",
    "for test_set, train_set in zip(test_sets, train_sets):\n",
    "\n",
    "    for dataset, title in experiment_list:\n",
    "        SVC_clf = RandomForestClassifier(random_state=0, class_weight=\"balanced\").fit(dataset.loc[train_set].iloc[:,:122], dataset[\"Class\"].loc[train_set])\n",
    "        if title[-8:] == \"balanced\":\n",
    "            SVC_clf = SVC(gamma=\"auto\", class_weight=\"balanced\").fit(dataset.loc[train_set].iloc[:,:122], dataset[\"Class\"].loc[train_set])\n",
    "\n",
    "        preds = SVC_clf.predict(dataset.loc[test_set].iloc[:,:122])\n",
    "#         reports.append(classification_report(dataset[\"Class\"].loc[test_set], preds))\n",
    "        reports.append(( precision_recall_fscore_support(dataset[\"Class\"].loc[test_set], preds,zero_division=0, average=\"weighted\"),precision_recall_fscore_support(dataset[\"Class\"].loc[test_set], preds,zero_division=0, average=\"macro\"), accuracy_score(dataset[\"Class\"].loc[test_set], preds)))\n",
    "#             precision_recall_fscore_support(dataset[\"Class\"].loc[test_set], preds, average=\"macro\"), precision_recall_fscore_support(dataset[\"Class\"].loc[test_set], preds, average=\"micro\")))\n",
    "#         print(\"{}\".format(title))\n",
    "#         print(classification_report(dataset[\"Class\"].loc[test_set], preds))\n",
    "\n",
    "\n",
    "\n",
    "#         disp = plot_confusion_matrix(SVC_clf, dataset.loc[test_set].iloc[:,:122], dataset[\"Class\"].loc[test_set],\n",
    "#                                  cmap=plt.cm.Blues,\n",
    "#                                  normalize=None)\n",
    "    #     disp.ax_.set_title(\"{}\".format(title))\n",
    "#         disp.ax_.set_xlabel(\"Predicted class\")\n",
    "#         disp.ax_.set_ylabel(\"True class\")\n",
    "    #     plt.savefig('figures/classification_matrix.png', dpi=300, bbox_inches = 'tight',pad_inches = 0)\n",
    "\n",
    "    #     plt.show()\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.mean(f1s_weighted), np.median(f1s_weighted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.mean(accuracies), np.median(accuracies))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracies = []\n",
    "f1s_weighted = []\n",
    "f1s_average = []\n",
    "for report in reports:# weighted(precision, recall, f1), accuracy\n",
    "    accuracies.append(report[2])\n",
    "    f1s_weighted.append(report[0][2])\n",
    "    f1s_average.append(report[1][2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = (3.32, 3.32)\n",
    "plt.rcParams.update({'font.size': 6})\n",
    "\n",
    "plt.hist(f1s_average, bins=15, label=\"average F1\")\n",
    "plt.hist(f1s_weighted, bins=15, label=\"weighted F1\", alpha=0.7)\n",
    "plt.hist(accuracies, bins=15, label=\"accuracy\", alpha=0.7)\n",
    "plt.legend()\n",
    "# plt.xlim((0.65,0.95))\n",
    "axes = plt.gca()\n",
    "axes.tick_params(axis=\"x\", which=\"major\", length=2, width=0.75, labelsize=6, direction=\"in\")\n",
    "axes.tick_params(axis=\"y\", which=\"major\", length=2, width=0.75, labelsize=6, direction=\"in\")\n",
    "plt.xlabel(\"Test score\")\n",
    "plt.ylabel(\"No. classification experiments\")\n",
    "\n",
    "\n",
    "# plt.savefig('figures/classification_histogram.png', dpi=300, bbox_inches = 'tight',pad_inches = 0.01)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "[i/72 for i in range(1,73)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classification_report(dataset[\"Class\"].loc[test_set], preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a data frame containing the counts of light curve segments in each of the Gaussian components, for each observation\n",
    "obs_component_counts_df_122 = pd.DataFrame(np.zeros((len(ObID_GaussComps_dict_122),len(np.unique(shape_moments_GMM122_labels)))), index=np.unique(seg_ObIDs), columns=list(range(122)), dtype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# populate the data frame\n",
    "for ObID, GaussComps in ObID_GaussComps_dict_122.items():\n",
    "    for comp_id, comp_count in np.array(np.unique(GaussComps, return_counts=True)).T:\n",
    "        obs_component_counts_df_122.loc[ObID][comp_id] = comp_count\n",
    "obs_component_counts_df_122"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from  sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import plot_confusion_matrix\n",
    "\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.metrics import accuracy_score\n",
    "#(y_true, y_pred, *, beta=1.0, labels=None, pos_label=1, average=None, warn_for=('precision', 'recall', 'f-score'), sample_weight=None, zero_division='warn')\n",
    "\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (3.32, 3.32)#(6.97, 6.97)\n",
    "plt.rcParams.update({'font.size': 6})\n",
    "\n",
    "experiment_list = [\n",
    "\n",
    "    [obs_component_counts_df_122, \"Classification with proportional make up of observations in terms of 114 Gaussian mixture components\"],\n",
    "\n",
    "]\n",
    "\n",
    "reports = []\n",
    "# for test_set, train_set in zip(test_sets, train_sets):\n",
    "n=1#3\n",
    "test_set=test_sets[n]\n",
    "train_set=train_sets[n]\n",
    "for dataset, title in experiment_list:\n",
    "    SVC_clf = RandomForestClassifier(random_state=0, class_weight=\"balanced\").fit(dataset.loc[train_set].iloc[:,:122], dataset[\"Class\"].loc[train_set])\n",
    "    if title[-8:] == \"balanced\":\n",
    "        SVC_clf = SVC(gamma=\"auto\", class_weight=\"balanced\").fit(dataset.loc[train_set].iloc[:,:122], dataset[\"Class\"].loc[train_set])\n",
    "\n",
    "    preds = SVC_clf.predict(dataset.loc[test_set].iloc[:,:122])\n",
    "#         reports.append(classification_report(dataset[\"Class\"].loc[test_set], preds))\n",
    "    reports.append(( precision_recall_fscore_support(dataset[\"Class\"].loc[test_set], preds,zero_division=0, average=\"weighted\"),precision_recall_fscore_support(dataset[\"Class\"].loc[test_set], preds,zero_division=0, average=\"macro\"), accuracy_score(dataset[\"Class\"].loc[test_set], preds)))\n",
    "#             precision_recall_fscore_support(dataset[\"Class\"].loc[test_set], preds, average=\"macro\"), precision_recall_fscore_support(dataset[\"Class\"].loc[test_set], preds, average=\"micro\")))\n",
    "#     print(\"{}\".format(title))\n",
    "    print(classification_report(dataset[\"Class\"].loc[test_set], preds))\n",
    "\n",
    "\n",
    "\n",
    "    disp = plot_confusion_matrix(SVC_clf, dataset.loc[test_set].iloc[:,:122], dataset[\"Class\"].loc[test_set],\n",
    "                             cmap=plt.cm.Blues,\n",
    "                             normalize=None)\n",
    "#     disp.ax_.set_title(\"{}\".format(title))\n",
    "    disp.ax_.set_xlabel(\"Predicted class\")\n",
    "    disp.ax_.set_ylabel(\"True class\")\n",
    "    disp.ax_.set_yticklabels([r\"$\\{}$\".format(class_name) for class_name in np.unique(dataset[\"Class\"])[1:]])\n",
    "    disp.ax_.set_xticklabels([r\"$\\{}$\".format(class_name) for class_name in np.unique(dataset[\"Class\"])[1:]])\n",
    "    plt.savefig('figures/classification_matrix.png', dpi=300, bbox_inches = 'tight',pad_inches = 0)\n",
    "\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "misclas = np.where(dataset[\"Class\"].loc[test_set] != preds)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.where(ids == ob_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = (13.32, 3.32)\n",
    "for m in misclas:\n",
    "    ob_id = dataset.loc[test_set].index.values[m]\n",
    "    ob_ind = np.where(np.array(ids) == ob_id)[0][0]\n",
    "    plt.plot(lcs[ob_ind][0], lcs[ob_ind][1])\n",
    "    print(\"true: \", ob_state[ob_id], \"predicted: \", preds[m])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[ob_state[x] for x in [\"40703-01-30-00\", \"40703-01-30-01\", \"40703-01-30-02\", \"40703-01-14-00\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inv_ob_state[\"kappa\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.where(np.array(ids) == '40703-01-12-00')[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = (130.32, 3.32)\n",
    "for m in [\"40703-01-30-00\", \"40703-01-30-01\", \"40703-01-30-02\", \"40703-01-14-00\", \"40703-01-12-00\", \"40703-01-26-00\"]:\n",
    "    ob_ind = np.where(np.array(ids) == m)[0][0]\n",
    "    plt.plot(lcs[ob_ind][0], lcs[ob_ind][1])\n",
    "    print(\"true: \", ob_state[m])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[r\"$\\{}$\".format(class_name) for class_name in np.unique(dataset[\"Class\"])[1:]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification without bad kappa/omega "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compar_results = np.zeros((2,1))\n",
    "\n",
    "# inverse the ob_state dictionary, so that inv_ob_state contains {\"state name\" : [list of observation IDs], ...}\n",
    "inv_ob_state = {} \n",
    "for k, v in ob_state.items():\n",
    "    if k in [\"40703-01-30-00\", \"40703-01-30-01\", \"40703-01-30-02\", \"40703-01-14-00\"]:\n",
    "        continue\n",
    "    inv_ob_state[v] = inv_ob_state.get(v, [])\n",
    "    inv_ob_state[v].append(k)\n",
    "\n",
    "test_sets = []\n",
    "train_sets = []\n",
    "# split the observations into test and training sets\n",
    "for test_iter in range(10):\n",
    "    print(test_iter)\n",
    "    test_set = []\n",
    "    train_set = []\n",
    "    \n",
    "    # split is stratified, so done separately for each class\n",
    "    for class_name in [\"alpha\", \"beta\", \"gamma\", \"delta\", \"theta\", \"kappa\", \"lambda\", \"mu\", \"nu\", \"rho\", \"phi\", \"chi\", \"eta\", \"omega\"]:\n",
    "\n",
    "        class_obs_all = inv_ob_state[class_name] # all labeled observation IDs of this class\n",
    "        # exclude observations which did not produce any light curve segments\n",
    "        class_obs = []\n",
    "        for ob in class_obs_all:\n",
    "            if ob in seg_ObIDs: # seg_ObIDs contains observation ID for each of the 468202 light curve segments\n",
    "                class_obs.append(ob)\n",
    "        \n",
    "        # pick 1/3 of observations for the test set (previously replace=True was set!!!!)\n",
    "        test_obs = np.random.choice(class_obs, size=int(np.ceil(len(class_obs)/3)), replace=False) \n",
    "\n",
    "        if len(test_obs) == 0:\n",
    "            print(class_name)\n",
    "        \n",
    "        # use the remaining observations as training set\n",
    "        train_obs = []\n",
    "        for ob in class_obs:\n",
    "            if ob not in test_obs:\n",
    "                train_obs.append(ob)\n",
    "        test_set.append(test_obs)\n",
    "        train_set.append(train_obs)\n",
    "\n",
    "    test_set=np.hstack(test_set)\n",
    "    train_set=np.hstack(train_set)\n",
    "    test_sets.append(test_set)\n",
    "    train_sets.append(train_set)\n",
    "    clear_output(wait=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inv_ob_state[\"kappa\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pseudo_CLR_transformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "obs_component_counts_df_122"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(obs_component_counts_df_122[\"Class\"].values[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pseudo_CLR_transformed[\"Class\"] = \"Unknown\"\n",
    "for k,v in ob_state.items():\n",
    "    if str(k) in obs_component_counts_df_122.index.values:\n",
    "        pseudo_CLR_transformed.loc[str(k), \"Class\"] = v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_component_counts_df_122.iloc[:,:122].div(np.sum(obs_component_counts_df_122.iloc[:,:122], axis=1), axis=\"rows\")\n",
    "obs_component_counts_df_122[\"Class\"] = \"Unknown\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k,v in ob_state.items():\n",
    "    if str(k) in obs_component_counts_df_122.index.values:\n",
    "        obs_component_counts_df_122.loc[str(k), \"Class\"] = v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "obs_component_counts_df_122"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "pseudo_CLR_transformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "biclen(list(ob_state.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "148+72"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_component_counts_df_122.loc[test_set].Class.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "inv_ob_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "for t in test_set:\n",
    "    print(ob_state[t])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_component_counts_df_122.iloc[:,:122] = obs_component_counts_df_122.iloc[:,:122].div(np.sum(obs_component_counts_df_122.iloc[:,:122], axis=1), axis=\"rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from  sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import plot_confusion_matrix\n",
    "\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.metrics import accuracy_score\n",
    "#(y_true, y_pred, *, beta=1.0, labels=None, pos_label=1, average=None, warn_for=('precision', 'recall', 'f-score'), sample_weight=None, zero_division='warn')\n",
    "\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (6.97, 6.97)\n",
    "plt.rcParams.update({'font.size': 6})\n",
    "\n",
    "experiment_list = [\n",
    "\n",
    "    [obs_component_counts_df_122, \"Classification with proportional make up of observations in terms of 114 Gaussian mixture components\"],\n",
    "\n",
    "]\n",
    "n=0\n",
    "reports = []\n",
    "for test_set, train_set in zip(test_sets, train_sets):\n",
    "    print(n)\n",
    "    n+=1\n",
    "    clear_output(wait=True)\n",
    "    for dataset, title in experiment_list:\n",
    "        \n",
    "        SVC_clf = RandomForestClassifier(random_state=0, class_weight=\"balanced\").fit(dataset.loc[train_set].iloc[:,:122], dataset[\"Class\"].loc[train_set])\n",
    "        if title[-8:] == \"balanced\":\n",
    "            SVC_clf = SVC(gamma=\"auto\", class_weight=\"balanced\").fit(dataset.loc[train_set].iloc[:,:122], dataset[\"Class\"].loc[train_set])\n",
    "\n",
    "        preds = SVC_clf.predict(dataset.loc[test_set].iloc[:,:122])\n",
    "#         reports.append(classification_report(dataset[\"Class\"].loc[test_set], preds))\n",
    "        reports.append(( precision_recall_fscore_support(dataset[\"Class\"].loc[test_set], preds,zero_division=0, average=\"weighted\"),precision_recall_fscore_support(dataset[\"Class\"].loc[test_set], preds,zero_division=0, average=\"macro\"), accuracy_score(dataset[\"Class\"].loc[test_set], preds)))\n",
    "#             precision_recall_fscore_support(dataset[\"Class\"].loc[test_set], preds, average=\"macro\"), precision_recall_fscore_support(dataset[\"Class\"].loc[test_set], preds, average=\"micro\")))\n",
    "#         print(\"{}\".format(title))\n",
    "#         print(classification_report(dataset[\"Class\"].loc[test_set], preds))\n",
    "\n",
    "\n",
    "\n",
    "#         disp = plot_confusion_matrix(SVC_clf, dataset.loc[test_set].iloc[:,:122], dataset[\"Class\"].loc[test_set],\n",
    "#                                  cmap=plt.cm.Blues,\n",
    "#                                  normalize=None)\n",
    "    #     disp.ax_.set_title(\"{}\".format(title))\n",
    "#         disp.ax_.set_xlabel(\"Predicted class\")\n",
    "#         disp.ax_.set_ylabel(\"True class\")\n",
    "    #     plt.savefig('figures/classification_matrix.png', dpi=300, bbox_inches = 'tight',pad_inches = 0)\n",
    "\n",
    "    #     plt.show()\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "for tt in test_sets:\n",
    "    print(len(tt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[\"Class\"].loc[test_set]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision_recall_fscore_support(np.concatenate((np.ones(26), np.zeros(72-26))), np.ones(72),zero_division=0, average=\"weighted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.mean(f1s_weighted), np.median(f1s_weighted), np.std(f1s_weighted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.mean(accuracies), np.median(accuracies), np.std(accuracies))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracies = []\n",
    "f1s_weighted = []\n",
    "f1s_average = []\n",
    "for report in reports:# weighted(precision, recall, f1), accuracy\n",
    "    accuracies.append(report[2])\n",
    "    f1s_weighted.append(report[0][2])\n",
    "    f1s_average.append(report[1][2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = (3.32, 3.32)\n",
    "plt.rcParams.update({'font.size': 6})\n",
    "\n",
    "# plt.hist(f1s_average, bins=15, label=\"average F1\")\n",
    "plt.hist(f1s_weighted, bins=15, label=\"weighted F1\", alpha=0.7, color=\"blue\")\n",
    "plt.hist(accuracies, bins=15, label=\"accuracy\", alpha=0.8, color=\"orange\")\n",
    "plt.legend()\n",
    "# plt.xlim((0.65,0.95))\n",
    "axes = plt.gca()\n",
    "axes.tick_params(axis=\"x\", which=\"major\", length=2, width=0.75, labelsize=6, direction=\"in\")\n",
    "axes.tick_params(axis=\"y\", which=\"major\", length=2, width=0.75, labelsize=6, direction=\"in\")\n",
    "plt.xlabel(\"Test score\")\n",
    "plt.ylabel(\"No. classification experiments\")\n",
    "\n",
    "plt.axvline(0.19160997732426302, c=\"blue\")\n",
    "\n",
    "plt.axvline(0.3611111111111111, c=\"orange\")\n",
    "\n",
    "\n",
    "plt.savefig('figures/classification_histogram_lines.png', dpi=300, bbox_inches = 'tight',pad_inches = 0.01)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "[i/72 for i in range(1,73)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classification_report(dataset[\"Class\"].loc[test_set], preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a data frame containing the counts of light curve segments in each of the Gaussian components, for each observation\n",
    "obs_component_counts_df_122 = pd.DataFrame(np.zeros((len(ObID_GaussComps_dict_122),len(np.unique(shape_moments_GMM122_labels)))), index=np.unique(seg_ObIDs), columns=list(range(122)), dtype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# populate the data frame\n",
    "for ObID, GaussComps in ObID_GaussComps_dict_122.items():\n",
    "    for comp_id, comp_count in np.array(np.unique(GaussComps, return_counts=True)).T:\n",
    "        obs_component_counts_df_122.loc[ObID][comp_id] = comp_count\n",
    "obs_component_counts_df_122"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from  sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import plot_confusion_matrix\n",
    "\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.metrics import accuracy_score\n",
    "#(y_true, y_pred, *, beta=1.0, labels=None, pos_label=1, average=None, warn_for=('precision', 'recall', 'f-score'), sample_weight=None, zero_division='warn')\n",
    "\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (3.32, 3.32)#(6.97, 6.97)\n",
    "plt.rcParams.update({'font.size': 6})\n",
    "\n",
    "experiment_list = [\n",
    "\n",
    "    [obs_component_counts_df_122, \"Classification with proportional make up of observations in terms of 114 Gaussian mixture components\"],\n",
    "\n",
    "]\n",
    "\n",
    "reports = []\n",
    "# for test_set, train_set in zip(test_sets, train_sets):\n",
    "n=1#3\n",
    "test_set=test_sets[n]\n",
    "train_set=train_sets[n]\n",
    "for dataset, title in experiment_list:\n",
    "    SVC_clf = RandomForestClassifier(random_state=0, class_weight=\"balanced\").fit(dataset.loc[train_set].iloc[:,:122], dataset[\"Class\"].loc[train_set])\n",
    "    if title[-8:] == \"balanced\":\n",
    "        SVC_clf = SVC(gamma=\"auto\", class_weight=\"balanced\").fit(dataset.loc[train_set].iloc[:,:122], dataset[\"Class\"].loc[train_set])\n",
    "\n",
    "    preds = SVC_clf.predict(dataset.loc[test_set].iloc[:,:122])\n",
    "#         reports.append(classification_report(dataset[\"Class\"].loc[test_set], preds))\n",
    "    reports.append(( precision_recall_fscore_support(dataset[\"Class\"].loc[test_set], preds,zero_division=0, average=\"weighted\"),precision_recall_fscore_support(dataset[\"Class\"].loc[test_set], preds,zero_division=0, average=\"macro\"), accuracy_score(dataset[\"Class\"].loc[test_set], preds)))\n",
    "#             precision_recall_fscore_support(dataset[\"Class\"].loc[test_set], preds, average=\"macro\"), precision_recall_fscore_support(dataset[\"Class\"].loc[test_set], preds, average=\"micro\")))\n",
    "#     print(\"{}\".format(title))\n",
    "    print(classification_report(dataset[\"Class\"].loc[test_set], preds))\n",
    "\n",
    "\n",
    "\n",
    "    disp = plot_confusion_matrix(SVC_clf, dataset.loc[test_set].iloc[:,:122], dataset[\"Class\"].loc[test_set],\n",
    "                             cmap=plt.cm.Blues,\n",
    "                             normalize=None)\n",
    "#     disp.ax_.set_title(\"{}\".format(title))\n",
    "    disp.ax_.set_xlabel(\"Predicted class\")\n",
    "    disp.ax_.set_ylabel(\"True class\")\n",
    "    disp.ax_.set_yticklabels([r\"$\\{}$\".format(class_name) for class_name in np.unique(dataset[\"Class\"])[1:]])\n",
    "    disp.ax_.set_xticklabels([r\"$\\{}$\".format(class_name) for class_name in np.unique(dataset[\"Class\"])[1:]])\n",
    "    plt.savefig('figures/classification_matrix.png', dpi=300, bbox_inches = 'tight',pad_inches = 0)\n",
    "\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ob in inv_ob_state[\"nu\"]:\n",
    "    if ob in seg_ObIDs:\n",
    "        print(ob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification with four chi sub classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load observation classifications from Huppenkothen 2017\n",
    "clean_belloni = open('{}/1915Belloniclass_updated.dat'.format(data_dir))\n",
    "lines = clean_belloni.readlines()\n",
    "states = lines[0].split()\n",
    "belloni_clean = {}\n",
    "for h,l in zip(states, lines[1:]):\n",
    "    belloni_clean[h] = l.split()\n",
    "    #state: obsID1, obsID2...\n",
    "ob_state = {}\n",
    "for state, obs in belloni_clean.items():\n",
    "#     if state == \"chi1\" or state == \"chi2\" or state == \"chi3\" or state == \"chi4\": state = \"chi\"\n",
    "    for ob in obs:\n",
    "        ob_state[ob] = state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "ob_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compar_results = np.zeros((2,1))\n",
    "\n",
    "# inverse the ob_state dictionary, so that inv_ob_state contains {\"state name\" : [list of observation IDs], ...}\n",
    "inv_ob_state = {} \n",
    "for k, v in ob_state.items():\n",
    "    inv_ob_state[v] = inv_ob_state.get(v, [])\n",
    "    inv_ob_state[v].append(k)\n",
    "\n",
    "test_sets = []\n",
    "train_sets = []\n",
    "# split the observations into test and training sets\n",
    "for test_iter in range(1000):\n",
    "    test_set = []\n",
    "    train_set = []\n",
    "    \n",
    "    # split is stratified, so done separately for each class\n",
    "    for class_name in [\"alpha\", \"beta\", \"gamma\", \"delta\", \"theta\", \"kappa\", \"lambda\", \"mu\", \"nu\", \"rho\", \"phi\", \"chi1\",\"chi2\",\"chi3\",\"chi4\", \"eta\", \"omega\"]:\n",
    "\n",
    "        class_obs_all = inv_ob_state[class_name] # all labeled observation IDs of this class\n",
    "        # exclude observations which did not produce any light curve segments\n",
    "        class_obs = []\n",
    "        for ob in class_obs_all:\n",
    "            if ob in seg_ObIDs: # seg_ObIDs contains observation ID for each of the 468202 light curve segments\n",
    "                class_obs.append(ob)\n",
    "        \n",
    "        # pick 1/3 of observations for the test set (previously replace=True was set!!!!)\n",
    "        test_obs = np.random.choice(class_obs, size=int(np.ceil(len(class_obs)/3)), replace=False) \n",
    "\n",
    "        if len(test_obs) == 0:\n",
    "            print(class_name)\n",
    "        \n",
    "        # use the remaining observations as training set\n",
    "        train_obs = []\n",
    "        for ob in class_obs:\n",
    "            if ob not in test_obs:\n",
    "                train_obs.append(ob)\n",
    "        test_set.append(test_obs)\n",
    "        train_set.append(train_obs)\n",
    "\n",
    "    test_set=np.hstack(test_set)\n",
    "    train_set=np.hstack(train_set)\n",
    "    test_sets.append(test_set)\n",
    "    train_sets.append(train_set)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inv_ob_state.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sets[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sets[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pseudo_CLR_transformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "obs_component_counts_df_122"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(obs_component_counts_df_122[\"Class\"].values[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pseudo_CLR_transformed[\"Class\"] = \"Unknown\"\n",
    "for k,v in ob_state.items():\n",
    "    if str(k) in obs_component_counts_df_122.index.values:\n",
    "        pseudo_CLR_transformed.loc[str(k), \"Class\"] = v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_component_counts_df_122.iloc[:,:122].div(np.sum(obs_component_counts_df_122.iloc[:,:122], axis=1), axis=\"rows\")\n",
    "obs_component_counts_df_122[\"Class\"] = \"Unknown\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k,v in ob_state.items():\n",
    "    if str(k) in obs_component_counts_df_122.index.values:\n",
    "        obs_component_counts_df_122.loc[str(k), \"Class\"] = v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "obs_component_counts_df_122"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "pseudo_CLR_transformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(list(ob_state.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "148+72"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "[ob_state[x] for x in obs_component_counts_df_122.loc[test_set].index.values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "inv_ob_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "for t in test_set:\n",
    "    print(ob_state[t])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_component_counts_df_122.iloc[:,:122] = obs_component_counts_df_122.iloc[:,:122].div(np.sum(obs_component_counts_df_122.iloc[:,:122], axis=1), axis=\"rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from  sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import plot_confusion_matrix\n",
    "\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.metrics import accuracy_score\n",
    "#(y_true, y_pred, *, beta=1.0, labels=None, pos_label=1, average=None, warn_for=('precision', 'recall', 'f-score'), sample_weight=None, zero_division='warn')\n",
    "\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (6.97, 6.97)\n",
    "plt.rcParams.update({'font.size': 6})\n",
    "\n",
    "experiment_list = [\n",
    "\n",
    "    [obs_component_counts_df_122, \"Classification with proportional make up of observations in terms of 114 Gaussian mixture components\"],\n",
    "\n",
    "]\n",
    "\n",
    "\n",
    "reports = []\n",
    "for test_set, train_set in zip(test_sets, train_sets):\n",
    "\n",
    "    for dataset, title in experiment_list:\n",
    "        SVC_clf = RandomForestClassifier(random_state=0, class_weight=\"balanced\").fit(dataset.loc[train_set].iloc[:,:122], [ob_state[x] for x in train_set])\n",
    "        if title[-8:] == \"balanced\":\n",
    "            SVC_clf = SVC(gamma=\"auto\", class_weight=\"balanced\").fit(dataset.loc[train_set].iloc[:,:122], [ob_state[x] for x in train_set])\n",
    "\n",
    "        preds = SVC_clf.predict(dataset.loc[test_set].iloc[:,:122])\n",
    "#         reports.append(classification_report(dataset[\"Class\"].loc[test_set], preds))\n",
    "        reports.append(( precision_recall_fscore_support([ob_state[x] for x in test_set], preds,zero_division=0, average=\"weighted\"),precision_recall_fscore_support([ob_state[x] for x in test_set], preds,zero_division=0, average=\"macro\"), accuracy_score([ob_state[x] for x in test_set], preds)))\n",
    "#             precision_recall_fscore_support(dataset[\"Class\"].loc[test_set], preds, average=\"macro\"), precision_recall_fscore_support(dataset[\"Class\"].loc[test_set], preds, average=\"micro\")))\n",
    "#         print(\"{}\".format(title))\n",
    "#         print(classification_report(dataset[\"Class\"].loc[test_set], preds))\n",
    "\n",
    "\n",
    "\n",
    "#         disp = plot_confusion_matrix(SVC_clf, dataset.loc[test_set].iloc[:,:122], dataset[\"Class\"].loc[test_set],\n",
    "#                                  cmap=plt.cm.Blues,\n",
    "#                                  normalize=None)\n",
    "    #     disp.ax_.set_title(\"{}\".format(title))\n",
    "#         disp.ax_.set_xlabel(\"Predicted class\")\n",
    "#         disp.ax_.set_ylabel(\"True class\")\n",
    "    #     plt.savefig('figures/classification_matrix.png', dpi=300, bbox_inches = 'tight',pad_inches = 0)\n",
    "\n",
    "    #     plt.show()\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracies = []\n",
    "f1s_weighted = []\n",
    "f1s_average = []\n",
    "for report in reports:# weighted(precision, recall, f1), accuracy\n",
    "    accuracies.append(report[2])\n",
    "    f1s_weighted.append(report[0][2])\n",
    "    f1s_average.append(report[1][2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = (3.32, 3.32)\n",
    "plt.rcParams.update({'font.size': 6})\n",
    "\n",
    "plt.hist(f1s_average, bins=15, label=\"average F1\")\n",
    "plt.hist(f1s_weighted, bins=15, label=\"weighted F1\", alpha=0.7)\n",
    "plt.hist(accuracies, bins=15, label=\"accuracy\", alpha=0.7)\n",
    "plt.legend()\n",
    "# plt.xlim((0.65,0.95))\n",
    "axes = plt.gca()\n",
    "axes.tick_params(axis=\"x\", which=\"major\", length=2, width=0.75, labelsize=6, direction=\"in\")\n",
    "axes.tick_params(axis=\"y\", which=\"major\", length=2, width=0.75, labelsize=6, direction=\"in\")\n",
    "plt.xlabel(\"Test score\")\n",
    "plt.ylabel(\"No. classification experiments\")\n",
    "plt.title(\"Classification with 4 chi subclasses\")\n",
    "\n",
    "\n",
    "plt.savefig('figures/classification_histogram_chi_subclasses.png', dpi=300, bbox_inches = 'tight',pad_inches = 0.01)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.mean(f1s_weighted), np.median(f1s_weighted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.mean(accuracies), np.median(accuracies))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "[i/72 for i in range(1,73)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classification_report(dataset[\"Class\"].loc[test_set], preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a data frame containing the counts of light curve segments in each of the Gaussian components, for each observation\n",
    "obs_component_counts_df_122 = pd.DataFrame(np.zeros((len(ObID_GaussComps_dict_122),len(np.unique(shape_moments_GMM122_labels)))), index=np.unique(seg_ObIDs), columns=list(range(122)), dtype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# populate the data frame\n",
    "for ObID, GaussComps in ObID_GaussComps_dict_122.items():\n",
    "    for comp_id, comp_count in np.array(np.unique(GaussComps, return_counts=True)).T:\n",
    "        obs_component_counts_df_122.loc[ObID][comp_id] = comp_count\n",
    "obs_component_counts_df_122"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from  sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import plot_confusion_matrix\n",
    "\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.metrics import accuracy_score\n",
    "#(y_true, y_pred, *, beta=1.0, labels=None, pos_label=1, average=None, warn_for=('precision', 'recall', 'f-score'), sample_weight=None, zero_division='warn')\n",
    "\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (3.32, 3.32)#(6.97, 6.97)\n",
    "plt.rcParams.update({'font.size': 6})\n",
    "\n",
    "experiment_list = [\n",
    "\n",
    "    [obs_component_counts_df_122, \"Classification with proportional make up of observations in terms of 114 Gaussian mixture components\"],\n",
    "\n",
    "]\n",
    "\n",
    "reports = []\n",
    "# for test_set, train_set in zip(test_sets, train_sets):\n",
    "n=1#3\n",
    "test_set=test_sets[n]\n",
    "train_set=train_sets[n]\n",
    "for dataset, title in experiment_list:\n",
    "    SVC_clf = RandomForestClassifier(random_state=0, class_weight=\"balanced\").fit(dataset.loc[train_set].iloc[:,:122], [ob_state[x] for x in train_set])\n",
    "    if title[-8:] == \"balanced\":\n",
    "        SVC_clf = SVC(gamma=\"auto\", class_weight=\"balanced\").fit(dataset.loc[train_set].iloc[:,:122], [ob_state[x] for x in train_set])\n",
    "\n",
    "    preds = SVC_clf.predict(dataset.loc[test_set].iloc[:,:122])\n",
    "#         reports.append(classification_report(dataset[\"Class\"].loc[test_set], preds))\n",
    "    reports.append(( precision_recall_fscore_support([ob_state[x] for x in test_set], preds,zero_division=0, average=\"weighted\"),precision_recall_fscore_support([ob_state[x] for x in test_set], preds,zero_division=0, average=\"macro\"), accuracy_score([ob_state[x] for x in test_set], preds)))\n",
    "#             precision_recall_fscore_support(dataset[\"Class\"].loc[test_set], preds, average=\"macro\"), precision_recall_fscore_support(dataset[\"Class\"].loc[test_set], preds, average=\"micro\")))\n",
    "#     print(\"{}\".format(title))\n",
    "    print(classification_report([ob_state[x] for x in test_set], preds))\n",
    "\n",
    "\n",
    "\n",
    "    disp = plot_confusion_matrix(SVC_clf, dataset.loc[test_set].iloc[:,:122], [ob_state[x] for x in test_set],\n",
    "                             cmap=plt.cm.Blues,\n",
    "                             normalize=None)\n",
    "# #     disp.ax_.set_title(\"{}\".format(title))\n",
    "#     disp.ax_.set_xlabel(\"Predicted class\")\n",
    "#     disp.ax_.set_ylabel(\"True class\")\n",
    "#     disp.ax_.set_yticklabels([r\"$\\{}$\".format(class_name) for class_name in np.unique([ob_state[x] for x in test_set])[1:]])\n",
    "#     disp.ax_.set_xticklabels([r\"$\\{}$\".format(class_name) for class_name in np.unique([ob_state[x] for x in test_set])[1:]])\n",
    "# #     plt.savefig('figures/classification_matrix.png', dpi=300, bbox_inches = 'tight',pad_inches = 0)\n",
    "\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jakub-tf",
   "language": "python",
   "name": "jakub-tf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
