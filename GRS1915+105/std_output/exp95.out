2020-03-19 11:36:06.358393: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX
2020-03-19 11:36:06.660350: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:897] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-03-19 11:36:06.660884: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1405] Found device 0 with properties: 
name: TITAN Xp major: 6 minor: 1 memoryClockRate(GHz): 1.582
pciBusID: 0000:01:00.0
totalMemory: 11.91GiB freeMemory: 11.75GiB
2020-03-19 11:36:06.660901: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1484] Adding visible gpu devices: 0
2020-03-19 11:36:06.922867: I tensorflow/core/common_runtime/gpu/gpu_device.cc:965] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-03-19 11:36:06.922913: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971]      0 
2020-03-19 11:36:06.922922: I tensorflow/core/common_runtime/gpu/gpu_device.cc:984] 0:   N 
2020-03-19 11:36:06.923172: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1097] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 11366 MB memory) -> physical GPU (device: 0, name: TITAN Xp, pci bus id: 0000:01:00.0, compute capability: 6.1)
2020-03-19 11:36:07.131076: I tensorflow/core/kernels/cuda_solvers.cc:159] Creating CudaSolver handles for stream 0x5557b06f4d60
Epoch 1/8000

Epoch 00001: val_loss improved from inf to 19.05371, saving model to ../../model_weights/model_2020-03-19_11-36-07.h5
 - 112s - loss: 23.8640 - val_loss: 19.0537
Epoch 2/8000

Epoch 00002: val_loss improved from 19.05371 to 16.98310, saving model to ../../model_weights/model_2020-03-19_11-36-07.h5
 - 110s - loss: 17.9097 - val_loss: 16.9831
Epoch 3/8000

Epoch 00003: val_loss improved from 16.98310 to 16.43699, saving model to ../../model_weights/model_2020-03-19_11-36-07.h5
 - 110s - loss: 16.4181 - val_loss: 16.4370
Epoch 4/8000

Epoch 00004: val_loss improved from 16.43699 to 15.56428, saving model to ../../model_weights/model_2020-03-19_11-36-07.h5
 - 111s - loss: 15.7812 - val_loss: 15.5643
Epoch 5/8000

Epoch 00005: val_loss improved from 15.56428 to 14.73755, saving model to ../../model_weights/model_2020-03-19_11-36-07.h5
 - 111s - loss: 15.0700 - val_loss: 14.7376
Epoch 6/8000

Epoch 00006: val_loss improved from 14.73755 to 14.58922, saving model to ../../model_weights/model_2020-03-19_11-36-07.h5
 - 110s - loss: 14.7830 - val_loss: 14.5892
Epoch 7/8000

Epoch 00007: val_loss improved from 14.58922 to 14.40331, saving model to ../../model_weights/model_2020-03-19_11-36-07.h5
 - 110s - loss: 14.4847 - val_loss: 14.4033
Epoch 8/8000

Epoch 00008: val_loss improved from 14.40331 to 14.30428, saving model to ../../model_weights/model_2020-03-19_11-36-07.h5
 - 110s - loss: 14.2835 - val_loss: 14.3043
Epoch 9/8000

Epoch 00009: val_loss improved from 14.30428 to 14.08646, saving model to ../../model_weights/model_2020-03-19_11-36-07.h5
 - 110s - loss: 14.2382 - val_loss: 14.0865
Epoch 10/8000

Epoch 00010: val_loss improved from 14.08646 to 13.69739, saving model to ../../model_weights/model_2020-03-19_11-36-07.h5
 - 110s - loss: 14.0489 - val_loss: 13.6974
Epoch 11/8000

Epoch 00011: val_loss improved from 13.69739 to 13.60679, saving model to ../../model_weights/model_2020-03-19_11-36-07.h5
 - 111s - loss: 13.7751 - val_loss: 13.6068
Epoch 12/8000

Epoch 00012: val_loss improved from 13.60679 to 13.59658, saving model to ../../model_weights/model_2020-03-19_11-36-07.h5
 - 111s - loss: 13.7836 - val_loss: 13.5966
Epoch 13/8000

Epoch 00013: val_loss improved from 13.59658 to 13.34744, saving model to ../../model_weights/model_2020-03-19_11-36-07.h5
 - 111s - loss: 13.5972 - val_loss: 13.3474
Epoch 14/8000

Epoch 00014: val_loss did not improve from 13.34744
 - 111s - loss: 13.5647 - val_loss: 13.4883
Epoch 15/8000

Epoch 00015: val_loss improved from 13.34744 to 13.27897, saving model to ../../model_weights/model_2020-03-19_11-36-07.h5
 - 112s - loss: 13.3567 - val_loss: 13.2790
Epoch 16/8000

Epoch 00016: val_loss improved from 13.27897 to 13.15372, saving model to ../../model_weights/model_2020-03-19_11-36-07.h5
 - 112s - loss: 13.2488 - val_loss: 13.1537
Epoch 17/8000

Epoch 00017: val_loss did not improve from 13.15372
 - 111s - loss: 13.2177 - val_loss: 13.4634
Epoch 18/8000

Epoch 00018: val_loss did not improve from 13.15372
 - 111s - loss: 13.1555 - val_loss: 13.5913
Epoch 19/8000

Epoch 00019: val_loss did not improve from 13.15372
 - 110s - loss: 13.1853 - val_loss: 13.1706
Epoch 20/8000

Epoch 00020: val_loss did not improve from 13.15372
 - 111s - loss: 13.3955 - val_loss: 13.6884
Epoch 21/8000

Epoch 00021: val_loss improved from 13.15372 to 13.05593, saving model to ../../model_weights/model_2020-03-19_11-36-07.h5
 - 111s - loss: 13.3134 - val_loss: 13.0559
Epoch 22/8000

Epoch 00022: val_loss improved from 13.05593 to 12.89501, saving model to ../../model_weights/model_2020-03-19_11-36-07.h5
 - 112s - loss: 12.8489 - val_loss: 12.8950
Epoch 23/8000

Epoch 00023: val_loss did not improve from 12.89501
 - 112s - loss: 13.2156 - val_loss: 14.2739
Epoch 24/8000

Epoch 00024: val_loss improved from 12.89501 to 12.81064, saving model to ../../model_weights/model_2020-03-19_11-36-07.h5
 - 111s - loss: 13.0540 - val_loss: 12.8106
Epoch 25/8000

Epoch 00025: val_loss did not improve from 12.81064
 - 111s - loss: 12.9648 - val_loss: 13.0588
Epoch 26/8000

Epoch 00026: val_loss improved from 12.81064 to 12.73224, saving model to ../../model_weights/model_2020-03-19_11-36-07.h5
 - 111s - loss: 12.7951 - val_loss: 12.7322
Epoch 27/8000

Epoch 00027: val_loss improved from 12.73224 to 12.61753, saving model to ../../model_weights/model_2020-03-19_11-36-07.h5
 - 111s - loss: 12.7023 - val_loss: 12.6175
Epoch 28/8000

Epoch 00028: val_loss did not improve from 12.61753
 - 110s - loss: 12.8204 - val_loss: 12.9053
Epoch 29/8000

Epoch 00029: val_loss did not improve from 12.61753
 - 110s - loss: 12.8669 - val_loss: 13.2542
Epoch 30/8000

Epoch 00030: val_loss did not improve from 12.61753
 - 110s - loss: 12.7486 - val_loss: 13.4305
Epoch 31/8000

Epoch 00031: val_loss did not improve from 12.61753
 - 110s - loss: 12.7989 - val_loss: 12.8039
Epoch 32/8000

Epoch 00032: val_loss did not improve from 12.61753
 - 111s - loss: 12.8521 - val_loss: 12.8320
Epoch 33/8000

Epoch 00033: val_loss improved from 12.61753 to 12.56687, saving model to ../../model_weights/model_2020-03-19_11-36-07.h5
 - 111s - loss: 12.6524 - val_loss: 12.5669
Epoch 34/8000

Epoch 00034: val_loss did not improve from 12.56687
 - 111s - loss: 12.5846 - val_loss: 12.9318
Epoch 35/8000

Epoch 00035: val_loss improved from 12.56687 to 12.56377, saving model to ../../model_weights/model_2020-03-19_11-36-07.h5
 - 111s - loss: 12.6023 - val_loss: 12.5638
Epoch 36/8000

Epoch 00036: val_loss did not improve from 12.56377
 - 111s - loss: 12.7597 - val_loss: 12.8086
Epoch 37/8000

Epoch 00037: val_loss did not improve from 12.56377
 - 111s - loss: 12.5809 - val_loss: 12.6090
Epoch 38/8000

Epoch 00038: val_loss improved from 12.56377 to 12.49595, saving model to ../../model_weights/model_2020-03-19_11-36-07.h5
 - 110s - loss: 12.6311 - val_loss: 12.4960
Epoch 39/8000

Epoch 00039: val_loss improved from 12.49595 to 12.46344, saving model to ../../model_weights/model_2020-03-19_11-36-07.h5
 - 111s - loss: 12.5223 - val_loss: 12.4634
Epoch 40/8000

Epoch 00040: val_loss did not improve from 12.46344
 - 111s - loss: 12.8442 - val_loss: 12.6017
Epoch 41/8000

Epoch 00041: val_loss did not improve from 12.46344
 - 111s - loss: 12.7938 - val_loss: 12.6927
Epoch 42/8000

Epoch 00042: val_loss did not improve from 12.46344
 - 111s - loss: 12.7300 - val_loss: 12.6092
Epoch 43/8000

Epoch 00043: val_loss did not improve from 12.46344
 - 111s - loss: 12.4830 - val_loss: 12.6539
Epoch 44/8000

Epoch 00044: val_loss improved from 12.46344 to 12.36579, saving model to ../../model_weights/model_2020-03-19_11-36-07.h5
 - 111s - loss: 12.2894 - val_loss: 12.3658
Epoch 45/8000

Epoch 00045: val_loss improved from 12.36579 to 12.26145, saving model to ../../model_weights/model_2020-03-19_11-36-07.h5
 - 111s - loss: 12.3226 - val_loss: 12.2614
Epoch 46/8000

Epoch 00046: val_loss did not improve from 12.26145
 - 111s - loss: 12.3362 - val_loss: 12.6536
Epoch 47/8000

Epoch 00047: val_loss did not improve from 12.26145
 - 111s - loss: 12.3838 - val_loss: 12.4831
Epoch 48/8000

Epoch 00048: val_loss improved from 12.26145 to 12.03749, saving model to ../../model_weights/model_2020-03-19_11-36-07.h5
 - 110s - loss: 12.2102 - val_loss: 12.0375
Epoch 49/8000

Epoch 00049: val_loss did not improve from 12.03749
 - 110s - loss: 12.3820 - val_loss: 12.5780
Epoch 50/8000

Epoch 00050: val_loss did not improve from 12.03749
 - 111s - loss: 12.3535 - val_loss: 12.2081
Epoch 51/8000

Epoch 00051: val_loss did not improve from 12.03749
 - 111s - loss: 12.2939 - val_loss: 12.4039
Epoch 52/8000

Epoch 00052: val_loss did not improve from 12.03749
 - 111s - loss: 12.4123 - val_loss: 12.9333
Epoch 53/8000

Epoch 00053: val_loss did not improve from 12.03749
 - 111s - loss: 12.6095 - val_loss: 13.9279
Epoch 54/8000

Epoch 00054: val_loss did not improve from 12.03749
 - 111s - loss: 12.6066 - val_loss: 12.7170
Epoch 55/8000

Epoch 00055: val_loss did not improve from 12.03749
 - 111s - loss: 12.4224 - val_loss: 12.4463
Epoch 56/8000

Epoch 00056: val_loss did not improve from 12.03749
 - 111s - loss: 12.4560 - val_loss: 12.4955
Epoch 57/8000

Epoch 00057: val_loss did not improve from 12.03749
 - 111s - loss: 12.4160 - val_loss: 12.2795
Epoch 58/8000

Epoch 00058: val_loss did not improve from 12.03749
 - 110s - loss: 12.1637 - val_loss: 12.0945
Epoch 59/8000

Epoch 00059: val_loss did not improve from 12.03749
 - 110s - loss: 12.1842 - val_loss: 12.2996
Epoch 60/8000

Epoch 00060: val_loss did not improve from 12.03749
 - 111s - loss: 12.1333 - val_loss: 12.1402
Epoch 61/8000

Epoch 00061: val_loss did not improve from 12.03749
 - 111s - loss: 12.1419 - val_loss: 12.3803
Epoch 62/8000

Epoch 00062: val_loss did not improve from 12.03749
 - 110s - loss: 12.1431 - val_loss: 12.1955
Epoch 63/8000

Epoch 00063: val_loss did not improve from 12.03749
 - 111s - loss: 12.0679 - val_loss: 12.2259
Epoch 64/8000

Epoch 00064: val_loss did not improve from 12.03749
 - 111s - loss: 12.3146 - val_loss: 12.1126
Epoch 65/8000

Epoch 00065: val_loss improved from 12.03749 to 12.02760, saving model to ../../model_weights/model_2020-03-19_11-36-07.h5
 - 111s - loss: 12.1692 - val_loss: 12.0276
Epoch 66/8000

Epoch 00066: val_loss did not improve from 12.02760
 - 110s - loss: 12.3244 - val_loss: 12.3860
Epoch 67/8000

Epoch 00067: val_loss did not improve from 12.02760
 - 111s - loss: 12.1675 - val_loss: 12.5147
Epoch 68/8000

Epoch 00068: val_loss did not improve from 12.02760
 - 111s - loss: 12.3084 - val_loss: 12.4588
Epoch 69/8000

Epoch 00069: val_loss did not improve from 12.02760
 - 110s - loss: 12.3695 - val_loss: 12.5429
Epoch 70/8000

Epoch 00070: val_loss did not improve from 12.02760
 - 110s - loss: 12.3256 - val_loss: 12.3850
Epoch 71/8000

Epoch 00071: val_loss did not improve from 12.02760
 - 110s - loss: 12.2650 - val_loss: 12.1869
Epoch 72/8000

Epoch 00072: val_loss did not improve from 12.02760
 - 111s - loss: 12.2002 - val_loss: 12.5232
Epoch 73/8000

Epoch 00073: val_loss did not improve from 12.02760
 - 110s - loss: 12.3851 - val_loss: 12.6496
Epoch 74/8000

Epoch 00074: val_loss improved from 12.02760 to 11.94512, saving model to ../../model_weights/model_2020-03-19_11-36-07.h5
 - 111s - loss: 12.2382 - val_loss: 11.9451
Epoch 75/8000

Epoch 00075: val_loss did not improve from 11.94512
 - 111s - loss: 12.2055 - val_loss: 12.1421
Epoch 76/8000

Epoch 00076: val_loss did not improve from 11.94512
 - 110s - loss: 12.2075 - val_loss: 12.0699
Epoch 77/8000

Epoch 00077: val_loss did not improve from 11.94512
 - 111s - loss: 12.2079 - val_loss: 12.0387
Epoch 78/8000

Epoch 00078: val_loss did not improve from 11.94512
 - 111s - loss: 12.0035 - val_loss: 12.8215
Epoch 79/8000

Epoch 00079: val_loss did not improve from 11.94512
 - 111s - loss: 12.0739 - val_loss: 12.0285
Epoch 80/8000

Epoch 00080: val_loss did not improve from 11.94512
 - 110s - loss: 12.1207 - val_loss: 12.1434
Epoch 81/8000

Epoch 00081: val_loss did not improve from 11.94512
 - 111s - loss: 12.0306 - val_loss: 12.2690
Epoch 82/8000

Epoch 00082: val_loss did not improve from 11.94512
 - 110s - loss: 12.0354 - val_loss: 12.4016
Epoch 83/8000

Epoch 00083: val_loss did not improve from 11.94512
 - 110s - loss: 12.1985 - val_loss: 12.1877
Epoch 84/8000

Epoch 00084: val_loss did not improve from 11.94512
 - 111s - loss: 12.1315 - val_loss: 12.1145
Epoch 85/8000

Epoch 00085: val_loss did not improve from 11.94512
 - 111s - loss: 12.1001 - val_loss: 11.9604
Epoch 86/8000

Epoch 00086: val_loss did not improve from 11.94512
 - 111s - loss: 11.9346 - val_loss: 11.9702
Epoch 87/8000

Epoch 00087: val_loss did not improve from 11.94512
 - 111s - loss: 11.8174 - val_loss: 11.9580
Epoch 88/8000

Epoch 00088: val_loss improved from 11.94512 to 11.58657, saving model to ../../model_weights/model_2020-03-19_11-36-07.h5
 - 111s - loss: 11.7913 - val_loss: 11.5866
Epoch 89/8000

Epoch 00089: val_loss did not improve from 11.58657
 - 111s - loss: 11.8226 - val_loss: 11.7856
Epoch 90/8000

Epoch 00090: val_loss did not improve from 11.58657
 - 110s - loss: 11.6909 - val_loss: 11.7199
Epoch 91/8000

Epoch 00091: val_loss did not improve from 11.58657
 - 110s - loss: 11.6940 - val_loss: 11.8782
Epoch 92/8000

Epoch 00092: val_loss did not improve from 11.58657
 - 110s - loss: 11.7558 - val_loss: 12.1143
Epoch 93/8000

Epoch 00093: val_loss did not improve from 11.58657
 - 110s - loss: 11.8973 - val_loss: 12.0291
Epoch 94/8000

Epoch 00094: val_loss did not improve from 11.58657
 - 110s - loss: 11.7128 - val_loss: 11.7945
Epoch 95/8000

Epoch 00095: val_loss did not improve from 11.58657
 - 111s - loss: 11.6933 - val_loss: 11.7623
Epoch 96/8000

Epoch 00096: val_loss did not improve from 11.58657
 - 111s - loss: 11.6673 - val_loss: 11.8693
Epoch 97/8000

Epoch 00097: val_loss did not improve from 11.58657
 - 110s - loss: 11.5303 - val_loss: 11.7688
Epoch 98/8000

Epoch 00098: val_loss did not improve from 11.58657
 - 111s - loss: 11.5808 - val_loss: 11.6044
Epoch 99/8000

Epoch 00099: val_loss did not improve from 11.58657
 - 110s - loss: 11.6670 - val_loss: 13.4274
Epoch 100/8000

Epoch 00100: val_loss improved from 11.58657 to 11.57019, saving model to ../../model_weights/model_2020-03-19_11-36-07.h5
 - 111s - loss: 11.9849 - val_loss: 11.5702
Epoch 101/8000

Epoch 00101: val_loss did not improve from 11.57019
 - 110s - loss: 11.6954 - val_loss: 11.6213
Epoch 102/8000

Epoch 00102: val_loss did not improve from 11.57019
 - 111s - loss: 11.6199 - val_loss: 11.6939
Epoch 103/8000

Epoch 00103: val_loss did not improve from 11.57019
 - 111s - loss: 11.6377 - val_loss: 11.7564
Epoch 104/8000

Epoch 00104: val_loss did not improve from 11.57019
 - 110s - loss: 11.5228 - val_loss: 11.7425
Epoch 105/8000

Epoch 00105: val_loss did not improve from 11.57019
 - 111s - loss: 11.6268 - val_loss: 11.6764
Epoch 106/8000

Epoch 00106: val_loss did not improve from 11.57019
 - 111s - loss: 11.5879 - val_loss: 11.6103
Epoch 107/8000

Epoch 00107: val_loss did not improve from 11.57019
 - 111s - loss: 11.9120 - val_loss: 11.9041
Epoch 108/8000

Epoch 00108: val_loss did not improve from 11.57019
 - 111s - loss: 11.6825 - val_loss: 11.7930
Epoch 109/8000

Epoch 00109: val_loss did not improve from 11.57019
 - 111s - loss: 11.9222 - val_loss: 11.9590
Epoch 110/8000

Epoch 00110: val_loss did not improve from 11.57019
 - 111s - loss: 11.9164 - val_loss: 11.8581
Epoch 111/8000

Epoch 00111: val_loss did not improve from 11.57019
 - 110s - loss: 11.6773 - val_loss: 12.1021
Epoch 112/8000

Epoch 00112: val_loss did not improve from 11.57019
 - 110s - loss: 11.5774 - val_loss: 11.6432
Epoch 113/8000

Epoch 00113: val_loss did not improve from 11.57019
 - 110s - loss: 11.7769 - val_loss: 12.3006
Epoch 114/8000

Epoch 00114: val_loss did not improve from 11.57019
 - 111s - loss: 11.6415 - val_loss: 11.6368
Epoch 115/8000

Epoch 00115: val_loss did not improve from 11.57019
 - 111s - loss: 11.5367 - val_loss: 11.6220
Epoch 116/8000

Epoch 00116: val_loss did not improve from 11.57019
 - 111s - loss: 11.6003 - val_loss: 11.6314
Epoch 117/8000

Epoch 00117: val_loss did not improve from 11.57019
 - 111s - loss: 11.7084 - val_loss: 11.7935
Epoch 118/8000

Epoch 00118: val_loss improved from 11.57019 to 11.45334, saving model to ../../model_weights/model_2020-03-19_11-36-07.h5
 - 111s - loss: 11.5067 - val_loss: 11.4533
Epoch 119/8000

Epoch 00119: val_loss did not improve from 11.45334
 - 111s - loss: 11.4828 - val_loss: 11.5423
Epoch 120/8000

Epoch 00120: val_loss did not improve from 11.45334
 - 110s - loss: 11.5832 - val_loss: 12.2431
Epoch 121/8000

Epoch 00121: val_loss did not improve from 11.45334
 - 110s - loss: 11.7204 - val_loss: 11.4957
Epoch 122/8000

Epoch 00122: val_loss did not improve from 11.45334
 - 110s - loss: 11.4676 - val_loss: 12.2227
Epoch 123/8000

Epoch 00123: val_loss did not improve from 11.45334
 - 111s - loss: 11.4322 - val_loss: 12.1505
Epoch 124/8000

Epoch 00124: val_loss did not improve from 11.45334
 - 111s - loss: 11.4382 - val_loss: 11.5566
Epoch 125/8000

Epoch 00125: val_loss did not improve from 11.45334
 - 110s - loss: 11.4715 - val_loss: 11.7715
Epoch 126/8000

Epoch 00126: val_loss improved from 11.45334 to 11.42193, saving model to ../../model_weights/model_2020-03-19_11-36-07.h5
 - 111s - loss: 11.3235 - val_loss: 11.4219
Epoch 127/8000

Epoch 00127: val_loss did not improve from 11.42193
 - 111s - loss: 11.4352 - val_loss: 11.6157
Epoch 128/8000

Epoch 00128: val_loss improved from 11.42193 to 11.40543, saving model to ../../model_weights/model_2020-03-19_11-36-07.h5
 - 111s - loss: 11.5302 - val_loss: 11.4054
Epoch 129/8000

Epoch 00129: val_loss did not improve from 11.40543
 - 111s - loss: 11.3248 - val_loss: 12.1077
Epoch 130/8000

Epoch 00130: val_loss did not improve from 11.40543
 - 111s - loss: 11.3780 - val_loss: 11.4616
Epoch 131/8000

Epoch 00131: val_loss improved from 11.40543 to 11.25467, saving model to ../../model_weights/model_2020-03-19_11-36-07.h5
 - 110s - loss: 11.2717 - val_loss: 11.2547
Epoch 132/8000

Epoch 00132: val_loss did not improve from 11.25467
 - 110s - loss: 11.3072 - val_loss: 12.3050
Epoch 133/8000

Epoch 00133: val_loss did not improve from 11.25467
 - 111s - loss: 11.2957 - val_loss: 11.3119
Epoch 134/8000

Epoch 00134: val_loss improved from 11.25467 to 11.19415, saving model to ../../model_weights/model_2020-03-19_11-36-07.h5
 - 111s - loss: 11.2801 - val_loss: 11.1942
Epoch 135/8000

Epoch 00135: val_loss did not improve from 11.19415
 - 110s - loss: 11.1370 - val_loss: 11.3490
Epoch 136/8000

Epoch 00136: val_loss did not improve from 11.19415
 - 111s - loss: 11.2578 - val_loss: 11.6178
Epoch 137/8000

Epoch 00137: val_loss did not improve from 11.19415
 - 111s - loss: 11.4163 - val_loss: 11.5782
Epoch 138/8000

Epoch 00138: val_loss did not improve from 11.19415
 - 111s - loss: 11.3930 - val_loss: 11.4498
Epoch 139/8000

Epoch 00139: val_loss did not improve from 11.19415
 - 110s - loss: 11.4661 - val_loss: 11.5835
Epoch 140/8000

Epoch 00140: val_loss did not improve from 11.19415
 - 110s - loss: 11.3169 - val_loss: 11.4370
Epoch 141/8000

Epoch 00141: val_loss did not improve from 11.19415
 - 110s - loss: 11.1757 - val_loss: 12.0148
Epoch 142/8000

Epoch 00142: val_loss improved from 11.19415 to 11.09081, saving model to ../../model_weights/model_2020-03-19_11-36-07.h5
 - 110s - loss: 11.7079 - val_loss: 11.0908
Epoch 143/8000

Epoch 00143: val_loss did not improve from 11.09081
 - 110s - loss: 11.2930 - val_loss: 11.1797
Epoch 144/8000

Epoch 00144: val_loss did not improve from 11.09081
 - 111s - loss: 11.2517 - val_loss: 11.4026
Epoch 145/8000

Epoch 00145: val_loss did not improve from 11.09081
 - 111s - loss: 11.0953 - val_loss: 11.3840
Epoch 146/8000

Epoch 00146: val_loss did not improve from 11.09081
 - 110s - loss: 11.1619 - val_loss: 11.2989
Epoch 147/8000

Epoch 00147: val_loss did not improve from 11.09081
 - 111s - loss: 11.4659 - val_loss: 11.3810
Epoch 148/8000

Epoch 00148: val_loss did not improve from 11.09081
 - 111s - loss: 11.1779 - val_loss: 11.3023
Epoch 149/8000

Epoch 00149: val_loss did not improve from 11.09081
 - 111s - loss: 11.1409 - val_loss: 11.7223
Epoch 150/8000

Epoch 00150: val_loss improved from 11.09081 to 11.02517, saving model to ../../model_weights/model_2020-03-19_11-36-07.h5
 - 111s - loss: 11.0958 - val_loss: 11.0252
Epoch 151/8000

Epoch 00151: val_loss did not improve from 11.02517
 - 111s - loss: 11.0796 - val_loss: 11.1285
Epoch 152/8000

Epoch 00152: val_loss did not improve from 11.02517
 - 111s - loss: 11.0701 - val_loss: 11.1951
Epoch 153/8000

Epoch 00153: val_loss did not improve from 11.02517
 - 110s - loss: 11.0855 - val_loss: 11.1674
Epoch 154/8000

Epoch 00154: val_loss improved from 11.02517 to 10.91746, saving model to ../../model_weights/model_2020-03-19_11-36-07.h5
 - 110s - loss: 11.0266 - val_loss: 10.9175
Epoch 155/8000

Epoch 00155: val_loss did not improve from 10.91746
 - 110s - loss: 11.0393 - val_loss: 11.6532
Epoch 156/8000

Epoch 00156: val_loss did not improve from 10.91746
 - 110s - loss: 11.2922 - val_loss: 11.3617
Epoch 157/8000

Epoch 00157: val_loss did not improve from 10.91746
 - 110s - loss: 11.4994 - val_loss: 11.1237
Epoch 158/8000

Epoch 00158: val_loss did not improve from 10.91746
 - 111s - loss: 11.6513 - val_loss: 11.5193
Epoch 159/8000

Epoch 00159: val_loss did not improve from 10.91746
 - 111s - loss: 11.3915 - val_loss: 12.0986
Epoch 160/8000

Epoch 00160: val_loss did not improve from 10.91746
 - 111s - loss: 11.2903 - val_loss: 11.4211
Epoch 161/8000

Epoch 00161: val_loss did not improve from 10.91746
 - 111s - loss: 10.9246 - val_loss: 11.0677
Epoch 162/8000

Epoch 00162: val_loss did not improve from 10.91746
 - 111s - loss: 11.0168 - val_loss: 11.0551
Epoch 163/8000

Epoch 00163: val_loss did not improve from 10.91746
 - 110s - loss: 11.4414 - val_loss: 11.6047
Epoch 164/8000

Epoch 00164: val_loss did not improve from 10.91746
 - 110s - loss: 11.0897 - val_loss: 10.9490
Epoch 165/8000

Epoch 00165: val_loss did not improve from 10.91746
 - 111s - loss: 11.1996 - val_loss: 11.2800
Epoch 166/8000

Epoch 00166: val_loss did not improve from 10.91746
 - 111s - loss: 11.1332 - val_loss: 11.1930
Epoch 167/8000

Epoch 00167: val_loss did not improve from 10.91746
 - 110s - loss: 11.0751 - val_loss: 11.0266
Epoch 168/8000

Epoch 00168: val_loss did not improve from 10.91746
 - 111s - loss: 11.0175 - val_loss: 11.2806
Epoch 169/8000

Epoch 00169: val_loss did not improve from 10.91746
 - 111s - loss: 11.0149 - val_loss: 11.1142
Epoch 170/8000

Epoch 00170: val_loss did not improve from 10.91746
 - 111s - loss: 11.0430 - val_loss: 11.2774
Epoch 171/8000

Epoch 00171: val_loss did not improve from 10.91746
 - 111s - loss: 11.2690 - val_loss: 11.2495
Epoch 172/8000

Epoch 00172: val_loss did not improve from 10.91746
 - 111s - loss: 11.1096 - val_loss: 11.3555
Epoch 173/8000

Epoch 00173: val_loss did not improve from 10.91746
 - 110s - loss: 11.0144 - val_loss: 10.9176
Epoch 174/8000

Epoch 00174: val_loss improved from 10.91746 to 10.90348, saving model to ../../model_weights/model_2020-03-19_11-36-07.h5
 - 110s - loss: 10.9759 - val_loss: 10.9035
Epoch 175/8000

Epoch 00175: val_loss did not improve from 10.90348
 - 110s - loss: 10.9470 - val_loss: 11.6347
Epoch 176/8000

Epoch 00176: val_loss did not improve from 10.90348
 - 110s - loss: 11.0885 - val_loss: 11.6608
Epoch 177/8000

Epoch 00177: val_loss did not improve from 10.90348
 - 111s - loss: 11.0485 - val_loss: 11.0218
Epoch 178/8000

Epoch 00178: val_loss did not improve from 10.90348
 - 111s - loss: 10.7951 - val_loss: 11.1487
Epoch 179/8000

Epoch 00179: val_loss did not improve from 10.90348
 - 111s - loss: 11.1375 - val_loss: 11.1523
Epoch 180/8000

Epoch 00180: val_loss did not improve from 10.90348
 - 111s - loss: 10.9466 - val_loss: 11.3497
Epoch 181/8000

Epoch 00181: val_loss did not improve from 10.90348
 - 110s - loss: 10.8732 - val_loss: 11.1731
Epoch 182/8000

Epoch 00182: val_loss did not improve from 10.90348
 - 110s - loss: 11.1779 - val_loss: 11.1673
Epoch 183/8000

Epoch 00183: val_loss did not improve from 10.90348
 - 111s - loss: 10.8717 - val_loss: 10.9610
Epoch 184/8000

Epoch 00184: val_loss did not improve from 10.90348
 - 111s - loss: 10.9583 - val_loss: 11.1564
Epoch 185/8000

Epoch 00185: val_loss did not improve from 10.90348
 - 110s - loss: 10.8760 - val_loss: 11.0529
Epoch 186/8000

Epoch 00186: val_loss did not improve from 10.90348
 - 111s - loss: 11.0429 - val_loss: 11.5190
Epoch 187/8000

Epoch 00187: val_loss did not improve from 10.90348
 - 111s - loss: 12.3173 - val_loss: 12.4699
Epoch 188/8000

Epoch 00188: val_loss did not improve from 10.90348
 - 110s - loss: 11.7638 - val_loss: 11.2018
Epoch 189/8000

Epoch 00189: val_loss did not improve from 10.90348
 - 111s - loss: 11.4744 - val_loss: 11.3811
Epoch 190/8000

Epoch 00190: val_loss did not improve from 10.90348
 - 111s - loss: 11.2415 - val_loss: 11.5631
Epoch 191/8000

Epoch 00191: val_loss did not improve from 10.90348
 - 111s - loss: 11.4838 - val_loss: 12.1095
Epoch 192/8000

Epoch 00192: val_loss did not improve from 10.90348
 - 110s - loss: 11.9491 - val_loss: 11.5671
Epoch 193/8000

Epoch 00193: val_loss did not improve from 10.90348
 - 111s - loss: 11.2636 - val_loss: 11.2281
Epoch 194/8000

Epoch 00194: val_loss did not improve from 10.90348
 - 111s - loss: 10.9405 - val_loss: 11.0276
Epoch 195/8000

Epoch 00195: val_loss did not improve from 10.90348
 - 110s - loss: 11.3892 - val_loss: 12.0506
Epoch 196/8000

Epoch 00196: val_loss did not improve from 10.90348
 - 111s - loss: 12.1011 - val_loss: 11.4522
Epoch 197/8000

Epoch 00197: val_loss did not improve from 10.90348
 - 111s - loss: 11.4934 - val_loss: 11.5822
Epoch 198/8000

Epoch 00198: val_loss did not improve from 10.90348
 - 111s - loss: 11.4166 - val_loss: 11.2006
Epoch 199/8000

Epoch 00199: val_loss did not improve from 10.90348
 - 111s - loss: 11.1906 - val_loss: 11.5064
Epoch 200/8000

Epoch 00200: val_loss did not improve from 10.90348
 - 111s - loss: 11.4329 - val_loss: 11.5842
Epoch 201/8000

Epoch 00201: val_loss did not improve from 10.90348
 - 111s - loss: 11.4445 - val_loss: 12.2846
Epoch 202/8000

Epoch 00202: val_loss did not improve from 10.90348
 - 110s - loss: 11.7840 - val_loss: 11.5203
Epoch 203/8000

Epoch 00203: val_loss did not improve from 10.90348
 - 110s - loss: 11.2223 - val_loss: 11.2737
Epoch 204/8000

Epoch 00204: val_loss did not improve from 10.90348
 - 110s - loss: 10.9828 - val_loss: 11.2324
Epoch 205/8000

Epoch 00205: val_loss did not improve from 10.90348
 - 111s - loss: 10.9730 - val_loss: 11.4150
Epoch 206/8000

Epoch 00206: val_loss improved from 10.90348 to 10.84073, saving model to ../../model_weights/model_2020-03-19_11-36-07.h5
 - 111s - loss: 10.9760 - val_loss: 10.8407
Epoch 207/8000

Epoch 00207: val_loss did not improve from 10.84073
 - 111s - loss: 10.9689 - val_loss: 11.1198
Epoch 208/8000

Epoch 00208: val_loss did not improve from 10.84073
 - 111s - loss: 11.0565 - val_loss: 11.0832
Epoch 209/8000

Epoch 00209: val_loss did not improve from 10.84073
 - 110s - loss: 10.9540 - val_loss: 10.8856
Epoch 210/8000

Epoch 00210: val_loss did not improve from 10.84073
 - 111s - loss: 11.0724 - val_loss: 11.1322
Epoch 211/8000

Epoch 00211: val_loss did not improve from 10.84073
 - 111s - loss: 11.0365 - val_loss: 11.2374
Epoch 212/8000

Epoch 00212: val_loss improved from 10.84073 to 10.77050, saving model to ../../model_weights/model_2020-03-19_11-36-07.h5
 - 111s - loss: 11.0526 - val_loss: 10.7705
Epoch 213/8000

Epoch 00213: val_loss did not improve from 10.77050
 - 110s - loss: 11.4305 - val_loss: 12.6811
Epoch 214/8000

Epoch 00214: val_loss did not improve from 10.77050
 - 111s - loss: 12.3490 - val_loss: 11.7627
Epoch 215/8000

Epoch 00215: val_loss did not improve from 10.77050
 - 110s - loss: 11.6079 - val_loss: 11.4827
Epoch 216/8000

Epoch 00216: val_loss did not improve from 10.77050
 - 110s - loss: 11.2916 - val_loss: 11.2010
Epoch 217/8000

Epoch 00217: val_loss did not improve from 10.77050
 - 110s - loss: 11.0867 - val_loss: 11.6307
Epoch 218/8000

Epoch 00218: val_loss did not improve from 10.77050
 - 111s - loss: 11.0434 - val_loss: 11.2941
Epoch 219/8000

Epoch 00219: val_loss did not improve from 10.77050
 - 111s - loss: 11.5384 - val_loss: 11.7001
Epoch 220/8000

Epoch 00220: val_loss did not improve from 10.77050
 - 111s - loss: 11.9480 - val_loss: 11.5696
Epoch 221/8000

Epoch 00221: val_loss did not improve from 10.77050
 - 111s - loss: 11.8018 - val_loss: 12.3257
Epoch 222/8000

Epoch 00222: val_loss did not improve from 10.77050
 - 111s - loss: 11.8630 - val_loss: 11.7655
Epoch 223/8000

Epoch 00223: val_loss did not improve from 10.77050
 - 111s - loss: 12.0987 - val_loss: 11.9097
Epoch 224/8000

Epoch 00224: val_loss did not improve from 10.77050
 - 111s - loss: 12.6655 - val_loss: 12.2658
Epoch 225/8000

Epoch 00225: val_loss did not improve from 10.77050
 - 110s - loss: 11.9905 - val_loss: 12.1935
Epoch 226/8000

Epoch 00226: val_loss did not improve from 10.77050
 - 110s - loss: 12.0733 - val_loss: 11.6478
Epoch 227/8000

Epoch 00227: val_loss did not improve from 10.77050
 - 110s - loss: 11.5742 - val_loss: 11.3745
Epoch 228/8000

Epoch 00228: val_loss did not improve from 10.77050
 - 111s - loss: 12.0668 - val_loss: 12.1360
Epoch 229/8000

Epoch 00229: val_loss did not improve from 10.77050
 - 111s - loss: 11.7854 - val_loss: 11.8461
Epoch 230/8000

Epoch 00230: val_loss did not improve from 10.77050
 - 110s - loss: 12.1504 - val_loss: 11.9825
Epoch 231/8000

Epoch 00231: val_loss did not improve from 10.77050
 - 110s - loss: 11.5392 - val_loss: 11.7027
Epoch 232/8000

Epoch 00232: val_loss did not improve from 10.77050
 - 111s - loss: 11.5965 - val_loss: 11.5502
Epoch 233/8000

Epoch 00233: val_loss did not improve from 10.77050
 - 111s - loss: 11.4579 - val_loss: 11.3530
Epoch 234/8000

Epoch 00234: val_loss did not improve from 10.77050
 - 111s - loss: 11.2569 - val_loss: 11.0714
Epoch 235/8000

Epoch 00235: val_loss did not improve from 10.77050
 - 111s - loss: 11.1961 - val_loss: 11.5483
Epoch 236/8000

Epoch 00236: val_loss did not improve from 10.77050
 - 110s - loss: 11.1227 - val_loss: 11.3243
Epoch 237/8000

Epoch 00237: val_loss did not improve from 10.77050
 - 110s - loss: 11.1879 - val_loss: 11.0531
Epoch 238/8000

Epoch 00238: val_loss did not improve from 10.77050
 - 110s - loss: 10.9620 - val_loss: 11.2000
Epoch 239/8000

Epoch 00239: val_loss did not improve from 10.77050
 - 110s - loss: 11.9281 - val_loss: 11.8169
Epoch 240/8000

Epoch 00240: val_loss did not improve from 10.77050
 - 111s - loss: 11.5113 - val_loss: 11.2938
Epoch 241/8000

Epoch 00241: val_loss did not improve from 10.77050
 - 111s - loss: 11.2006 - val_loss: 11.3629
Epoch 242/8000

Epoch 00242: val_loss did not improve from 10.77050
 - 111s - loss: 11.0134 - val_loss: 11.3263
Epoch 243/8000

Epoch 00243: val_loss did not improve from 10.77050
 - 111s - loss: 11.0556 - val_loss: 11.5074
Epoch 244/8000

Epoch 00244: val_loss did not improve from 10.77050
 - 110s - loss: 11.4610 - val_loss: 11.3266
Epoch 245/8000

Epoch 00245: val_loss did not improve from 10.77050
 - 110s - loss: 11.3556 - val_loss: 11.7240
Epoch 246/8000

Epoch 00246: val_loss did not improve from 10.77050
 - 110s - loss: 10.9312 - val_loss: 10.8807
Epoch 247/8000

Epoch 00247: val_loss did not improve from 10.77050
 - 110s - loss: 10.9201 - val_loss: 11.0315
Epoch 248/8000

Epoch 00248: val_loss did not improve from 10.77050
 - 110s - loss: 10.8860 - val_loss: 10.9207
Epoch 249/8000

Epoch 00249: val_loss did not improve from 10.77050
 - 111s - loss: 11.0230 - val_loss: 10.8907
Epoch 250/8000

Epoch 00250: val_loss did not improve from 10.77050
 - 111s - loss: 11.1332 - val_loss: 11.2273
Epoch 251/8000

Epoch 00251: val_loss did not improve from 10.77050
 - 110s - loss: 11.1022 - val_loss: 10.9676
Epoch 252/8000

Epoch 00252: val_loss did not improve from 10.77050
 - 111s - loss: 10.8780 - val_loss: 10.9925
Epoch 253/8000

Epoch 00253: val_loss improved from 10.77050 to 10.76851, saving model to ../../model_weights/model_2020-03-19_11-36-07.h5
 - 111s - loss: 10.8662 - val_loss: 10.7685
Epoch 254/8000

Epoch 00254: val_loss did not improve from 10.76851
 - 111s - loss: 10.8245 - val_loss: 11.0356
Epoch 255/8000

Epoch 00255: val_loss did not improve from 10.76851
 - 110s - loss: 10.9946 - val_loss: 11.0957
Epoch 256/8000

Epoch 00256: val_loss did not improve from 10.76851
 - 111s - loss: 10.8799 - val_loss: 11.5923
Epoch 257/8000

Epoch 00257: val_loss did not improve from 10.76851
 - 111s - loss: 11.6582 - val_loss: 11.2370
Epoch 258/8000

Epoch 00258: val_loss did not improve from 10.76851
 - 110s - loss: 11.0553 - val_loss: 11.5387
Epoch 259/8000

Epoch 00259: val_loss did not improve from 10.76851
 - 111s - loss: 11.4437 - val_loss: 11.2047
Epoch 260/8000

Epoch 00260: val_loss did not improve from 10.76851
 - 111s - loss: 11.1354 - val_loss: 11.2739
Epoch 261/8000

Epoch 00261: val_loss did not improve from 10.76851
 - 111s - loss: 10.8467 - val_loss: 11.3324
Epoch 262/8000

Epoch 00262: val_loss did not improve from 10.76851
 - 111s - loss: 11.0864 - val_loss: 11.3740
Epoch 263/8000

Epoch 00263: val_loss did not improve from 10.76851
 - 111s - loss: 10.9711 - val_loss: 13.5112
Epoch 264/8000

Epoch 00264: val_loss did not improve from 10.76851
 - 111s - loss: 28.9878 - val_loss: 26.0446
Epoch 265/8000

Epoch 00265: val_loss did not improve from 10.76851
 - 110s - loss: 22.1652 - val_loss: 20.1483
Epoch 266/8000

Epoch 00266: val_loss did not improve from 10.76851
 - 110s - loss: 19.4578 - val_loss: 18.9432
Epoch 267/8000

Epoch 00267: val_loss did not improve from 10.76851
 - 110s - loss: 20.9153 - val_loss: 18.9144
Epoch 268/8000

Epoch 00268: val_loss did not improve from 10.76851
 - 111s - loss: 18.0850 - val_loss: 17.5743
Epoch 269/8000

Epoch 00269: val_loss did not improve from 10.76851
 - 111s - loss: 17.2373 - val_loss: 17.2872
Epoch 270/8000

Epoch 00270: val_loss did not improve from 10.76851
 - 111s - loss: 19.3431 - val_loss: 16.7746
Epoch 271/8000

Epoch 00271: val_loss did not improve from 10.76851
 - 111s - loss: 16.5332 - val_loss: 16.3418
Epoch 272/8000

Epoch 00272: val_loss did not improve from 10.76851
 - 111s - loss: 16.9492 - val_loss: 16.0386
Epoch 273/8000

Epoch 00273: val_loss did not improve from 10.76851
 - 111s - loss: 15.9273 - val_loss: 15.9457
Epoch 274/8000

Epoch 00274: val_loss did not improve from 10.76851
 - 111s - loss: 15.6203 - val_loss: 15.5607
Epoch 275/8000

Epoch 00275: val_loss did not improve from 10.76851
 - 111s - loss: 35.4860 - val_loss: 20.7697
Epoch 276/8000

Epoch 00276: val_loss did not improve from 10.76851
 - 111s - loss: 19.1423 - val_loss: 18.1465
Epoch 277/8000

Epoch 00277: val_loss did not improve from 10.76851
 - 111s - loss: 17.6763 - val_loss: 17.1410
Epoch 278/8000

Epoch 00278: val_loss did not improve from 10.76851
 - 111s - loss: 17.9819 - val_loss: 22.5042
Epoch 279/8000

Epoch 00279: val_loss did not improve from 10.76851
 - 111s - loss: 17.0756 - val_loss: 16.3523
Epoch 280/8000

Epoch 00280: val_loss did not improve from 10.76851
 - 111s - loss: 16.1011 - val_loss: 15.9588
Epoch 281/8000

Epoch 00281: val_loss did not improve from 10.76851
 - 111s - loss: 15.7474 - val_loss: 15.6713
Epoch 282/8000

Epoch 00282: val_loss did not improve from 10.76851
 - 111s - loss: 15.4414 - val_loss: 15.3083
Epoch 283/8000

Epoch 00283: val_loss did not improve from 10.76851
 - 111s - loss: 15.1843 - val_loss: 15.0679
Epoch 284/8000

Epoch 00284: val_loss did not improve from 10.76851
 - 111s - loss: 14.9816 - val_loss: 14.8351
Epoch 285/8000

Epoch 00285: val_loss did not improve from 10.76851
 - 111s - loss: 14.7197 - val_loss: 14.6377
Epoch 286/8000

Epoch 00286: val_loss did not improve from 10.76851
 - 110s - loss: 14.5505 - val_loss: 14.5847
Epoch 287/8000

Epoch 00287: val_loss did not improve from 10.76851
 - 110s - loss: 14.4012 - val_loss: 14.4243
Epoch 288/8000

Epoch 00288: val_loss did not improve from 10.76851
 - 110s - loss: 14.2664 - val_loss: 14.1843
Epoch 289/8000

Epoch 00289: val_loss did not improve from 10.76851
 - 111s - loss: 14.1615 - val_loss: 14.0010
Epoch 290/8000

Epoch 00290: val_loss did not improve from 10.76851
 - 110s - loss: 14.0314 - val_loss: 13.8261
Epoch 291/8000

Epoch 00291: val_loss did not improve from 10.76851
 - 111s - loss: 13.8875 - val_loss: 13.9668
Epoch 292/8000

Epoch 00292: val_loss did not improve from 10.76851
 - 111s - loss: 13.9022 - val_loss: 13.7030
Epoch 293/8000

Epoch 00293: val_loss did not improve from 10.76851
 - 110s - loss: 13.6037 - val_loss: 13.4261
Epoch 294/8000

Epoch 00294: val_loss did not improve from 10.76851
 - 111s - loss: 13.5742 - val_loss: 13.4951
Epoch 295/8000

Epoch 00295: val_loss did not improve from 10.76851
 - 110s - loss: 13.6158 - val_loss: 13.3775
Epoch 296/8000

Epoch 00296: val_loss did not improve from 10.76851
 - 111s - loss: 13.4323 - val_loss: 13.3220
Epoch 297/8000

Epoch 00297: val_loss did not improve from 10.76851
 - 110s - loss: 13.3782 - val_loss: 13.2419
Epoch 298/8000

Epoch 00298: val_loss did not improve from 10.76851
 - 111s - loss: 13.4683 - val_loss: 13.2339
Epoch 299/8000

Epoch 00299: val_loss did not improve from 10.76851
 - 110s - loss: 13.1592 - val_loss: 13.0293
Epoch 300/8000

Epoch 00300: val_loss did not improve from 10.76851
 - 110s - loss: 13.1999 - val_loss: 13.3550
Epoch 301/8000

Epoch 00301: val_loss did not improve from 10.76851
 - 110s - loss: 13.1000 - val_loss: 13.0895
Epoch 302/8000

Epoch 00302: val_loss did not improve from 10.76851
 - 111s - loss: 13.1270 - val_loss: 12.9646
Epoch 303/8000

Epoch 00303: val_loss did not improve from 10.76851
 - 111s - loss: 13.0083 - val_loss: 12.8331
Epoch 304/8000

Epoch 00304: val_loss did not improve from 10.76851
 - 110s - loss: 12.9166 - val_loss: 13.1307
Epoch 305/8000

Epoch 00305: val_loss did not improve from 10.76851
 - 111s - loss: 12.9858 - val_loss: 13.0066
Epoch 306/8000

Epoch 00306: val_loss did not improve from 10.76851
 - 111s - loss: 12.9356 - val_loss: 12.6507
Epoch 307/8000

Epoch 00307: val_loss did not improve from 10.76851
 - 110s - loss: 12.8524 - val_loss: 12.8306
Epoch 308/8000

Epoch 00308: val_loss did not improve from 10.76851
 - 110s - loss: 12.7634 - val_loss: 12.7929
Epoch 309/8000

Epoch 00309: val_loss did not improve from 10.76851
 - 110s - loss: 12.7741 - val_loss: 12.5101
Epoch 310/8000

Epoch 00310: val_loss did not improve from 10.76851
 - 110s - loss: 12.6518 - val_loss: 12.5604
Epoch 311/8000

Epoch 00311: val_loss did not improve from 10.76851
 - 110s - loss: 12.6000 - val_loss: 12.5254
Epoch 312/8000

Epoch 00312: val_loss did not improve from 10.76851
 - 111s - loss: 12.5315 - val_loss: 12.6264
Epoch 313/8000

Epoch 00313: val_loss did not improve from 10.76851
 - 111s - loss: 12.4684 - val_loss: 12.5164
Epoch 314/8000

Epoch 00314: val_loss did not improve from 10.76851
 - 110s - loss: 12.8315 - val_loss: 12.5395
Epoch 315/8000

Epoch 00315: val_loss did not improve from 10.76851
 - 111s - loss: 12.9539 - val_loss: 14.0688
Epoch 316/8000

Epoch 00316: val_loss did not improve from 10.76851
 - 111s - loss: 12.7768 - val_loss: 12.4761
Epoch 317/8000

Epoch 00317: val_loss did not improve from 10.76851
 - 111s - loss: 12.4061 - val_loss: 12.3572
Epoch 318/8000

Epoch 00318: val_loss did not improve from 10.76851
 - 110s - loss: 12.3601 - val_loss: 12.6974
Epoch 319/8000

Epoch 00319: val_loss did not improve from 10.76851
 - 111s - loss: 12.3863 - val_loss: 12.5253
Epoch 320/8000

Epoch 00320: val_loss did not improve from 10.76851
 - 111s - loss: 12.4070 - val_loss: 12.1631
Epoch 321/8000

Epoch 00321: val_loss did not improve from 10.76851
 - 110s - loss: 12.2993 - val_loss: 12.3685
Epoch 322/8000

Epoch 00322: val_loss did not improve from 10.76851
 - 111s - loss: 12.3220 - val_loss: 12.2792
Epoch 323/8000

Epoch 00323: val_loss did not improve from 10.76851
 - 111s - loss: 12.5011 - val_loss: 12.3120
Epoch 324/8000

Epoch 00324: val_loss did not improve from 10.76851
 - 111s - loss: 12.3914 - val_loss: 12.1940
Epoch 325/8000

Epoch 00325: val_loss did not improve from 10.76851
 - 111s - loss: 12.3029 - val_loss: 12.1259
Epoch 326/8000

Epoch 00326: val_loss did not improve from 10.76851
 - 111s - loss: 12.2281 - val_loss: 12.0383
Epoch 327/8000

Epoch 00327: val_loss did not improve from 10.76851
 - 111s - loss: 12.3524 - val_loss: 12.4610
Epoch 328/8000

Epoch 00328: val_loss did not improve from 10.76851
 - 110s - loss: 12.1837 - val_loss: 12.2499
Epoch 329/8000

Epoch 00329: val_loss did not improve from 10.76851
 - 110s - loss: 12.3888 - val_loss: 12.2508
Epoch 330/8000

Epoch 00330: val_loss did not improve from 10.76851
 - 111s - loss: 12.1389 - val_loss: 12.0890
Epoch 331/8000

Epoch 00331: val_loss did not improve from 10.76851
 - 111s - loss: 12.0564 - val_loss: 12.3355
Epoch 332/8000

Epoch 00332: val_loss did not improve from 10.76851
 - 110s - loss: 12.1092 - val_loss: 11.9935
Epoch 333/8000

Epoch 00333: val_loss did not improve from 10.76851
 - 111s - loss: 12.5417 - val_loss: 11.9929
Epoch 334/8000

Epoch 00334: val_loss did not improve from 10.76851
 - 111s - loss: 12.1533 - val_loss: 12.0732
Epoch 335/8000

Epoch 00335: val_loss did not improve from 10.76851
 - 110s - loss: 12.1088 - val_loss: 12.0505
Epoch 336/8000

Epoch 00336: val_loss did not improve from 10.76851
 - 111s - loss: 12.1024 - val_loss: 12.0061
Epoch 337/8000

Epoch 00337: val_loss did not improve from 10.76851
 - 110s - loss: 12.2091 - val_loss: 12.0912
Epoch 338/8000

Epoch 00338: val_loss did not improve from 10.76851
 - 110s - loss: 12.1172 - val_loss: 12.0426
Epoch 339/8000

Epoch 00339: val_loss did not improve from 10.76851
 - 110s - loss: 11.8591 - val_loss: 11.7400
Epoch 340/8000

Epoch 00340: val_loss did not improve from 10.76851
 - 111s - loss: 11.8499 - val_loss: 13.1616
Epoch 341/8000

Epoch 00341: val_loss did not improve from 10.76851
 - 111s - loss: 12.0921 - val_loss: 11.8500
Epoch 342/8000

Epoch 00342: val_loss did not improve from 10.76851
 - 110s - loss: 11.7423 - val_loss: 11.7039
Epoch 343/8000

Epoch 00343: val_loss did not improve from 10.76851
 - 111s - loss: 11.9190 - val_loss: 12.1897
Epoch 344/8000

Epoch 00344: val_loss did not improve from 10.76851
 - 111s - loss: 12.0021 - val_loss: 12.0302
Epoch 345/8000

Epoch 00345: val_loss did not improve from 10.76851
 - 111s - loss: 11.7988 - val_loss: 12.0400
Epoch 346/8000

Epoch 00346: val_loss did not improve from 10.76851
 - 111s - loss: 11.7035 - val_loss: 11.5111
Epoch 347/8000

Epoch 00347: val_loss did not improve from 10.76851
 - 111s - loss: 11.6817 - val_loss: 11.9440
Epoch 348/8000

Epoch 00348: val_loss did not improve from 10.76851
 - 110s - loss: 11.6423 - val_loss: 12.0552
Epoch 349/8000

Epoch 00349: val_loss did not improve from 10.76851
 - 110s - loss: 11.7259 - val_loss: 11.6220
Epoch 350/8000

Epoch 00350: val_loss did not improve from 10.76851
 - 111s - loss: 11.5555 - val_loss: 11.5755
Epoch 351/8000

Epoch 00351: val_loss did not improve from 10.76851
 - 111s - loss: 11.6047 - val_loss: 12.0665
Epoch 352/8000

Epoch 00352: val_loss did not improve from 10.76851
 - 111s - loss: 11.5069 - val_loss: 11.6603
Epoch 353/8000

Epoch 00353: val_loss did not improve from 10.76851
 - 111s - loss: 11.6736 - val_loss: 11.6616
Epoch 00353: early stopping
