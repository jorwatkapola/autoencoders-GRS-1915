2020-11-22 07:55:13.182627: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX
2020-11-22 07:55:13.492543: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:897] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-11-22 07:55:13.493108: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1405] Found device 0 with properties: 
name: TITAN Xp major: 6 minor: 1 memoryClockRate(GHz): 1.582
pciBusID: 0000:01:00.0
totalMemory: 11.91GiB freeMemory: 11.75GiB
2020-11-22 07:55:13.493143: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1484] Adding visible gpu devices: 0
2020-11-22 07:55:13.764984: I tensorflow/core/common_runtime/gpu/gpu_device.cc:965] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-11-22 07:55:13.765029: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971]      0 
2020-11-22 07:55:13.765038: I tensorflow/core/common_runtime/gpu/gpu_device.cc:984] 0:   N 
2020-11-22 07:55:13.765318: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1097] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 11366 MB memory) -> physical GPU (device: 0, name: TITAN Xp, pci bus id: 0000:01:00.0, compute capability: 6.1)
2020-11-22 07:55:13.966782: I tensorflow/core/kernels/cuda_solvers.cc:159] Creating CudaSolver handles for stream 0x561414fb3130
Epoch 1/8000

Epoch 00001: val_loss improved from inf to 66.64625, saving model to ../../model_weights/model_2020-11-22_07-55-14.h5
 - 243s - loss: 62.6152 - kl_loss: 3.4748 - val_loss: 66.6463 - val_kl_loss: 3.3892
Epoch 2/8000

Epoch 00002: val_loss improved from 66.64625 to 66.62322, saving model to ../../model_weights/model_2020-11-22_07-55-14.h5
 - 244s - loss: 62.3590 - kl_loss: 3.4989 - val_loss: 66.6232 - val_kl_loss: 3.3943
Epoch 3/8000

Epoch 00003: val_loss improved from 66.62322 to 66.40187, saving model to ../../model_weights/model_2020-11-22_07-55-14.h5
 - 244s - loss: 62.2289 - kl_loss: 3.5390 - val_loss: 66.4019 - val_kl_loss: 3.3834
Epoch 4/8000

Epoch 00004: val_loss did not improve from 66.40187
 - 244s - loss: 62.6253 - kl_loss: 3.5943 - val_loss: 66.4480 - val_kl_loss: 3.4114
Epoch 5/8000

Epoch 00005: val_loss did not improve from 66.40187
 - 245s - loss: 62.7830 - kl_loss: 3.5855 - val_loss: 66.7895 - val_kl_loss: 3.4532
Epoch 6/8000

Epoch 00006: val_loss improved from 66.40187 to 66.32157, saving model to ../../model_weights/model_2020-11-22_07-55-14.h5
 - 244s - loss: 62.5759 - kl_loss: 3.5883 - val_loss: 66.3216 - val_kl_loss: 3.4558
Epoch 7/8000

Epoch 00007: val_loss did not improve from 66.32157
 - 244s - loss: 62.1316 - kl_loss: 3.5502 - val_loss: 66.3581 - val_kl_loss: 3.3911
Epoch 8/8000

Epoch 00008: val_loss did not improve from 66.32157
 - 245s - loss: 62.2492 - kl_loss: 3.5825 - val_loss: 66.5347 - val_kl_loss: 3.4161
Epoch 9/8000

Epoch 00009: val_loss did not improve from 66.32157
 - 245s - loss: 62.6124 - kl_loss: 3.6538 - val_loss: 66.8318 - val_kl_loss: 3.5699
Epoch 10/8000

Epoch 00010: val_loss did not improve from 66.32157
 - 246s - loss: 62.5204 - kl_loss: 3.6546 - val_loss: 66.6137 - val_kl_loss: 3.4872
Epoch 11/8000

Epoch 00011: val_loss did not improve from 66.32157
 - 246s - loss: 63.2106 - kl_loss: 3.7293 - val_loss: 67.4522 - val_kl_loss: 3.6314
Epoch 12/8000

Epoch 00012: val_loss did not improve from 66.32157
 - 245s - loss: 65.8721 - kl_loss: 3.9128 - val_loss: 70.3128 - val_kl_loss: 3.9337
Epoch 13/8000

Epoch 00013: val_loss did not improve from 66.32157
 - 244s - loss: 68.3340 - kl_loss: 4.1119 - val_loss: 71.6718 - val_kl_loss: 4.0534
Epoch 14/8000

Epoch 00014: val_loss did not improve from 66.32157
 - 245s - loss: 68.8718 - kl_loss: 4.0997 - val_loss: 70.8349 - val_kl_loss: 3.9074
Epoch 15/8000

Epoch 00015: val_loss did not improve from 66.32157
 - 248s - loss: 68.4760 - kl_loss: 4.0911 - val_loss: 70.9208 - val_kl_loss: 3.9033
Epoch 16/8000

Epoch 00016: val_loss did not improve from 66.32157
 - 245s - loss: 67.4436 - kl_loss: 3.9993 - val_loss: 70.0250 - val_kl_loss: 3.8095
Epoch 17/8000

Epoch 00017: val_loss did not improve from 66.32157
 - 245s - loss: 67.3160 - kl_loss: 4.0328 - val_loss: 70.1181 - val_kl_loss: 3.9605
Epoch 18/8000

Epoch 00018: val_loss did not improve from 66.32157
 - 246s - loss: 67.5804 - kl_loss: 4.0769 - val_loss: 69.4323 - val_kl_loss: 3.8384
Epoch 19/8000

Epoch 00019: val_loss did not improve from 66.32157
 - 246s - loss: 66.8979 - kl_loss: 4.0033 - val_loss: 69.5727 - val_kl_loss: 3.8863
Epoch 20/8000

Epoch 00020: val_loss did not improve from 66.32157
 - 245s - loss: 68.0341 - kl_loss: 4.1660 - val_loss: 70.3607 - val_kl_loss: 3.8897
Epoch 21/8000

Epoch 00021: val_loss did not improve from 66.32157
 - 244s - loss: 67.2172 - kl_loss: 4.0479 - val_loss: 69.3678 - val_kl_loss: 3.7942
Epoch 22/8000

Epoch 00022: val_loss did not improve from 66.32157
 - 248s - loss: 66.1497 - kl_loss: 3.9425 - val_loss: 69.0328 - val_kl_loss: 3.7920
Epoch 23/8000

Epoch 00023: val_loss did not improve from 66.32157
 - 246s - loss: 65.6231 - kl_loss: 3.9429 - val_loss: 69.6586 - val_kl_loss: 3.8844
Epoch 24/8000

Epoch 00024: val_loss did not improve from 66.32157
 - 247s - loss: 65.7904 - kl_loss: 4.0180 - val_loss: 68.7545 - val_kl_loss: 3.7574
Epoch 25/8000

Epoch 00025: val_loss did not improve from 66.32157
 - 245s - loss: 65.7288 - kl_loss: 4.0217 - val_loss: 69.6602 - val_kl_loss: 4.0052
Epoch 26/8000

Epoch 00026: val_loss did not improve from 66.32157
 - 246s - loss: 66.1174 - kl_loss: 4.0643 - val_loss: 68.7011 - val_kl_loss: 3.8112
Epoch 27/8000

Epoch 00027: val_loss did not improve from 66.32157
 - 245s - loss: 65.8043 - kl_loss: 4.0356 - val_loss: 69.3325 - val_kl_loss: 3.8868
Epoch 28/8000

Epoch 00028: val_loss did not improve from 66.32157
 - 245s - loss: 65.9166 - kl_loss: 3.9989 - val_loss: 68.9765 - val_kl_loss: 3.8251
Epoch 29/8000

Epoch 00029: val_loss did not improve from 66.32157
 - 248s - loss: 65.8779 - kl_loss: 4.0248 - val_loss: 69.2490 - val_kl_loss: 3.8754
Epoch 30/8000

Epoch 00030: val_loss did not improve from 66.32157
 - 245s - loss: 66.9370 - kl_loss: 4.1387 - val_loss: 71.4504 - val_kl_loss: 4.1772
Epoch 31/8000

Epoch 00031: val_loss did not improve from 66.32157
 - 246s - loss: 67.5407 - kl_loss: 4.1677 - val_loss: 71.0189 - val_kl_loss: 4.1677
Epoch 32/8000

Epoch 00032: val_loss did not improve from 66.32157
 - 246s - loss: 67.0867 - kl_loss: 4.1238 - val_loss: 70.0582 - val_kl_loss: 3.9269
Epoch 33/8000

Epoch 00033: val_loss did not improve from 66.32157
 - 246s - loss: 67.1687 - kl_loss: 4.1064 - val_loss: 71.2302 - val_kl_loss: 4.0920
Epoch 34/8000

Epoch 00034: val_loss did not improve from 66.32157
 - 245s - loss: 68.9219 - kl_loss: 4.2245 - val_loss: 69.7769 - val_kl_loss: 3.8273
Epoch 35/8000

Epoch 00035: val_loss did not improve from 66.32157
 - 244s - loss: 66.3056 - kl_loss: 3.9953 - val_loss: 69.6306 - val_kl_loss: 3.9428
Epoch 36/8000

Epoch 00036: val_loss did not improve from 66.32157
 - 247s - loss: 65.6795 - kl_loss: 3.9726 - val_loss: 68.9243 - val_kl_loss: 3.8068
Epoch 37/8000

Epoch 00037: val_loss did not improve from 66.32157
 - 246s - loss: 66.1926 - kl_loss: 4.0483 - val_loss: 69.5396 - val_kl_loss: 3.8996
Epoch 38/8000

Epoch 00038: val_loss did not improve from 66.32157
 - 247s - loss: 66.2204 - kl_loss: 4.0430 - val_loss: 69.0488 - val_kl_loss: 3.7842
Epoch 39/8000

Epoch 00039: val_loss did not improve from 66.32157
 - 246s - loss: 65.8562 - kl_loss: 3.9988 - val_loss: 69.5215 - val_kl_loss: 3.9026
Epoch 40/8000

Epoch 00040: val_loss did not improve from 66.32157
 - 245s - loss: 66.5220 - kl_loss: 4.0611 - val_loss: 69.4185 - val_kl_loss: 3.8926
Epoch 41/8000

Epoch 00041: val_loss did not improve from 66.32157
 - 245s - loss: 66.7766 - kl_loss: 4.0947 - val_loss: 69.2952 - val_kl_loss: 3.8041
Epoch 42/8000

Epoch 00042: val_loss did not improve from 66.32157
 - 245s - loss: 65.2964 - kl_loss: 3.9156 - val_loss: 68.9910 - val_kl_loss: 3.7249
Epoch 43/8000

Epoch 00043: val_loss did not improve from 66.32157
 - 248s - loss: 64.9006 - kl_loss: 3.9101 - val_loss: 67.9088 - val_kl_loss: 3.7181
Epoch 44/8000

Epoch 00044: val_loss did not improve from 66.32157
 - 245s - loss: 63.9942 - kl_loss: 3.8114 - val_loss: 67.4064 - val_kl_loss: 3.5172
Epoch 45/8000

Epoch 00045: val_loss did not improve from 66.32157
 - 246s - loss: 63.4131 - kl_loss: 3.7464 - val_loss: 67.6666 - val_kl_loss: 3.6070
Epoch 46/8000

Epoch 00046: val_loss did not improve from 66.32157
 - 246s - loss: 64.7604 - kl_loss: 3.8902 - val_loss: 68.3802 - val_kl_loss: 3.7344
Epoch 47/8000

Epoch 00047: val_loss did not improve from 66.32157
 - 246s - loss: 65.8009 - kl_loss: 3.9744 - val_loss: 69.9138 - val_kl_loss: 3.8762
Epoch 48/8000

Epoch 00048: val_loss did not improve from 66.32157
 - 245s - loss: 68.5759 - kl_loss: 4.2100 - val_loss: 72.4418 - val_kl_loss: 4.1663
Epoch 49/8000

Epoch 00049: val_loss did not improve from 66.32157
 - 244s - loss: 69.0961 - kl_loss: 4.2301 - val_loss: 71.9663 - val_kl_loss: 4.0733
Epoch 50/8000

Epoch 00050: val_loss did not improve from 66.32157
 - 248s - loss: 70.2852 - kl_loss: 4.2784 - val_loss: 71.5289 - val_kl_loss: 4.0027
Epoch 51/8000

Epoch 00051: val_loss did not improve from 66.32157
 - 246s - loss: 67.8426 - kl_loss: 4.1214 - val_loss: 69.9274 - val_kl_loss: 3.8210
Epoch 52/8000

Epoch 00052: val_loss did not improve from 66.32157
 - 246s - loss: 66.5875 - kl_loss: 4.0093 - val_loss: 69.8607 - val_kl_loss: 3.8803
Epoch 53/8000

Epoch 00053: val_loss did not improve from 66.32157
 - 245s - loss: 66.1768 - kl_loss: 3.9461 - val_loss: 68.7511 - val_kl_loss: 3.6155
Epoch 54/8000

Epoch 00054: val_loss did not improve from 66.32157
 - 246s - loss: 65.1993 - kl_loss: 3.8869 - val_loss: 68.9373 - val_kl_loss: 3.7037
Epoch 55/8000

Epoch 00055: val_loss did not improve from 66.32157
 - 245s - loss: 65.9005 - kl_loss: 3.9583 - val_loss: 68.9034 - val_kl_loss: 3.7706
Epoch 56/8000

Epoch 00056: val_loss did not improve from 66.32157
 - 245s - loss: 65.2969 - kl_loss: 3.8675 - val_loss: 68.7170 - val_kl_loss: 3.6318
Epoch 00056: early stopping
