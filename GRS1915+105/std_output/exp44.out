2020-01-10 16:20:20.379343: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX
2020-01-10 16:20:20.500290: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:897] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-01-10 16:20:20.500901: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1405] Found device 0 with properties: 
name: TITAN Xp major: 6 minor: 1 memoryClockRate(GHz): 1.582
pciBusID: 0000:01:00.0
totalMemory: 11.91GiB freeMemory: 11.73GiB
2020-01-10 16:20:20.500919: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1484] Adding visible gpu devices: 0
2020-01-10 16:20:20.733063: I tensorflow/core/common_runtime/gpu/gpu_device.cc:965] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-01-10 16:20:20.733109: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971]      0 
2020-01-10 16:20:20.733118: I tensorflow/core/common_runtime/gpu/gpu_device.cc:984] 0:   N 
2020-01-10 16:20:20.733387: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1097] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 11348 MB memory) -> physical GPU (device: 0, name: TITAN Xp, pci bus id: 0000:01:00.0, compute capability: 6.1)
2020-01-10 16:20:20.947608: I tensorflow/core/kernels/cuda_solvers.cc:159] Creating CudaSolver handles for stream 0x564ba37f5020
Train on 32962 samples, validate on 1735 samples
Epoch 1/8000
 - 41s - loss: 0.7567 - val_loss: 0.2914

Epoch 00001: val_loss improved from inf to 0.29145, saving model to model_weights/lstm_autoencoder_2020-01-10_16-20-21.h5
Epoch 2/8000
 - 40s - loss: 0.3017 - val_loss: 0.2832

Epoch 00002: val_loss improved from 0.29145 to 0.28322, saving model to model_weights/lstm_autoencoder_2020-01-10_16-20-21.h5
Epoch 3/8000
 - 40s - loss: 0.3086 - val_loss: 0.2939

Epoch 00003: val_loss did not improve from 0.28322
Epoch 4/8000
 - 40s - loss: 0.3001 - val_loss: 0.2978

Epoch 00004: val_loss did not improve from 0.28322
Epoch 5/8000
 - 40s - loss: 0.2839 - val_loss: 0.2589

Epoch 00005: val_loss improved from 0.28322 to 0.25888, saving model to model_weights/lstm_autoencoder_2020-01-10_16-20-21.h5
Epoch 6/8000
 - 40s - loss: 0.2688 - val_loss: 0.2566

Epoch 00006: val_loss improved from 0.25888 to 0.25656, saving model to model_weights/lstm_autoencoder_2020-01-10_16-20-21.h5
Epoch 7/8000
 - 40s - loss: 0.2562 - val_loss: 0.2543

Epoch 00007: val_loss improved from 0.25656 to 0.25431, saving model to model_weights/lstm_autoencoder_2020-01-10_16-20-21.h5
Epoch 8/8000
 - 40s - loss: 0.2514 - val_loss: 0.2387

Epoch 00008: val_loss improved from 0.25431 to 0.23874, saving model to model_weights/lstm_autoencoder_2020-01-10_16-20-21.h5
Epoch 9/8000
 - 40s - loss: 0.2421 - val_loss: 0.2317

Epoch 00009: val_loss improved from 0.23874 to 0.23171, saving model to model_weights/lstm_autoencoder_2020-01-10_16-20-21.h5
Epoch 10/8000
 - 40s - loss: 0.2341 - val_loss: 0.2273

Epoch 00010: val_loss improved from 0.23171 to 0.22731, saving model to model_weights/lstm_autoencoder_2020-01-10_16-20-21.h5
Epoch 11/8000
 - 40s - loss: 0.2357 - val_loss: 0.2230

Epoch 00011: val_loss improved from 0.22731 to 0.22301, saving model to model_weights/lstm_autoencoder_2020-01-10_16-20-21.h5
Epoch 12/8000
 - 40s - loss: 0.2279 - val_loss: 0.2167

Epoch 00012: val_loss improved from 0.22301 to 0.21667, saving model to model_weights/lstm_autoencoder_2020-01-10_16-20-21.h5
Epoch 13/8000
 - 40s - loss: 0.2193 - val_loss: 0.2189

Epoch 00013: val_loss did not improve from 0.21667
Epoch 14/8000
 - 40s - loss: 0.2196 - val_loss: 0.2150

Epoch 00014: val_loss improved from 0.21667 to 0.21503, saving model to model_weights/lstm_autoencoder_2020-01-10_16-20-21.h5
Epoch 15/8000
 - 40s - loss: 0.2192 - val_loss: 0.2134

Epoch 00015: val_loss improved from 0.21503 to 0.21342, saving model to model_weights/lstm_autoencoder_2020-01-10_16-20-21.h5
Epoch 16/8000
 - 40s - loss: 0.2141 - val_loss: 0.2060

Epoch 00016: val_loss improved from 0.21342 to 0.20596, saving model to model_weights/lstm_autoencoder_2020-01-10_16-20-21.h5
Epoch 17/8000
 - 40s - loss: 0.2093 - val_loss: 0.2389

Epoch 00017: val_loss did not improve from 0.20596
Epoch 18/8000
 - 40s - loss: 0.2106 - val_loss: 0.2088

Epoch 00018: val_loss did not improve from 0.20596
Epoch 19/8000
 - 40s - loss: 0.2044 - val_loss: 0.1964

Epoch 00019: val_loss improved from 0.20596 to 0.19645, saving model to model_weights/lstm_autoencoder_2020-01-10_16-20-21.h5
Epoch 20/8000
 - 40s - loss: 0.2070 - val_loss: 0.2139

Epoch 00020: val_loss did not improve from 0.19645
Epoch 21/8000
 - 40s - loss: 0.2046 - val_loss: 0.2027

Epoch 00021: val_loss did not improve from 0.19645
Epoch 22/8000
 - 40s - loss: 0.2015 - val_loss: 0.1916

Epoch 00022: val_loss improved from 0.19645 to 0.19160, saving model to model_weights/lstm_autoencoder_2020-01-10_16-20-21.h5
Epoch 23/8000
 - 40s - loss: 0.1976 - val_loss: 0.2316

Epoch 00023: val_loss did not improve from 0.19160
Epoch 24/8000
 - 40s - loss: 0.1963 - val_loss: 0.1885

Epoch 00024: val_loss improved from 0.19160 to 0.18849, saving model to model_weights/lstm_autoencoder_2020-01-10_16-20-21.h5
Epoch 25/8000
 - 40s - loss: 0.1952 - val_loss: 0.1932

Epoch 00025: val_loss did not improve from 0.18849
Epoch 26/8000
 - 40s - loss: 0.1944 - val_loss: 0.1973

Epoch 00026: val_loss did not improve from 0.18849
Epoch 27/8000
 - 40s - loss: 0.1889 - val_loss: 0.1823

Epoch 00027: val_loss improved from 0.18849 to 0.18235, saving model to model_weights/lstm_autoencoder_2020-01-10_16-20-21.h5
Epoch 28/8000
 - 40s - loss: 0.1896 - val_loss: 0.1921

Epoch 00028: val_loss did not improve from 0.18235
Epoch 29/8000
 - 40s - loss: 0.1946 - val_loss: 0.1940

Epoch 00029: val_loss did not improve from 0.18235
Epoch 30/8000
 - 40s - loss: 0.1858 - val_loss: 0.1803

Epoch 00030: val_loss improved from 0.18235 to 0.18027, saving model to model_weights/lstm_autoencoder_2020-01-10_16-20-21.h5
Epoch 31/8000
 - 40s - loss: 0.1877 - val_loss: 0.1811

Epoch 00031: val_loss did not improve from 0.18027
Epoch 32/8000
 - 40s - loss: 0.1894 - val_loss: 0.1856

Epoch 00032: val_loss did not improve from 0.18027
Epoch 33/8000
 - 40s - loss: 0.1889 - val_loss: 0.1839

Epoch 00033: val_loss did not improve from 0.18027
Epoch 34/8000
 - 40s - loss: 0.1850 - val_loss: 0.1791

Epoch 00034: val_loss improved from 0.18027 to 0.17914, saving model to model_weights/lstm_autoencoder_2020-01-10_16-20-21.h5
Epoch 35/8000
 - 40s - loss: 0.1823 - val_loss: 0.1792

Epoch 00035: val_loss did not improve from 0.17914
Epoch 36/8000
 - 40s - loss: 0.1792 - val_loss: 0.1731

Epoch 00036: val_loss improved from 0.17914 to 0.17313, saving model to model_weights/lstm_autoencoder_2020-01-10_16-20-21.h5
Epoch 37/8000
 - 40s - loss: 0.1830 - val_loss: 0.1853

Epoch 00037: val_loss did not improve from 0.17313
Epoch 38/8000
 - 40s - loss: 0.1811 - val_loss: 0.1740

Epoch 00038: val_loss did not improve from 0.17313
Epoch 39/8000
 - 40s - loss: 0.1804 - val_loss: 0.1742

Epoch 00039: val_loss did not improve from 0.17313
Epoch 40/8000
 - 40s - loss: 0.1769 - val_loss: 0.1866

Epoch 00040: val_loss did not improve from 0.17313
Epoch 41/8000
 - 40s - loss: 0.1824 - val_loss: 0.1773

Epoch 00041: val_loss did not improve from 0.17313
Epoch 42/8000
 - 40s - loss: 0.1812 - val_loss: 0.1733

Epoch 00042: val_loss did not improve from 0.17313
Epoch 43/8000
 - 40s - loss: 0.1738 - val_loss: 0.1758

Epoch 00043: val_loss did not improve from 0.17313
Epoch 44/8000
 - 40s - loss: 0.1769 - val_loss: 0.1676

Epoch 00044: val_loss improved from 0.17313 to 0.16764, saving model to model_weights/lstm_autoencoder_2020-01-10_16-20-21.h5
Epoch 45/8000
 - 40s - loss: 0.1807 - val_loss: 0.1844

Epoch 00045: val_loss did not improve from 0.16764
Epoch 46/8000
 - 40s - loss: 0.1756 - val_loss: 0.1677

Epoch 00046: val_loss did not improve from 0.16764
Epoch 47/8000
 - 40s - loss: 0.1740 - val_loss: 0.1707

Epoch 00047: val_loss did not improve from 0.16764
Epoch 48/8000
 - 40s - loss: 0.1736 - val_loss: 0.1646

Epoch 00048: val_loss improved from 0.16764 to 0.16462, saving model to model_weights/lstm_autoencoder_2020-01-10_16-20-21.h5
Epoch 49/8000
 - 40s - loss: 0.1718 - val_loss: 0.1687

Epoch 00049: val_loss did not improve from 0.16462
Epoch 50/8000
 - 40s - loss: 0.1779 - val_loss: 0.2039

Epoch 00050: val_loss did not improve from 0.16462
Epoch 51/8000
 - 40s - loss: 0.1879 - val_loss: 0.1787

Epoch 00051: val_loss did not improve from 0.16462
Epoch 52/8000
 - 40s - loss: 0.1750 - val_loss: 0.1688

Epoch 00052: val_loss did not improve from 0.16462
Epoch 53/8000
 - 40s - loss: 0.1736 - val_loss: 0.1712

Epoch 00053: val_loss did not improve from 0.16462
Epoch 54/8000
 - 40s - loss: 0.1698 - val_loss: 0.1719

Epoch 00054: val_loss did not improve from 0.16462
Epoch 55/8000
 - 40s - loss: 0.1742 - val_loss: 0.1850

Epoch 00055: val_loss did not improve from 0.16462
Epoch 56/8000
 - 40s - loss: 0.1763 - val_loss: 0.1631

Epoch 00056: val_loss improved from 0.16462 to 0.16314, saving model to model_weights/lstm_autoencoder_2020-01-10_16-20-21.h5
Epoch 57/8000
 - 40s - loss: 0.1690 - val_loss: 0.1775

Epoch 00057: val_loss did not improve from 0.16314
Epoch 58/8000
 - 40s - loss: 0.1679 - val_loss: 0.1663

Epoch 00058: val_loss did not improve from 0.16314
Epoch 59/8000
 - 40s - loss: 0.1685 - val_loss: 0.1762

Epoch 00059: val_loss did not improve from 0.16314
Epoch 60/8000
 - 40s - loss: 0.1710 - val_loss: 0.1817

Epoch 00060: val_loss did not improve from 0.16314
Epoch 61/8000
 - 40s - loss: 0.1824 - val_loss: 0.1694

Epoch 00061: val_loss did not improve from 0.16314
Epoch 62/8000
 - 40s - loss: 0.1706 - val_loss: 0.1664

Epoch 00062: val_loss did not improve from 0.16314
Epoch 63/8000
 - 40s - loss: 0.1699 - val_loss: 0.1988

Epoch 00063: val_loss did not improve from 0.16314
Epoch 64/8000
 - 40s - loss: 0.1667 - val_loss: 0.1587

Epoch 00064: val_loss improved from 0.16314 to 0.15868, saving model to model_weights/lstm_autoencoder_2020-01-10_16-20-21.h5
Epoch 65/8000
 - 40s - loss: 0.1701 - val_loss: 0.1712

Epoch 00065: val_loss did not improve from 0.15868
Epoch 66/8000
 - 40s - loss: 0.1730 - val_loss: 0.1763

Epoch 00066: val_loss did not improve from 0.15868
Epoch 67/8000
 - 40s - loss: 0.1686 - val_loss: 0.1920

Epoch 00067: val_loss did not improve from 0.15868
Epoch 68/8000
 - 40s - loss: 0.1661 - val_loss: 0.1671

Epoch 00068: val_loss did not improve from 0.15868
Epoch 69/8000
 - 40s - loss: 0.1650 - val_loss: 0.1680

Epoch 00069: val_loss did not improve from 0.15868
Epoch 70/8000
 - 40s - loss: 0.1654 - val_loss: 0.1827

Epoch 00070: val_loss did not improve from 0.15868
Epoch 71/8000
 - 40s - loss: 0.1687 - val_loss: 0.1664

Epoch 00071: val_loss did not improve from 0.15868
Epoch 72/8000
 - 40s - loss: 0.1669 - val_loss: 0.1625

Epoch 00072: val_loss did not improve from 0.15868
Epoch 73/8000
 - 40s - loss: 0.1645 - val_loss: 0.1562

Epoch 00073: val_loss improved from 0.15868 to 0.15616, saving model to model_weights/lstm_autoencoder_2020-01-10_16-20-21.h5
Epoch 74/8000
 - 40s - loss: 0.1655 - val_loss: 0.1624

Epoch 00074: val_loss did not improve from 0.15616
Epoch 75/8000
 - 40s - loss: 0.1847 - val_loss: 0.1887

Epoch 00075: val_loss did not improve from 0.15616
Epoch 76/8000
 - 40s - loss: 0.3768 - val_loss: 0.2738

Epoch 00076: val_loss did not improve from 0.15616
Epoch 77/8000
 - 40s - loss: 0.2728 - val_loss: 0.2557

Epoch 00077: val_loss did not improve from 0.15616
Epoch 78/8000
 - 40s - loss: 0.2494 - val_loss: 0.2418

Epoch 00078: val_loss did not improve from 0.15616
Epoch 79/8000
 - 40s - loss: 0.2352 - val_loss: 0.2229

Epoch 00079: val_loss did not improve from 0.15616
Epoch 80/8000
 - 40s - loss: 0.2289 - val_loss: 0.2199

Epoch 00080: val_loss did not improve from 0.15616
Epoch 81/8000
 - 40s - loss: 0.2214 - val_loss: 0.2112

Epoch 00081: val_loss did not improve from 0.15616
Epoch 82/8000
 - 40s - loss: 0.2184 - val_loss: 0.2094

Epoch 00082: val_loss did not improve from 0.15616
Epoch 83/8000
 - 40s - loss: 0.2149 - val_loss: 0.2091

Epoch 00083: val_loss did not improve from 0.15616
Epoch 84/8000
 - 40s - loss: 0.2110 - val_loss: 0.2045

Epoch 00084: val_loss did not improve from 0.15616
Epoch 85/8000
 - 40s - loss: 0.2097 - val_loss: 0.2052

Epoch 00085: val_loss did not improve from 0.15616
Epoch 86/8000
 - 40s - loss: 0.2097 - val_loss: 0.2174

Epoch 00086: val_loss did not improve from 0.15616
Epoch 87/8000
 - 40s - loss: 0.2038 - val_loss: 0.1933

Epoch 00087: val_loss did not improve from 0.15616
Epoch 88/8000
 - 40s - loss: 0.2018 - val_loss: 0.1924

Epoch 00088: val_loss did not improve from 0.15616
Epoch 89/8000
 - 40s - loss: 0.2120 - val_loss: 0.2173

Epoch 00089: val_loss did not improve from 0.15616
Epoch 90/8000
 - 40s - loss: 0.2029 - val_loss: 0.2016

Epoch 00090: val_loss did not improve from 0.15616
Epoch 91/8000
 - 40s - loss: 0.1977 - val_loss: 0.1904

Epoch 00091: val_loss did not improve from 0.15616
Epoch 92/8000
 - 40s - loss: 0.1988 - val_loss: 0.1963

Epoch 00092: val_loss did not improve from 0.15616
Epoch 93/8000
 - 40s - loss: 0.1942 - val_loss: 0.2115

Epoch 00093: val_loss did not improve from 0.15616
Epoch 94/8000
 - 40s - loss: 0.2003 - val_loss: 0.1939

Epoch 00094: val_loss did not improve from 0.15616
Epoch 95/8000
 - 40s - loss: 0.1944 - val_loss: 0.1848

Epoch 00095: val_loss did not improve from 0.15616
Epoch 96/8000
 - 40s - loss: 0.1918 - val_loss: 0.1860

Epoch 00096: val_loss did not improve from 0.15616
Epoch 97/8000
 - 40s - loss: 0.1905 - val_loss: 0.2024

Epoch 00097: val_loss did not improve from 0.15616
Epoch 98/8000
 - 40s - loss: 0.2438 - val_loss: 0.2011

Epoch 00098: val_loss did not improve from 0.15616
Epoch 99/8000
 - 40s - loss: 0.2094 - val_loss: 0.1967

Epoch 00099: val_loss did not improve from 0.15616
Epoch 100/8000
 - 40s - loss: 0.1999 - val_loss: 0.2035

Epoch 00100: val_loss did not improve from 0.15616
Epoch 101/8000
 - 40s - loss: 0.1985 - val_loss: 0.1845

Epoch 00101: val_loss did not improve from 0.15616
Epoch 102/8000
 - 40s - loss: 0.1914 - val_loss: 0.2107

Epoch 00102: val_loss did not improve from 0.15616
Epoch 103/8000
 - 40s - loss: 0.1922 - val_loss: 0.1826

Epoch 00103: val_loss did not improve from 0.15616
Epoch 104/8000
 - 40s - loss: 0.2000 - val_loss: 0.1984

Epoch 00104: val_loss did not improve from 0.15616
Epoch 105/8000
 - 40s - loss: 0.1944 - val_loss: 0.1945

Epoch 00105: val_loss did not improve from 0.15616
Epoch 106/8000
 - 40s - loss: 0.2350 - val_loss: 0.2306

Epoch 00106: val_loss did not improve from 0.15616
Epoch 107/8000
 - 40s - loss: 0.2159 - val_loss: 0.2182

Epoch 00107: val_loss did not improve from 0.15616
Epoch 108/8000
 - 40s - loss: 0.1933 - val_loss: 0.2045

Epoch 00108: val_loss did not improve from 0.15616
Epoch 109/8000
 - 40s - loss: 0.1888 - val_loss: 0.1917

Epoch 00109: val_loss did not improve from 0.15616
Epoch 110/8000
 - 40s - loss: 0.1929 - val_loss: 0.1753

Epoch 00110: val_loss did not improve from 0.15616
Epoch 111/8000
 - 40s - loss: 0.1924 - val_loss: 0.1858

Epoch 00111: val_loss did not improve from 0.15616
Epoch 112/8000
 - 40s - loss: 0.1890 - val_loss: 0.2053

Epoch 00112: val_loss did not improve from 0.15616
Epoch 113/8000
 - 40s - loss: 0.1846 - val_loss: 0.1975

Epoch 00113: val_loss did not improve from 0.15616
Epoch 114/8000
 - 40s - loss: 0.1919 - val_loss: 0.1887

Epoch 00114: val_loss did not improve from 0.15616
Epoch 115/8000
 - 40s - loss: 0.1881 - val_loss: 0.1799

Epoch 00115: val_loss did not improve from 0.15616
Epoch 116/8000
 - 40s - loss: 0.1849 - val_loss: 0.1968

Epoch 00116: val_loss did not improve from 0.15616
Epoch 117/8000
 - 40s - loss: 0.1897 - val_loss: 0.1791

Epoch 00117: val_loss did not improve from 0.15616
Epoch 118/8000
 - 40s - loss: 0.1815 - val_loss: 0.1759

Epoch 00118: val_loss did not improve from 0.15616
Epoch 119/8000
 - 40s - loss: 0.1901 - val_loss: 0.2007

Epoch 00119: val_loss did not improve from 0.15616
Epoch 120/8000
 - 40s - loss: 0.1860 - val_loss: 0.1862

Epoch 00120: val_loss did not improve from 0.15616
Epoch 121/8000
 - 40s - loss: 0.1815 - val_loss: 0.1704

Epoch 00121: val_loss did not improve from 0.15616
Epoch 122/8000
 - 40s - loss: 0.1774 - val_loss: 0.1755

Epoch 00122: val_loss did not improve from 0.15616
Epoch 123/8000
 - 40s - loss: 0.1849 - val_loss: 0.1743

Epoch 00123: val_loss did not improve from 0.15616
Epoch 00123: early stopping
